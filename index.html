<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://guileen.github.io">
  <title>桂糊涂的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Description">
<meta property="og:type" content="website">
<meta property="og:title" content="桂糊涂的博客">
<meta property="og:url" content="http://guileen.github.io/index.html">
<meta property="og:site_name" content="桂糊涂的博客">
<meta property="og:description" content="Description">
<meta property="article:author" content="桂糊涂">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="桂糊涂的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/avatar.jpg">
  
  
<link rel="stylesheet" href="/main.css">

  

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/img/avatar.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">桂糊涂</a></h1>
		</hgroup>

		
		<p class="header-subtitle">代码杂记</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories/%E9%A2%86%E5%AF%BC%E5%8A%9B">领导力</a></li>
	        
				<li><a href="/categories/AI">AI</a></li>
	        
				<li><a href="/categories/%E9%9A%8F%E7%AC%94">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/guileen" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="https://weibo.com/guileen" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">桂糊涂</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/img/avatar.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">桂糊涂</h1>
			</hgroup>
			
			<p class="header-subtitle">代码杂记</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/categories/%E9%A2%86%E5%AF%BC%E5%8A%9B">领导力</a></li>
		        
					<li><a href="/categories/AI">AI</a></li>
		        
					<li><a href="/categories/%E9%9A%8F%E7%AC%94">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/guileen" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="https://weibo.com/guileen" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-tao-of-tech-leadership" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/13/tao-of-tech-leadership/">关于领导力的一些思考</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如果说是公司是一个国家的话，那么CEO就是这个国家的主席，CTO就是这个国家的总理。一个国家在不同的阶段需要采用不同的社会制度，CTO也需要根据公司的发展阶段采用不同的管理模式和制度，引导合适的团队文化。此文是我对技术领导力的一次思考总结，《孙子兵法》、《实践论》、《矛盾论》给了我极大的启发。</p>
<ul>
<li>道与文化（道）</li>
<li>架构与领导（将）</li>
<li>制度与程序（法）</li>
<li>执行力与方法论（军争，九变，行军）</li>
<li>学习与训练（实践论）</li>
<li>决策与谋划（天地、形势、谋攻、矛盾论）</li>
</ul>
<h2 id="道与文化"><a href="#道与文化" class="headerlink" title="道与文化"></a>道与文化</h2><p>《孙子兵法》在《计》篇总结了战争胜负的五大要素曰道、天、地、将、法，其中道为第一要素。“道，令民与上同意也”，孙子认为<strong>道可以起到上下统一意志的作用</strong>。古今的战争史上，我们都可以看到道对战争结果的影响，比如国共内战，在战争初期国民党的力量远胜于共产党，但最终是共产党取得了战争的胜利，其根本原因就是中共领袖拥有极高的哲学智慧，其思想可“令民与上同意”。所谓的道不是指道家，而是指治国理念，可以理解为现代汉语所说的“主义”。春秋战国时期，诸侯争霸，百家争鸣，这些哲学家的治国理念在各国兴亡过程中发挥了巨大的影响力。这充分说明了哲学在国家治理中的重要性。</p>
<p>对于一个公司、一个团队而言，道也是至关重要的。谷歌、Facebook因其创始人都是技术极客，故而都有着非常吸引人的工程师文化，这使他们成为了行业领袖。苹果因乔布斯追求极致的产品理念而一举成功，而在乔布斯离开苹果的十多年里却无所建树，直到乔布斯重新回到苹果才使其重新辉煌。微软曾经是科技界的领军者，但在互联网时代却逐渐落后，而当纳德拉执掌微软CEO之后，微软则忽然焕然一新，展现出无限的活力。这些案例都充分说明了领导者的思想理念会对一家公司产生多么巨大的影响。</p>
<p>因为道的作用是统一意志，因而不可避免需要思想的宣传和同步，这也就逐渐形成了文化。所谓文化，也就是道在日常工作生活中的体现。<strong>有什么样的道，就会有什么样的文化，要实现什么样的道，就需要有什么要的文化</strong>。无论是我国的儒墨道法，还是西方的三大宗教，因为其根本思想的不同，也就会有不同的习俗文化，大众也有着不一样的价值观。在我国的前三十年，崇尚英雄，乐于奉献，以劳动为荣，以享乐为耻，而在后三十年，则强调时间就是金钱，崇尚消费，以企业家为榜样，这是因为前后三十年的治国之道不同而产生的文化变迁。这里我们不讨论各种价值观的优劣，仅从实际效果来看，这些文化都帮助了国家战略目标的达成。尤其是前三十年，任何国家实现工业化都是需要付出巨大的代价的，而我们并没有海外殖民地可以剥削，只能依靠本国人民的努力奉献才能实现。这些文化的建立和改变，都不是一朝一夕完成的，而是经过一次次的宣传和动员下逐步夯实的。</p>
<h3 id="道可道非恒道"><a href="#道可道非恒道" class="headerlink" title="道可道非恒道"></a>道可道非恒道</h3><p>既然道如此的重要，那么是否可以找到一个永远正确的道，一以贯之的执行呢？其兴也勃焉，其亡也忽焉，历朝历代都没有能跳出兴亡周期律。这说明并<strong>没有永远正确的道，只有在某一阶段相对正确的道</strong>。如果我们希望用一套方法来解决所有阶段的问题，肯定是错误的。但是，走一步看一步的想法也是不可取的，正所谓不谋万世者不足谋一时，不谋全局者不足谋一域。</p>
<p>在一个公司创业之初，首要目标是求得生存，建立一个根据地。对CTO的要求类似于对将领的要求，这一阶段的管理之道也就是治军之道，要求能指挥杀敌，能打胜仗。强调机动性、适应性。同时需要为下一阶段的发展做好准备，要善于学习和总结，并在队伍里培养骨干力量。这一阶段的公司文化，应当强调时间节点意识，持续交付，done is better than perfect。</p>
<p>当公司已经建立了根据地，进入到发展阶段，则需要有足够的大局观。这一阶段CTO更类似一个后勤部长，需要能够平衡不同业务线的需求，解决不断增长的需求与开发力量不足之间的矛盾。在这一阶段公司需要开展大型战役，在技术生产上需要给予足够可靠的支撑。这一阶段的团队文化，应强调秩序，要对各个需求部门建立需求管理制度，让听得到炮火的人有决策的权力，在运维上强调稳定和冗余。</p>
<p>当公司发展的过程中，不可避免需要与同业竞争。孙子曰：知己知彼百战不殆。我们需要对行业的最新发展保持关注，但也并不是人云亦云，对于新技术新动态需要有自己的判断。争夺技术制高点，是竞争的关键，而争夺技术制高点的关键则是在于人才。在这一阶段，CTO要扮演伯乐的角色，为公司招募精兵强将。塑造良好的工程师文化，鼓励内部推荐。</p>
<p>当公司的人数上升了一个数量级，使直接管理变得不可能，如何优化组织结构是这一阶段的关键。设计组织结构，需要根据公司的业务特点和实际情况来具体分析，组织结构会对员工的工作产生很大的影响，在实施的步骤上要循序渐进，并且为调整后可能出现的问题做好准备。</p>
<p>当公司已经取得了行业内的领导地位，则需要使公司能够持续的保持领先，避免安于现状。设立新的战略目标，建立激励创新的制度。设立研发中心，不仅要能够吸引行业内最优秀的人才，还需要能够培养最优秀的人才。</p>
<h3 id="CTO的目标是什么？"><a href="#CTO的目标是什么？" class="headerlink" title="CTO的目标是什么？"></a>CTO的目标是什么？</h3><p>一个国家的目标应该是人民的幸福生活，一个公司的目标则是公司的整体利益最大化。但从一个国家和公司的发展战略角度来说，这个目标是过于宽泛的，我们需要一个更具指导性的战略目标。</p>
<p>在人类文明发展史上，没有什么比科技的进步更能改变一个社会的面貌。在农业时代的中国即使有辽阔的疆土和数亿人口，依然会被工业化的日本所侵略。所以新中国提出的国家战略目标就是实现四个现代化，所谓现代化就是努力达到现代科技水平。对于一个公司而言，技术也同样是影响公司效益的最重要因素之一，如何使用技术来提升公司的生产力水平，提升公司的业务指标也就成为了一个CTO最重要的目标。</p>
<p>在工业化的早期阶段，必然是有大量的工作需要用手工劳动来处理的，但随着工业化的深入，需要使越来越多的工作自动化，减少手工劳动。这一过程，是技术与工艺流程不断促进的过程，使得其工艺流程更加标准化、工艺精度不断提高、制造成本不断降低。对于一个公司而言，也同样如此。在公司发展的早期，必然有大量凭借主观经验来实施的业务操作，这些操作可能存在着不可量化、信息不透明、效率低下、权责边界不清晰等等各种问题。在早期这类问题可能不会显现出来，但随着公司的发展，这类业务的持续增加，将使问题变得愈加严重，成为制约公司发展甚至影响公司存亡的问题。因此，对于一家公司而言<strong>推动技术与业务的高度融合并促进业务发展是CTO最重要的目标</strong>。</p>
<h3 id="集体向左，个人向右"><a href="#集体向左，个人向右" class="headerlink" title="集体向左，个人向右"></a>集体向左，个人向右</h3><p>对于企业文化，可以有很多不同形式的表达方式。如果不能够对企业文化进行深入理解，仅仅将企业文化当作一种心灵鸡汤，那么对于自己的管理之道可能不仅于事无补，还可能适得其反。我们需要透过企业文化的表象看到其本质，来进行分类。其实<strong>所有的企业文化，不外乎左右两种</strong>。</p>
<p>左右之争自古有之。西汉时贤良文学与桑弘羊就盐铁专卖制度进行辩论，儒生们认为盐铁专营是与民争利，反对战争，主张节用，批评桑弘羊骄横奢侈，而桑弘羊则认为国家应充实国库，加强国防，盐铁专营，批评儒生是古非今、迂腐。关于《盐铁论》孰是孰非的评价，并无定论，每个人在评价盐铁论时不可避免的带有自己的政治立场。西方启蒙运动之后，左派、右派的概念产生，左右的定义在不同时期有所不同，目前主要的区分是——社会主义是左，资本主义是右。对于中国而言，前三十年是左，后三十年是右，最近十年略向左转。对于国家制度而言，区分左右的标准主要是人人平等还是经济自由，国家在财富再分配中力度的大小。对于一家公司而言，分配制度自然也是区分左右的主要指标，比如强调低薪资高提成的，鼓励员工/部门之间相互竞争的，以经济激励为主，则偏右；强调员工持股，职级薪资，鼓励员工/部门之间相互合作的，以荣誉激励为主，则偏左。但归纳企业文化，若仅仅从分配制度来衡量，显然是不够的，我们还需要从其他维度来进行划分。</p>
<p>首先我们应当承认，<strong>左右两种思想不仅针锋相对，而且难分伯仲</strong>。而正因为其难分伯仲，也使得我们难以取舍。理解左右之争的根源，有利于我们做出正确的价值判断。左右之争的本质是平等与自由之争，平等的思想源于人是社会性的动物，需要彼此协作，自由源于人是创造性的动物，希望可以发挥个性。而平等和自由是有矛盾的，在自由竞争的状态下必然会产生不平等，若要兼顾平等则必然要限制部分的自由尤其是经济上的自由。从国家整体利益角度分析，如果强调平等则会影响创造力的发挥，影响科技的进步将使国家竞争力降低。如果强调自由则会加剧社会的不平等，产生腐败，使社会离心离德，甚至直接导致组织的崩溃。<strong>左和右的矛盾，也就是自由和平等的矛盾，也就是创造力和凝聚力的矛盾，也就是个人主义和集体主义的矛盾</strong>。</p>
<p>矛盾是对立的，也是统一的，正如太极阴中有阳阳中有阴。人类文明正是在左和右的斗争中不断的进步。对于一家公司而言，需要根据不同的部门、不同的职责、不同的目标来调整左右的尺度。比如对于QA、运维，需要强调规则、秩序、责任，而不应过多强调个性和自由。而对于设计师、研究人员、运营策划则需要给予充分的自由发挥的空间。对于执行型的岗位，应采用KPI考核模式，而对于创造型岗位，则应采用OKR考核模式。</p>
<p>无论是任何制度，都不应走向极端。对于一个研究院，我们会鼓励每个人发挥自己的创造性，但另一方面，会在薪资结构上采取相对固定的等级制薪资，而不是通过奖金来刺激创新。这是因为创新具有一定的不可预测性，创新也有较大的风险性，有时一个成功的创新恰恰是在多个失败的创新的基础上产生的。所以采用集体主义的模式来管理创新成本，同时又在执行层面鼓励发挥个体的创造性，这一案例体现了<strong>左中有右</strong>。成功科研机构大多采取这种管理模式。</p>
<p>在一个采用市场化规则管理的组织里，不同的部门之间在可能是通过代币、信用点来进行管理，以使各部门的资源分配能够自动平衡，但因为这一所谓市场显然是没有充分竞争的市场，也未必适合充分竞争，所以也需要使用固定价格，并且可能规定一些代币使用限制，以实现某种计划性的调控。这一案例体现了<strong>右中有左</strong>。</p>
<p>总体来说，我认为在<strong>企业文化建设和宣传上应偏向于集体主义的方向，在激励方式上偏向于荣誉激励</strong>，因为个人主义和物质激励是不需要过多宣传即可具备的。但在创造性方面，我们应鼓励个人提出、表达他们自己的见解。在文化宣传的方法上，应重视榜样的力量，多开展合适的团建活动。</p>
<h2 id="架构与将领"><a href="#架构与将领" class="headerlink" title="架构与将领"></a>架构与将领</h2><h3 id="组织与生命体的比较"><a href="#组织与生命体的比较" class="headerlink" title="组织与生命体的比较"></a>组织与生命体的比较</h3><p>国家或公司是复杂的人类社会组织。无论是宗教还是帝国，都有其组织架构。国家通过组织架构行使其意志，如果组织架构失去了其能力那么这个国家也就无异于死亡了。组织架构之于国家，如同四肢五脏之于大脑，动物如果没有了这些组织器官，空有大脑也无法发挥其意志。国家、公司、宗教这样的<strong>社会组织完全可以看作是一种更高级的生命形态</strong>。而生命体间最大的差别就是其组织形态的差别，不同的组织形态就是不同的物种。</p>
<p>人们对组织形态的设计，是通过不断的演化和竞争发展而来的，并不是一次性设计出来的。所有关于社会组织结构的研究必然以人类过往的经验为基础，不可能由某个哲学家直接提出。现在对于科层制（又称官僚制）的理论认为马科斯韦伯是鼻祖，显然这是不准确的，中国历史在启蒙运动时代对西方哲学产生了深远的影响，关于科层制的研究显然是在借鉴中国的官僚制度基础上得出的，而中国的官僚制度也是在春秋战国不断的竞争中逐步形成的。我们要对组织进行哲学上的思考，需要从历史的发展过程，从生命的起点开始推演。</p>
<p>评价社会组织优劣的角度可以有很多，我们在这里不做道德上的评价。我们需要区分组织的生存与构成组织的单元的生存并不相同，正如人的生存并不代表为了每一个细胞的生存，相反<strong>人能够生存的前提正是有赖于细胞的新陈代谢</strong>，如果细胞都是长生不老且无限繁殖的，那么对人体而言就是癌细胞。我们可以得出一个结论：<strong>个体利益不代表组织利益</strong>。因此我们此处评价社会组织优劣的标准是社会组织本身是否能够健康发展，而不以社会组织对于个体是否有利为评价标准，但也并不是说社会组织完全不考虑个体利益。</p>
<p>我们研究组织架构，就是为了构建一个能够适应环境的的社会生命形态。正如生命的目的都是生存与繁衍，社会组织的目的也同样是组织的生存。熵增定律已经指出，世界将趋向于越来越无序的状态，而生命为了能够维持自身的稳定，必须从环境中摄取“反熵”。所谓反熵，即是指需要不断的从外界汲取能量。在初始阶段是，组织如何从外界环境中汲取能量是主要的矛盾，比如光合作用就是在这一阶段产生，或使自己朝着更高更广的方向发展，<strong>模式和效率是这一阶段的关键字</strong>。但随着生命形态的发展，生命开始竞争更有效的汲取能量的手段，比如更快速的扩张挤占对手的生存空间，<strong>复制是这一阶段的关键字</strong>。而后又产生以其他生命体为食的动物形态，其关键字是<strong>信息感知能力（视、听、嗅、味、触）、信息处理能力（大脑）、移动能力（游泳、行走、飞行、速度）、武器、防御</strong>，这一类的形态也是极其丰富的。随后进入了一个军备竞赛的阶段，在军备竞赛中能够更快速演化的物种更有优势，于是产生了<strong>两性繁殖，这是一种融合的形态</strong>，可以取长补短，快速演化。随着军备竞赛的发展，<strong>个体又通过组合的方式结成社会</strong>。而人类社会相比其他动物的简单社会形态而言，又极不相同。因为人类拥有极强的信息处理能力，所以人类能够创造语言，而语言则能够更有效的传递信息。因此，<strong>人类可以通过自己的思考，在不改变人类基因的前提下，自发的改变自己的社会形态结构。这使得人类的社会形态具有类似生命的特点——竞争、融合、变异、进化</strong>。而人类社会的进化的速度是远快于基因突变的进化速度的。</p>
<p>通过上面对于生命的论述可以看出，社会组织也是一种生命体，而生命体也是由组织构成。一个优秀的高级社会组织应该具有高级生命体的特征，即模式、效率、复制、融合、信息的感知和处理、移动、武器、防御、组合。不同的社会组织自然在这些特征维度上各有长短，也有着不同的取胜之道，孰优孰劣只能由历史来检验。从组织架构的研究角度出发，我们选择<strong>效率、信息的感知和处理、融合</strong>几个角度进行分析，而其他诸如模式、武器、防御等特征则属于竞争策略的研究，不在这一段的讨论范围内。</p>
<h3 id="不同组织架构比较"><a href="#不同组织架构比较" class="headerlink" title="不同组织架构比较"></a>不同组织架构比较</h3><p>组织架构主要解决的问题是内部的信息协调，使一个庞大的组织高效运转，如同一人。</p>
<p>一个组织随着体量的增加，其效率将会降低，尤其是信息处理的效率会大幅降低。人能够维持紧密人际关系的上限为150人（邓巴数），这决定了一个简单团体的上限，而若要形成高效协作的团队那么这个人数将会更少，可能只在10人以内。春秋时管仲治理齐国，采用一个树状的结构，三十家为一邑，十邑为一卒，十卒为一乡，三乡为一县，十县为一属，全国共五属，每一层级各设一长官对下级进行管理，使齐国快速的强大了起来。说明了树形组织结构相比其他自然形成的松散结构能够更有效的组织起大规模的人员。因为在管理中涉及到不同专业，所以这一树形组织结构的进一步发展成为了官僚制，又称科层制。<strong>科层制的划分方式主要有专业分工、管辖范围</strong>。按专业能力划分，则按照专业能力的不同进入不同的树状序列中，每个节点的上级和下级都属于同一个专业，比如教育部和农业部。按照管辖范围划分，则每一层级有相同的权力和责任，只是所管理的对象不同，比如安徽省与江苏省。</p>
<p>按专业分工划分序列有利于专业本身传帮带，促进同行业的交流发展，避免外行领导内行。缺点是可能会缺少与其他专业的协调，以及缺少对特定管辖对象的关注。一般用于专业性强、战略性的部门。按管辖范围划分序列，每一层级有很好的平行特性，便于进行比较，可以有效的引入竞争机制来激励每个子部门的积极性，缺点则在于每个平行部门的客观条件不尽相同，无法简单进行比较，竞争机制的引入也会带来相应的副作用，同时不能保证每个平行部门都拥有高质量的专业人员，所以不能充分发挥某些专业的价值。<strong>总体来说专业性强、战略性的部门适合按专业划分，业务型、行政性的适合按管辖范围划分</strong>。</p>
<p>在战争时期，因为战争结果的胜负对组织的存亡影响巨大，必须充分保证战争机器的高效运转，因此需要对军事组织进行充分授权；但到了和平时期，军事组织过大的授权即是对组织本身最大的威胁，因此则需要减少授权。所以这种授权是临时性的。<strong>这种临时性大力度的授权可称之为项目制</strong>，在一段时期之内一个项目的负责人被授予大于其日常的资源调集能力。项目制通常与专业序列相配合。</p>
<p>如果采用科层制管理，有职责清晰、专业化、规则明确、结果稳定可预期的优点，但也会产生效率低、信息反馈延迟、抑制创造性、山头主义等缺点，通常我们用官僚主义来概括形容科层制的缺点。因此有了扁平化管理的提出，扁平化依然是一种科层制，传统科层制则被称为金字塔结构。扁平化相对于金字塔结构层级更少，每一层管理的成员更多。扁平化拥有信息效率高、充分授权激发创造性等优点，但也有主管人员精力分散、管理幅度大、管理者素质要求高、缺乏目标感、职责不清晰、协调并取得一致意见困难等缺点。简而言之<strong>金字塔结构需要较多的专业型管理人员，而扁平化管理则需要较少的全能型管理人员</strong>。</p>
<p>因为人天生所具有的惰性，因此需要考核激励制度。前面我们已经说过，在职能相同管辖范围不同的平行部门，因为容易对比，可以引入竞争机制来进行激励。但对于专业分工不同的部门，则无法通过简单的对比来进行激励。在计划经济的时代，有生产效率低、不经济等缺点，改革开放后，采用市场化的方式管理，有效激励了生产的热情。稻盛和夫在管理京瓷的时候，仿造市场的形式，创建了<strong>阿米巴模式</strong>，将整个公司切割为多个被称为阿米巴的小型组织，每个小型组织都是一个小独立的利润中心，按照小企业小商店的形式独立经营，比如制造过程的每一道工序都是一个阿米巴，阿米巴之间的物资交换和服务也都需要进行结算。阿米巴模式是一种内部市场化，有自组织的优点。但我们也要意识到，改革开放时期产生了“造导弹的不如卖茶叶蛋的“这种情况，也有降低成本破坏环境的现象发生，总之是为了个体利益而损害整体利益的情况，阿米巴模式也有同样的问题。<strong>阿米巴模式是一个偏右的制度，需要有集体文化的中和，另外在管理过程中强调量化管理，包括授权在内都需要量化衡量，需要有专门的部门来进行内部定价</strong>。</p>
<p>对于一家软件公司的技术部门是否可以采用阿米巴模式呢？不同部门对技术部门都有着五花八门的开发需求，而所有的部门都不必承担开发的成本，这样就使技术部门成为了一个纯成本部门而没有收益。在这种模式之下，技术部门必然会努力的减少成本，且缺乏积极性，这样技术部门就成为了其他部门不满的对象，以及其业绩不佳的借口。显然这种情况下的集体是一个缺乏凝聚力的集体。为了解决这个问题，我们不必拘泥于阿米巴模式的教条，我们需要知道其本质目标——<strong>让每个部门的付出都能得到合理的收益和荣誉，从而充分激发每个部门的积极性，并能够更紧密更高效的合作</strong>。这需要激励制度、核算制度、文化建设的多重配合。</p>
<h3 id="关于将领的一些思考"><a href="#关于将领的一些思考" class="headerlink" title="关于将领的一些思考"></a>关于将领的一些思考</h3><p>将领是组织的大脑。如同人体内一个个神经节点，虽然我们的大脑没有特地控制我们的心脏和肠胃，但这些组织都在按部就班的进行工作，这是因为这些组织各有他们的“主管”。孙子认为将领是制胜最重要的五个要素之一，而将领需要有“智信仁勇严”五种主要的素质，并且要“将能而君不御”。</p>
<p>孙子认为将领最重要的素质是“智”，在组织架构中的部门领导，我们可以认为最重要的素质是专业能力。一个缺乏专业能力的领导，无论其信仁勇严等道德素养如何优秀，都是很难胜任的。孙子所描述五大素质的是针对于军事上的领导者，对于不同的行业要求可能不尽相同，可以适度的参考。</p>
<p>所谓将能而君不御者，指的是授权。但这句话是对君主说的，而不是对将领说的，也就是君主应有向将领授权的觉悟，而不是将领有逾越授权的权力。<strong>在授权的事情上，要注意责、权、利对等</strong>，我们不能要求一个未被授权的人对结果负责，我们也不能要求一个虽被授权，但与其并无利益相关的人对结果负责。我们之所以有权要求一个人对结果负责，是因为我们赋予了其影响结果的权力，并支付了对于结果负责的成本。</p>
<p>将领从何而来呢？无非两种途径：内部提拔或外部招聘。<strong>内部提拔的好处是，了解公司的情况，能够更快的在新岗位上开展工作。外部招聘的好处是，能够获得新鲜的血液，带来新的观点和经验</strong>，也就是我们在生命特征中所提到的“<strong>融合</strong>”。这两种方式各自的优点也就对应着另一种方式的缺点。但是衡量哪种方式更好，依然<strong>取决于人选本身，而不取决于其是来自于内部提拔还是外部招聘</strong>。战国时秦国任用商鞅等客卿走向强盛，刘邦任用同乡的萧何建立基业，这二人对于其国家都是居功至伟的人物，一位是外部招聘、一位是内部提拔，足见两种方式并无优劣之分，主要在于人选本身。而韩信本服务于项羽，而项羽不能用之，转投刘邦，终使项羽败于其十面埋伏之下。项羽未能有效的执行内部提拔，而刘邦则内部提拔了萧何外部招聘了韩信。可见，对于将领的招聘和选拔，都应给予足够的重视，否则就会错失人才。</p>
<p>在我们意识到了将领的重要性后，又产生了一个新的讨论——因人设岗还是因岗设人？所谓因人设岗，也就是组织拥有了一个优秀的将领，为了发挥该将领的才能而特地设置一个适合他的部门。所谓因岗设人，指的是部门编制都是既定的，根据部门的要求来挑选合适的将领。对于这个问题，其实二者并没有冲突。在创业早期，有岗位编制尚未固定，或有大量空缺，为了兼顾将领与岗位职责，故而有因人设岗的情况，若是人才济济，僧多粥少，自然还需要仔细规划岗位，因岗设人。刘邦军中本无大将军一职，有了韩信而专设了大将军一职。<strong>因人设岗只因发生在关键专业关键人才之上，若非大才不应随意的打破组织编制的结构</strong>。</p>
<h2 id="制度与程序"><a href="#制度与程序" class="headerlink" title="制度与程序"></a>制度与程序</h2><p>孙子兵法将“法”列为影响胜败最重要的五个要素之一。法者，曲制、官道、主用也。其内涵包含组织架构、制度程序、权责赏罚。</p>
<h3 id="程序与规范"><a href="#程序与规范" class="headerlink" title="程序与规范"></a>程序与规范</h3><p>组织的意志通过组织架构向下传递，正如大脑通过神经系统控制四肢。这一过程取决于对信息向下的准确传递、子部门的有效运转。而子部门的有效运转依然依赖于信息向子节点的有效传递。组织能够做出正确决策的前提也依赖于各个子部门准确向上反馈的信息。在有些时候，为了能够更快的做出响应，并不需要事事由最高意志做出决定，而是由本部门相机而为，则是为了降低信息处理的延迟。因此信息是组织架构能够正常工作的关键。</p>
<p>人类能够成为万物灵长的原因之一，是因为人有极其强大的信息传递和处理能力，主要表现是语言。当人类发明语言之后，知识就不仅存储在一个人的大脑之中而是可以采用分布式的方式存储在集体的“云存储”上，人类的知识不会因为一个人的死亡而产生非常严重的损失。为了避免口头传播过程中的失真和篡改，人类发明了文字。尤其是汉字系统，不是一个简单的依附于语言的表音符号，因而可以将不同方言甚至不同语系的民族统一到一个文明系统内。秦始皇统一六国之后书同文、车同轨、统一度量衡所起到的作用是信息标准化。古代社会识字率并不高，因此文字起到的作用主要不是教化，而是国家机器的统治工具。</p>
<p>我们今天能够轻松建立起有效的社会组织，主要的原因之一在于先人已经帮我们建立了语言、文字、度量衡等一系列的信息沟通工具。但是在很多时候，日常的语言其信息标准化程度是不够的，过于灵活、自由、随意，因此<strong>我们需要在组织工作中使用特定的专业语言，遵循特定的确认流程</strong>。绝大多数的沟通问题，是因为语言的歧义，其次是缺乏确认反馈。工作中常见的任务指令流程可以约定流程如：发出指令、重复并回复。其他更复杂的流程亦是主要起到确认的作用，避免犯低级的错误。</p>
<p>工作中所用到的信息管理系统，都应有明确的使用规范。包括电子邮件的行文格式，也应该统一。对于一个技术团队，常见的流程有：项目立项流程、需求评审流程、代码提交流程、产品发布流程、服务部署流程。相关规范有：需求描述规范、UI设计规范、代码开发规范、版本管理规范、测试用例规范、Bug描述规范等。对于这些规范，尽量采用业界普遍认可的流程和规范，便于行业人才的交换。随着信息化的发展，也会有越来越多流程规范相关的辅助工具，但这些工具通常会比较灵活以满足不同客户的要求，因此我们在使用中依然要有自己对流程和规范的理解。此外过于繁杂的流程规范亦有可能适得其反，降低信息沟通的效率。<strong>我们应当始终坚持这样的原则：流程与规范应起到了降低出错率、提高信息交流效率的目的</strong>。</p>
<h3 id="控制论与正负反馈"><a href="#控制论与正负反馈" class="headerlink" title="控制论与正负反馈"></a>控制论与正负反馈</h3><p>控制论是研究生物与机械中通讯与控制的科学。控制论认为系统依靠正负两种反馈机制来达到控制自身行为的目的。正如古人认为万物中都存在阴阳二气一样。所谓正反馈指反馈信号与控制信息一致，将增加控制信号的强度。比如当我们发出了进食的指令，而在吃到食物后得到了美味的反馈，促使我们进一步的进食，这就是一个正反馈。但正反馈如果一直持续就会失控，一个典型的正反馈系统是将扬声器和话筒放在一起，就会形成一个增强回路。增强回路是一个很有效的武器，但对于系统控制来说，如果不加控制则有着可怕的后果。如果我们在进食时永远无法停止，那么对人体就是灾难性的。因此我们还需要一个负反馈，所谓负反馈就是指反馈信号与控制信号相反，将减少控制信号的强度。饱腹感就是一种负反馈。对于一辆汽车来说油门与刹车就对应着正负两种正负控制装置，由驾驶员的判断来向受控对象施加这两种反馈来达到驾驶的目的地。</p>
<p>对于一个组织而言，其正负反馈的手段就是赏罚。这一思想由法家提出，颇受诟病，之所以受到批评因为其惩罚过于残酷。后世管理的手段不断文明化，比如伐俸，批评与自我批评，但其本质仍然是赏罚。在现代社会中，鼓励与批评，绩效薪资制度，是最常用的赏罚手段。在一个有着良好团队氛围的环境中，普通的沟通交流，也是一种提供正负反馈的手段。在常规情况下，我们应尽量采用较温和的方式来进行管理，也就是儒家所倡导的己欲立而立人、己欲达而达人。孔子所倡导的治国理念，以提升自身的修养来影响他人，以君子的修身来达到齐家、治国、平天下的目的。用控制论的观点来说，孔子主张使用精神感召力来向系统施加正负反馈，这种精神感召之所以能够发挥作用，按照孟子的解释是因为人有“恻隐、羞恶、恭敬、是非”之心。儒法并用也是中国历史上常用的管理方式，这是文明的体现，无需诟病。</p>
<h3 id="审计与量化"><a href="#审计与量化" class="headerlink" title="审计与量化"></a>审计与量化</h3><p>为了能够更准确的向受控对象施加正负反馈，我们需要准确的了解受控对象。这一了解的过程，即是审计，审计的结果应是量化的。管理者应清晰了解系统的关键指标，并且能够清晰的了解每个指标背后所关联的责任部门。</p>
<h2 id="执行力与方法论"><a href="#执行力与方法论" class="headerlink" title="执行力与方法论"></a>执行力与方法论</h2><p>TODO：计划、冲刺、看板。作战指挥室，战时大脑。</p>
<h2 id="学习与训练"><a href="#学习与训练" class="headerlink" title="学习与训练"></a>学习与训练</h2><p>TODO：传帮带，外聘，轮岗，实践，理论。培养接班人，杰出领导者必须有好的接班人。</p>
<h2 id="决策与谋划"><a href="#决策与谋划" class="headerlink" title="决策与谋划"></a>决策与谋划</h2><p>TODO：</p>
<p>量化，谋，四象限方法论，要事优先（重要的事情优先于紧急的事情）</p>
<p>宏观形势。信息。决策委员会。</p>
<p>矛盾是发展的，抓主要矛盾。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2020/01/13/tao-of-tech-leadership/" class="archive-article-date">
  	<time datetime="2020-01-12T16:19:58.000Z" itemprop="datePublished"><i class="icon-clock"></i>2020-01-13</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/%E9%A2%86%E5%AF%BC%E5%8A%9B/">领导力</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-6" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2020/01/03/introduce-reinforcement-learning-6/">强化学习简介（六）：策略梯度实例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网络在各个动作的Q值是多少。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(PolicyNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.1</span>))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出层需要使用softmax</span></span><br><span class="line">        <span class="keyword">return</span> F.softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>不要忘了输出层的SoftMax。</p>
<h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><p>相对于DQN，我们也不需要额外的目标网络和参数复制操作，只需要一个策略网络即可。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">GAMMA = <span class="number">0.99</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">policy_net = PolicyNet(input_size, HIDDEN_SIZE, n_actions)</span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br></pre></td></tr></table></figure>

<h2 id="选择动作"><a href="#选择动作" class="headerlink" title="选择动作"></a>选择动作</h2><p>在选择动作时，我们不再需要特地设置探索概率，因为输出结果就是各个动作的概率分布。我们使用<code>torch.distributions.categorical.Categorical</code> 来进行取样。在每次选择动作时，我们同时记录对应的概率，以便后续使用。这个概率就是 `ln pi_theta(S_t,A_t)`</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">log_probs = []</span><br><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state),<span class="number">0</span>)</span><br><span class="line">    probs = policy_net(x)</span><br><span class="line">    c = Categorical(probs)</span><br><span class="line">    action = c.sample()</span><br><span class="line">    <span class="comment"># log action probs to plt</span></span><br><span class="line">    prob = c.log_prob(action)</span><br><span class="line">    log_probs.append(prob)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure>

<h2 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h2><p>为了更新参数，我们首先需要计算`v_t`，这在后续参数迭代中需要用到。</p>
<ul>
<li>` v_t = r_(t+1) + gamma * v_(t+1) `</li>
</ul>
<p>在模拟执行的时候，我们记录了每一步的reward，我们需要计算每一步的`v_t`，其顺序与执行顺序一致。根据公式我们需要倒序的计算`v_t`，然后将计算好的结果倒序排列，就形成了`v_1,v_2…v_t`的序列。最后我们需要将数据标准化。(TODO: 这里可能存在一个序列对应的问题，其中每一个状态的累计收益，是后续状态收益之和，不包含本轮收益)</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">values = []</span><br><span class="line">v = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> reward <span class="keyword">in</span> reversed(rewards):</span><br><span class="line">    v = v * GAMMA + reward</span><br><span class="line">    values.insert(<span class="number">0</span>, v)</span><br><span class="line">mean = np.mean(values)</span><br><span class="line">std = np.std(values)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">    values[i] = (values[i] - mean) / std</span><br></pre></td></tr></table></figure>

<p>接下来我们需要更新参数，参数更新的公式为：</p>
<ul>
<li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li>
</ul>
<p>我们将其转换为损失函数形式:</p>
<ul>
<li>` L(theta) = - v_t ln pi_theta(A_t|S_t) `</li>
</ul>
<p>这个损失函数的形式可以帮助我们更好的理解策略梯度的原理。如果一个动作价值为负值，但是其选择概率为正，则损失较大。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.random.choice(size, n):</span><br><span class="line">    loss.append(- values[i] * log_probs[i])</span><br><span class="line">loss = torch.cat(loss).sum()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">    param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>训练循环需要在一局结束之后进行。并清除rewards、log_probs缓存。对于cartpole-v1环境，要注意他的每一步奖励都是1，很显然在最后一步代表着游戏失败，我们需要施加一定的惩罚，我们将最后一步的奖励设为-100。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">5000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state)</span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward, done,_ = env.step(action.item())</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt;= <span class="number">2500</span>:</span><br><span class="line">            optimize_model()</span><br><span class="line">            print(<span class="string">'EP'</span>, i_episode)</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            rewards = []</span><br><span class="line">            log_probs = []</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure>

<p><img src="/files/cart-pg.png" alt="Clamp"></p>
<p><a href="/files/demo_dqn.py">完整代码</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2020/01/03/introduce-reinforcement-learning-6/" class="archive-article-date">
  	<time datetime="2020-01-03T06:40:51.000Z" itemprop="datePublished"><i class="icon-clock"></i>2020-01-03</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-5" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/30/introduce-reinforcement-learning-5/">强化学习简介（五）：策略梯度Policy Gradient</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问题，而Q值的目的也是为了选择动作。我们可以将增加学习的问题简化为选择动作的问题。那么我们可否使用深度学习直接做出动作选择呢？显然，我们可以定义一个网络`pi_theta`，其中输入为状态`s`，输出为每个动作`a`的概率。</p>
<p><img src="/img/rl-5/1.png" alt="策略梯度"></p>
<p>因为这个网络与策略函数的定义一样，所以被称为策略网络。`pi_theta(a|s)`，表示在`s`状态下选择动作`a`的概率。只要这个网络能够收敛，我们就可以直接得到最佳策略。这个网络的奖励函数也就是最终游戏的总奖励。</p>
<p>`J(theta) = sum_(s in S)d^pi(s)V^pi(s) = sum_(s in S)d^pi(s)sum_(a in A)pi_theta(a|s)Q^pi(s, a)`</p>
<p>`d^pi(s)`指状态`s`在马尔科夫链上的稳定分布，`d^pi(s) = lim_(t-&gt;oo)P(s_t=s|s_0,pi_theta)`。</p>
<p>但是这个表达式看上去是不可能计算的，因为状态的分布和Q值都是随着策略的更新而不断变化的。但是我们并不需要计算`J(theta)`，在梯度下降法中我们只需要计算梯度`grad_(theta)J(theta)`即可</p>
<p>`grad_(theta)V^pi(s)`<br>`= grad_(theta)(sum_(a in A)pi_theta(a|s)Q^pi(s, a))`<br>根据导数乘法规则<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_thetaQ^pi(s, a))`<br>展开`Q^pi(s,a)`为各各种可能的下一状态奖励之和<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_(theta)sum_(s’,r)P(s’,r|s,a)(r+V^pi(s’)))`<br>而其中状态转移函数`P(s’,r|s,a)`、奖励`r`由环境决定，与`grad_theta`无关，所以<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’,r)P(s’,r|s,a)grad_(theta)V^pi(s’))`<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p>
<p>现在我们有了一个形式非常好的递归表达式：<br>`grad_(theta)V^pi(s) = sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p>
<p>设 `rho^pi(s-&gt;x, k)` 表示在策略`pi^theta`下，`k`步以后状态`s`转移到状态`x`的概率。有：</p>
<ul>
<li>`rho^pi(s-&gt;s, k=0)=1`</li>
<li>`rho^pi(s-&gt;s’, k=1)=sum_(a)pi_(theta)(a|s)P(s’|s,a)`</li>
<li>`rho^pi(s-&gt;x, k+1) = sum_(s’)rho^pi(s-&gt;s’, k)rho^pi(s’-&gt;x, 1)`</li>
</ul>
<p>为了简化计算，令 `phi(s)=sum_(a in A)grad_(theta)pi_theta(a|s)Q^pi(s,a)`</p>
<p>`grad_(theta)V^pi(s)`<br>`= phi(s) + sum_(a in A)pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)sum_(a in A)pi_(theta)(a|s)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)(phi(s’) + sum_(s’’)rho^pi(s’-&gt;s’’,1)grad_(theta)V^pi(s’’)) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)grad_(theta)V^pi(s’’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)phi(s’’) + sum_(s’’’)rho^pi(s-&gt;s’’’,3)grad_(theta)V^pi(s’’’) `<br>`= …`<br>`= sum_(x in S)sum_(k=0)^(oo)rho^pi(s-&gt;x, k)phi(x)`</p>
<p>令 `eta(s)=sum_(k=0)^(oo)rho^pi(s_0-&gt;s, k)`</p>
<p>`grad_(theta)J(theta)=grad_(theta)V^pi(s_0)`<br>`= sum_(s)sum_(k=0)^(oo)rho^pi(s_0-&gt;s,k)phi(s)`<br>`= sum_(s)eta(s)phi(s)`<br>`= (sum_(s)eta(s))sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `sum_(s)eta(s)` 属于常数，对于求梯度而言常数可以忽略。<br>`prop sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `eta(s)/(sum_(s)eta(s))`表示`s`的稳定分布<br>`= sum_(s)d^pi(s)sum_a grad_(theta)pi_(theta)(a|s)Q^pi(s,a)`<br>`= sum_(s)d^pi(s)sum_a pi_(theta)(a|s)Q^pi(s,a)(grad_(theta)pi_(theta)(a|s))/(pi_(theta)(a|s))`<br>因 ` (ln x)’ = 1/x `<br>`= Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)]`</p>
<p>所以得出策略梯度最重要的定理：</p>
<p>` grad_(theta)J(theta)=Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)] `</p>
<p>其中的`Q^pi(s,a)`也就是状态s的累计收益，可以在一次完整的动作轨迹中累计计算得出。</p>
<h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>该算法被称为 REINFORCE</p>
<ul>
<li>随机初始化`theta`</li>
<li>生成一个完整的策略`pi_theta`的轨迹: `S1,A1,R2,S2,A2,…,ST`。</li>
<li>For t=1, 2, … , T-1:<ul>
<li>` v_t = sum_(i=0)^(oo) gamma^i R_(t+i+1) `</li>
<li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li>
</ul>
</li>
</ul>
<p>参考：<br><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Lilian Weng:Policy Gradient Algorithms</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/30/introduce-reinforcement-learning-5/" class="archive-article-date">
  	<time datetime="2019-12-30T06:37:07.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-30</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-4" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/30/introduce-reinforcement-learning-4/">强化学习简介（四）：DQN实战</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用<a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank" rel="noopener">CartPole-v1</a>。我们的输入是一幅图片，动作是施加一个向左向右的力量，我们需要尽可能的保持木棍的平衡。</p>
<p><img src="/files/cartpole-v1.gif" alt="CartPole-v1"></p>
<p>对于这个环境，尝试了很多次，总是不能达到很好的效果，一度怀疑自己的代码写的有问题。后来仔细看了这个环境的奖励，是每一帧返回奖励1，哪怕是最后一帧也是返回1 的奖励。这里很明显是不合理的俄。我们需要重新定义这个奖励函数，也就是在游戏结束的时候，给一个比较大的惩罚，r=-100。很快可以达到收敛。</p>
<h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>, (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.mem = []</span><br><span class="line">        self.pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Save a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.mem) &lt; self.cap:</span><br><span class="line">            self.mem.append(<span class="keyword">None</span>)</span><br><span class="line">        self.mem[self.pos] = Transition(*args)</span><br><span class="line">        self.pos = (self.pos + <span class="number">1</span>) % self.cap</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.mem, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.mem)</span><br></pre></td></tr></table></figure>

<h2 id="Q网络"><a href="#Q网络" class="headerlink" title="Q网络"></a>Q网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure>

<h2 id="初始化参数和状态"><a href="#初始化参数和状态" class="headerlink" title="初始化参数和状态"></a>初始化参数和状态</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络</span></span><br><span class="line">policy_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络</span></span><br><span class="line">target_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络从策略网络复制参数</span></span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure>

<h2 id="探索和选择最佳动作"><a href="#探索和选择最佳动作" class="headerlink" title="探索和选择最佳动作"></a>探索和选择最佳动作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state, no_explore=False)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold <span class="keyword">or</span> no_explore:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(x).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)</span><br></pre></td></tr></table></figure>

<h2 id="优化模型-关键代码"><a href="#优化模型-关键代码" class="headerlink" title="优化模型(关键代码)"></a>优化模型(关键代码)</h2><p>这里主要是抽样、目标值计算、损失计算的部分。损失计算采用Huber loss。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span>  s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>,</span><br><span class="line">                        batch.next_state)), dtype=torch.uint8)</span><br><span class="line">    non_final_next_states = torch.FloatTensor([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>])</span><br><span class="line">    state_batch = torch.FloatTensor(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.FloatTensor(batch.reward)</span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 限制网络更新的幅度，可以大幅提升训练的效果。</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure>

<h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>这里主要有主循环、获取输入、记录回放、训练、复制参数等环节。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state, i_episode%<span class="number">50</span>==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(i_episode%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward,done,_ = env.step(action.item())</span><br><span class="line">        <span class="comment"># reward = torch.tensor([reward])</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt; <span class="number">2500</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                optimize_model()</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br></pre></td></tr></table></figure>

<p><img src="/files/dqn2.png" alt="Clamp"></p>
<p><a href="/files/demo_dqn.py">完整代码</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/30/introduce-reinforcement-learning-4/" class="archive-article-date">
  	<time datetime="2019-12-30T06:37:03.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-30</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-3" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/28/introduce-reinforcement-learning-3/">强化学习简介（三）：DQN</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学习就有了施展的空间了。</p>
<p>Q表存储着状态s和动作a、奖励r的信息。我们知道深度神经网络，也具有存储信息的能力。DQN算法就是将Q-table存储结构替换为神经网络来存储信息。我们定义神经网络`f(s, w) ~~ Q(s)`，输出为一个向量`[Q(s, a_1), Q(s, a_2), Q(s, a_3), …, Q(s, a_n)]`。经过这样的改造，我们就可以用Q-learing的算法思路来解决更复杂的状态空间的问题了。我们可以通过下面两张图来对比Q-learning和DQN的异同。</p>
<p><img src="/img/rl-3/1.png" alt="Q-learning"></p>
<p><img src="/img/rl-3/2.png" alt="Deep-Q-learning"></p>
<p>网络结构要根据具体问题来设计。在神经网络训练的过程中，损失函数是关键。我们采用MSE来计算error。</p>
<p>`L(w) = (ubrace(r + argmax_aQ(s’, a’))_(目标值) - ubrace(Q(s, a))_(预测值))^2`</p>
<h2 id="基本算法描述"><a href="#基本算法描述" class="headerlink" title="基本算法描述"></a>基本算法描述</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">使用随机参数初始化网络Q</span><br><span class="line"><span class="keyword">while</span> 未收敛:</span><br><span class="line">  action 按一定概率随机选取，其余使用argmax Q(s)选取</span><br><span class="line">  模拟执行 action 获取 状态 s_, 奖励 r, 是否完成 done</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    target = r</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    target = r + gamma * argmax Q(s_)</span><br><span class="line">  loss = MSE(target, Q(s, a))</span><br><span class="line">  使用loss更新网络Q</span><br><span class="line">  s = s_</span><br></pre></td></tr></table></figure>

<p>但是，通过实验我们会发现，训练过程非常的不稳定。稳定性是强化学习所面临的主要问题之一，为了达到稳定的训练我们需要运用一些优化的手段。</p>
<h2 id="环境的稳定性"><a href="#环境的稳定性" class="headerlink" title="环境的稳定性"></a>环境的稳定性</h2><p>Agent生活在环境之中，并根据环境的反馈进行学习，但环境是否是稳定的呢？假设agent在学习出门穿衣的技能，它需要学会在冬天多穿，夏天少穿。但是这个agent只会根据当天的反馈来修正自己的行为，也就是说这个agent是没有记忆的。那么这个agent就会在多次失败后终于在冬天学会了多穿衣，但转眼之间到了夏天他又会陷入不断的失败，最终他在夏天学会了少穿衣之后，又会在冬天陷入失败，如此循环不断，永远不会收敛。如果要能够很好的训练，这个agent至少要有一整年的记忆空间，每一批都要从过去的记忆中抽取记忆来进行训练，就可以避免遗忘过去的教训。</p>
<p>在DeepMind的Atari 论文中提到</p>
<blockquote>
<p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p>
</blockquote>
<p>意思是，受生物学启发，他们采用了一种叫做经验回放（experience replay）的机制，随机抽取数据来到达“移除观察序列的相关性，平滑数据分布的改变”的目的。<br><img src="/img/rl-3/4.png" alt="DQN with Atari"></p>
<p>我们已经理解了要有经验回放的记忆，但是为什么一定要随机抽取呢？对此论文认为这个随机抽取可以移除序列相关性、平滑分布中的改变。当如何理解呢？简单的说就是在我们不清楚合理的周期的情况下，能够保证采样的合理性。我们仍然以四季穿衣举例，假设我们不使用随机采样，我们必须在每次训练中都采用365天左右的数据，才能使我们的数据样本分布合理。可是agent并不清楚一年365天这个规律，这恰恰是我们所要学习的内容。采用随机采用，就可以自然的做到数据的分布合理，而且只需要使用记忆中的部分数据，减少单次迭代的计算量。</p>
<p>在这个记忆里，我们并不记录当时的网络参数（分析过程），我们只记录（状态s，动作a，新状态s’, 单步奖励r)。显然，记忆的尺寸不可能无限大。当记忆体增大到一定程度之后，我们采用滚动的方式用最新的记忆替换掉最老的记忆。就像在学习围棋的过程中，有初学者阶段的对局记忆，也有高手阶段的对局记忆，在提升棋艺的角度来看，高手阶段的记忆显然比初学者阶段的记忆更有价值。</p>
<p>说句题外话，其实对于一个民族而言也是一样的。我们这个民族拥有一个非常好的传统，就是记述历史，也就是等于我们这个民族拥有足够大的记忆量，这是我们胜于其他民族的。但是这个历史记录中，掺杂了历史上不同阶段的评价，这些评价是根据当时的经验得出的。而根据DQN的算法描述来看，对我们最有价值的部分其实是原始信息，而不是那些附加在之上的评价，这些评价有正确的部分，也有错误的部分，我们不用去过多关心。我们只需要在今天的认知（也就是最新的训练结果）基础上，对历史原始信息（旧状态、动作、新状态、单步奖励）进行随机的抽样分析即可。</p>
<h2 id="网络稳定性"><a href="#网络稳定性" class="headerlink" title="网络稳定性"></a>网络稳定性</h2><p>DQN另一个稳定性问题与目标值计算有关。因为`target = r + gamma * argmax Q(s’)`，所以目标值与网络参数本身是相关，而参数在训练中是不断变化的，所以这会造成训练中的不稳定。一个神经网络可以自动收敛，取决于存在一个稳定的目标，如果目标本身在不断的游移变动，那么想要达到稳定就比较困难。这就像站在平地上的人很容易平衡，但如果让人站在一个不断晃动的木板上，就很难达到平衡。为了解决这个问题，我们需要构建一个稳定的目标函数。</p>
<p>解决的方法是采用两个网络代替一个网络。一个网络用于训练调整参数，称之为策略网络，另一个专门用于计算目标，称之为目标网络。目标网络与策略网络拥有完全一样的网络结构，在训练的过程中目标网络的参数是固定的。执行一小批训练之后，将策略网络最新的参数复制到目标网络中。</p>
<p><img src="/img/rl-3/3.png" alt="目标网络"></p>
<p>经验回放和目标网络的效果见下表（引用自Nature 论文）：<br><img src="/img/rl-3/5.png" alt="优化对比"></p>
<h2 id="其他DQN优化"><a href="#其他DQN优化" class="headerlink" title="其他DQN优化"></a>其他DQN优化</h2><p>关于DQN的优化，这篇文章描述的比较全面 <a href="https://zhuanlan.zhihu.com/p/21547911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21547911</a>。在之后的实践中考虑是否进一步深入。主要介绍3个改进：</p>
<p><img src="/img/rl-3/6.png" alt="DQN优化"></p>
<ul>
<li>Double DQN：对目标值计算的优化，a’使用策略网络选择的动作来代替目标网络选择的动作。</li>
<li>Prioritised replay：使用优先队列（priority queue）来存储经验，避免丢弃早期的重要经验。使用error作为优先级，仿生学技巧，类似于生物对可怕往事的记忆。</li>
<li>Dueling Network：将Q网络分成两个通道，一个输出V，一个输出A，最后再合起来得到Q。如下图所示（引用自Dueling Network论文）。这个方法主要是idea很简单但是很难想到，然后效果一级棒，因此也成为了ICML的best paper。</li>
</ul>
<p><img src="/img/rl-3/7.png" alt="Dueling Network"></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/28/introduce-reinforcement-learning-3/" class="archive-article-date">
  	<time datetime="2019-12-28T03:53:37.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-28</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-2" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/27/introduce-reinforcement-learning-2/">强化学习简介（二）：Q-learning实战</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用<code>gym</code>来作为我们对实验环境。安装方法：</p>
<figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym</span><br></pre></td></tr></table></figure>

<h2 id="初识环境"><a href="#初识环境" class="headerlink" title="初识环境"></a>初识环境</h2><p>我们的实验环境是一个冰湖滑行游戏，你将控制一个agent在冰面到达目标终点，前进方向并不总受你的控制，你还需要躲过冰窟。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># 构造游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"><span class="comment"># 动作空间-&gt; Discrete(4)</span></span><br><span class="line">print(env.action_space)</span><br><span class="line"><span class="comment"># 状态空间-&gt; Discrete(16)</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line"><span class="comment"># 初始化游戏环境，并得到状态s</span></span><br><span class="line">s = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 渲染游戏画面</span></span><br><span class="line">    env.render()</span><br><span class="line">    <span class="comment"># 从动作空间中随机选择一个动作a</span></span><br><span class="line">    a = env.action_space.sample()</span><br><span class="line">    <span class="comment"># 执行动作a，得到新状态s，奖励r，是否完成done</span></span><br><span class="line">    s, r, done, info = env.step(a) <span class="comment"># take a random action</span></span><br><span class="line">    print(s, r, done, info)</span><br><span class="line"><span class="comment"># 关闭环境</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>

<p>游戏画面示意如下：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SFFF       (S: 起点，安全)</span><br><span class="line">FHFH       (F: 冰面，安全)</span><br><span class="line">FFFH       (H: 冰窟，进入则失败)</span><br><span class="line">HFFG       (G: 终点，到达则成功)</span><br></pre></td></tr></table></figure>

<h2 id="Agent结构"><a href="#Agent结构" class="headerlink" title="Agent结构"></a>Agent结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLAgent</span><span class="params">()</span>:</span></span><br><span class="line">    q = <span class="keyword">None</span></span><br><span class="line">    action_space = <span class="keyword">None</span></span><br><span class="line">    epsilon = <span class="number">0.1</span> <span class="comment"># 探索率</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># 衰减率</span></span><br><span class="line">    lr = <span class="number">0.1</span> <span class="comment"># 学习率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, action_space, state_count, epsilon=<span class="number">0.1</span>, lr=<span class="number">0.1</span>, gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.q = [[<span class="number">0.</span> <span class="keyword">for</span> a <span class="keyword">in</span> range(action_space.n)] <span class="keyword">for</span> s <span class="keyword">in</span> range(state_count)]</span><br><span class="line">        self.action_space = action_space</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据状态s，选择动作a</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新状态变化并学习，状态s执行了a动作，得到了奖励r，状态转移到了s_</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure>

<p>这是一个Agent的一般结构，主要由初始化、选择动作、更新状态变化，三个方法构成。后续的其他算法将依然采用该结构。q表数据使用一个二维数组表示，其大小为 state_count action_count，对于这个项目而言是一个 `16*4` 的大小。</p>
<h2 id="添加Q-table的辅助方法"><a href="#添加Q-table的辅助方法" class="headerlink" title="添加Q-table的辅助方法"></a>添加Q-table的辅助方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回状态s的最佳动作a、及其r值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    max_r = -math.inf</span><br><span class="line">    max_a = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(self.action_space.n):</span><br><span class="line">        r = self.q_get(s, a)</span><br><span class="line">        <span class="keyword">if</span> r &gt; max_r:</span><br><span class="line">            max_a = a</span><br><span class="line">            max_r = r</span><br><span class="line">    <span class="keyword">return</span> max_a, max_r</span><br><span class="line"><span class="comment"># 获得 状态s，动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_get</span><span class="params">(self, s, a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.q[s][a]</span><br><span class="line"><span class="comment"># 更新 状态s 动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_put</span><span class="params">(self, s, a, v)</span>:</span></span><br><span class="line">    self.q[s][a] = v</span><br></pre></td></tr></table></figure>

<h2 id="Q-learning的关键步骤"><a href="#Q-learning的关键步骤" class="headerlink" title="Q-learning的关键步骤"></a>Q-learning的关键步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="comment"># 按一定概率进行随机探索</span></span><br><span class="line">        <span class="keyword">return</span> self.action_space.sample()</span><br><span class="line">    <span class="comment"># 返回最佳动作</span></span><br><span class="line">    a, _ = self.argmax(s)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">    q = self.q_get(s, a)</span><br><span class="line">    _, r_ = self.argmax(s_)</span><br><span class="line">    <span class="comment"># Q &lt;- Q + a(Q' - Q)</span></span><br><span class="line">    <span class="comment"># &lt;=&gt; Q &lt;- (1-a)Q + a(Q')</span></span><br><span class="line">    q = (<span class="number">1</span>-self.lr) * q + self.lr * (r + self.gamma * r_)</span><br><span class="line">    self.q_put(s, a, q)</span><br></pre></td></tr></table></figure>

<h2 id="训练主循环"><a href="#训练主循环" class="headerlink" title="训练主循环"></a>训练主循环</h2><p>我们进行10000局游戏的训练，每局游戏执行直到完成。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line">agent = QLAgent(env.action_space, env.observation_space.n)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># env.render()  # 训练过程不需要渲染</span></span><br><span class="line">        a = agent.choose_action(s) <span class="comment"># 选择动作</span></span><br><span class="line">        s_, r, done, info = env.step(a) <span class="comment"># 执行动作</span></span><br><span class="line">        agent.update_transition(s, a, r, s_) <span class="comment"># 更新状态变化</span></span><br><span class="line">        s = s_</span><br><span class="line"><span class="comment"># 显示训练后的Q表</span></span><br><span class="line">print(agent.q)</span><br></pre></td></tr></table></figure>

<h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><p>在测试中，我们只选择最佳策略，不再探索，也不再更新Q表。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获胜次数</span></span><br><span class="line">total_win = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># 选择最佳策略</span></span><br><span class="line">        a, _ = agent.argmax(s)</span><br><span class="line">        <span class="comment"># 执行动作 a</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">and</span> r == <span class="number">1</span>:</span><br><span class="line">            total_win += <span class="number">1</span></span><br><span class="line">        s = s_</span><br><span class="line">print(<span class="string">'Total win='</span>, total_win)</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure>

<p>最终测试的效果是在1万局中获胜了7284次，说明达到了不错的实验效果。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/27/introduce-reinforcement-learning-2/" class="archive-article-date">
  	<time datetime="2019-12-27T08:02:19.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-27</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-introduce-reinforcement-learning-1" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/27/introduce-reinforcement-learning-1/">强化学习简介（一）：Q-learning</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。</p>
<p><img src="/img/rl-1/4.png" alt="强化学习图示"></p>
<h2 id="K摇臂赌博机"><a href="#K摇臂赌博机" class="headerlink" title="K摇臂赌博机"></a>K摇臂赌博机</h2><p>我们可以考虑一个最简单的环境：一个动作可立刻获得奖励，目标是使每一个动作的奖励最大化。对这种单步强化学习任务，可以设计一个理论模型——“K-摇臂赌博机”。这个赌博机有K个摇臂，赌徒在投入一个硬币后可一选择按下一个摇臂，每个摇臂以一定概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。最终所获得的总奖励被称为累计奖励。</p>
<p><img src="/img/rl-1/2.jpg" alt="K摇臂赌博机"></p>
<p>对于这个简单模型，若要知道每个摇臂的概率，我们只需要进行足够多的尝试即可，这是“仅探索”策略；若要奖励最大化，则需要执行奖赏概率最大的动作即可，这是“仅利用”策略。但在更复杂的环境中，我们不可能对每一个状态的每个动作都进行足够多的探索。比如围棋，我们后续的探索是需要依赖于之前的探索的。因此我们需要在探索和利用之间进行平衡。我们在学习的过程中，必须要保持一定的概率`epsilon`进行探索，其余时候则执行学习到的策略。</p>
<h2 id="基本概念术语"><a href="#基本概念术语" class="headerlink" title="基本概念术语"></a>基本概念术语</h2><p>为了便于分析讨论，我们定义一些术语。</p>
<ul>
<li><p>机器agent处于环境`E`中。</p>
</li>
<li><p>状态空间为`S`，每个状态`s in S`是机器感知到的环境描述。</p>
</li>
<li><p>机器能够采取的可采取的动作a的集合即动作空间`A`，`a in A`。</p>
</li>
<li><p>转移函数`P`表示：当机器执行了一个动作`a`后，环境有一定概率从状态`s`改变为新的状态`s’`。即：`s’=P(s, a)`</p>
</li>
<li><p>奖赏函数`R`则表示了执行动作可能获得的奖赏`r`。即：`r=R(s,a)`。</p>
</li>
<li><p>环境可以描述为`E=&lt;&lt;S, A, P, R&gt;&gt;`。</p>
</li>
<li><p>强化学习的任务是习得一个策略（policy）`pi`，使机器在状态`s`下选择到最佳的`a`。策略有两种描述方法：</p>
<ol>
<li>`a=pi(s)` 表示状态`s`下将执行动作`a`，是一种确定性的表示法。</li>
<li>`pi(s, a)` 表示状态`s`下执行动作`a`的概率。这里有 `sum_a pi(s,a)=1`</li>
</ol>
</li>
<li><p>累计奖励指连续的执行一串动作之后的奖励总和。</p>
</li>
<li><p>`Q^(pi)(s, a)`表示在状态`s`下，执行动作`a`，再策略`pi`的累计奖励。为方便讨论后续直接写为`Q(s,a)`。</p>
</li>
<li><p>`V^(pi)(s)` 表示在状态`s`下，使用策略`pi`的累计奖励。为方便讨论后续直接写为`V(s)`。</p>
</li>
</ul>
<p>强化学习往往不会立刻得到奖励，而是在很多步之后才能得到一个诸如成功/失败的奖励，这是我们的算法需要反思之前所有的动作来学习。所以强化学习可以视作一种延迟标记的监督学习。</p>
<h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>对于我们要学习的`Q(s,a)`函数，我们可以使用一个Q-table来表示。Q-table是一个二维表，记录每个状态`s in S, a in A`的`Q`值。Q表被初始化为0的状态。在状态`s`执行了动作`a`之后，得到状态`s’`，奖励`r`。我们将潜在的`Q`函数记为`Q_(real)`，其值为当前奖励r与后续状态`s’`的最佳累计奖励之和。则有：</p>
<p>` Q_(real)(s, a) = r + gamma * argmax_aQ(s’, a) `<br>` err = Q_(real)(s, a) - Q(s, a) `</p>
<p>其中`gamma`为`Q`函数的偏差，`err`为误差，`alpha`为学习率。 可得出更新公式为：</p>
<p>` Q(s, a) leftarrow Q(s, a) + alpha*err `<br>即：<br>` Q(s,a) leftarrow (1-alpha)Q(s,a) + alpha(r + gamma * argmax_aQ(s’, a)) `</p>
<p>以上公式即为Q-learning的关键</p>
<h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化Q表</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    Q(s, a) = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">  <span class="keyword">if</span> rand() &lt; epsilon:</span><br><span class="line">    a = 从 A 中随机选取一个动作</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    a = 从 A 中选取使 Q(a) 最大的a</span><br><span class="line">  <span class="comment"># 从环境中获得反馈</span></span><br><span class="line">  r = R(s, a)</span><br><span class="line">  s1 = P(s, a)</span><br><span class="line">  <span class="comment"># 更新Q表</span></span><br><span class="line">  Q(s,a) = (<span class="number">1</span>-alpha)*Q(s,a) + alpha * (r + gamma * argmax Q(s1, a) - Q(s, a))</span><br><span class="line">  s = s1</span><br></pre></td></tr></table></figure>

<p>下图是一个Q表内存结构，在经过一定的学习后，Q表的内容将能够不断逼近每个状态的每个动作的累计收益。<br><img src="/img/rl-1/3.png" alt="Q-table"></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/27/introduce-reinforcement-learning-1/" class="archive-article-date">
  	<time datetime="2019-12-27T03:28:16.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-27</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-2019-new-thinking" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/25/2019-new-thinking/">2019年度回顾：新思潮的前夜</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>过去一年，事业上没多少进步，思想上有贯通之感。这些“无用之学”是一年的无所事事换来，所以需要记录一下。</p>
<p>今年不再听得到，改听微信读书。办了张图书馆的借书证。主要知识来源：书、知乎、B站。排序与阅读顺序无关</p>
<p>与同学论战，论战过程中不断提高自己的理论水平，发现有很多东西虽然有概念，如果缺乏实证没有说服力。世界银行提供了很多经济数据，很有价值，如GDP、PPP、Gini系数等。</p>
<p>读冯友兰的《中国哲学史》，思考中国哲学所具有的普世意义。温习《庄子》《墨子》的部分内容。尝试论证人生的意义，一个可以被自己接受的人生意义。</p>
<p>B站看秦晖讲的《中国哲学史》，其以群己界限、乡举里选、儒法斗争为叙述线索，似有可取之处，但其对民族性论述不以为然。新文化运动以来诸如胡适、鲁迅，皆有可取之处，却其对民族性的贬低又有精日、西奴之嫌。</p>
<p>读了《夏禹神话研究》。了解相关知识，对五帝、夏禹历史尝试进行分析，梳理一个叙事结构。录了几个关于五帝抖音视频，暂停了。</p>
<p>进一步了解了地理环境决定论，读《枪炮病菌与钢铁》。</p>
<p>了解加州学派的主要观点，读了《白银资本》《火枪与账簿》全球史视角下的东西方对比（明清时期）。</p>
<p>了解了明史，尤其是晚明史，读了《万历十五年》，《剑桥中国明代史》，B站看张西平、商传等学者的视频，重新认识了明朝。</p>
<p>了解东林党的历史和评价，褒贬不一。猜想：东林岳麓两家书院，传承中华文明，他们的学生从明末到民国，风格迥异。常凯申是江浙人，属于东林一脉，毛主席是湖南人，属于岳麓一脉。</p>
<p>了解了中学西传，启蒙运动与中国哲学的关系。西方近代哲学主要发端于启蒙运动，那么西方哲学到底是继承自中学西传，还是继承自基督教，还是继承自古希腊？这个问题引人深思。</p>
<p>读了白云先生的文章，虽有偏激之辞，但格局极大，有打通任督二脉之感。尤其是关于韩愈-陈抟-朱熹-宋明理学-王夫之-曾国藩、杨昌济、毛泽东的中华道统传承的论述。</p>
<p>了解王夫之生平，对此先贤之前无甚了解。他的《读通鉴论》看了一节。钦佩。读《曾国藩传》。</p>
<p>读《天朝的崩溃》，思考满清的农奴式统治的影响，新文化运动批评的民族性，其实是满遗余孽。</p>
<p>在知乎上回答了李约瑟难题，基本认为李约瑟难题是个伪命题。</p>
<p>读了《毛泽东自传》《毛泽东传》，一个越了解越觉得伟大的人，千古无二，真神，远胜佛祖、摩西之流。相信毛主义会再次回归神坛。</p>
<p>读了温铁军的《八次危机》，认识到前三十年的巨大成就和困难，以及后三十年的巨大经济与社会问题。</p>
<p>在读《临高启明》，一群工业党写的。了解现代国家形成的过程，工业化的巨大困难，以及工业化完成后将带来财富激增。</p>
<p>看了《无悔追踪》《一年又一年》《天道》这三部连续剧可以串起中国几十年民众的生活状态。</p>
<p>读了《战后日本经济史》，德国、苏联、日本 的崛起都有社会主义/国家资本主义的原因，与自由市场经济无关。而那些鼓吹自由市场理论的，都是在完成工业化后，才开始采用自由市场，工业化前都有压榨式的原始积累过程。上学时没学好马哲，现在要补习。</p>
<p>读了《大国悲剧：苏联解体的前因后果》，看了HBO电视剧《切尔诺贝利》。</p>
<p>了解中国在冷战中的地位与策略。战后历史就是中美苏的斗争历史，其他国家忙着搭便车。</p>
<p>看了《切腹》《寄生虫》《美国工厂》这几部电影，无产阶级无处不在啊。</p>
<h2 id="今年"><a href="#今年" class="headerlink" title="今年"></a>今年</h2><p>华为被视为民族英雄，联想被视作美帝买办。</p>
<p>非洲猪瘟，猪肉价格上涨。</p>
<p>香港动乱。</p>
<p>王健林内贷外投，转移财产。</p>
<p>国庆大阅兵，民心振奋。</p>
<p>华为胡玲发帖、251事件。</p>
<p>抖音上忽然走红 沈巍（流浪的大师）、李子柒（世外桃源）、牧马人（文革晚期、改革初期的淳朴生活）。</p>
<p>中美贸易战接近收官。</p>
<p>世界各地区社会运动不断。</p>
<h2 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h2><p>我们正站在世界巨变的前夜，这个巨变将以我们意向不到的方式出现。</p>
<p>美国会倒下，但我们不知道美国将会如何倒下。中美最根本的战争在金融领域，人民币国际化就是在争夺全球铸币权份额。美元可能会崩溃，但中国拥有的大量美元资产需要安全置换。如果美元霸权能够缓缓的衰落，对中国的影响最小，如果美元霸权骤然终结，也将对中国带来冲击。</p>
<p>在现在的国际环境下，依靠出口拉动经济的方式是不可持续的，最重要的是提振内需，但目前老百姓手里没钱，内需也拉不动。所以现在必须要平衡贫富差距。缩小贫富差距就要有人掏钱，因此有钱的人都想着抽逃资金。严打不仅稳定社会治安，也会收回大量不法收入。</p>
<p>房价不会大涨也不会大跌，上涨会让还没买房的人不满意，下跌会让已经买了房的人不满意，所以就锁住，不涨不跌。房子已经进入了计划时代，就像火车票必须凭身份证购买来抑制黄牛是一样的。信息化使计划经济具有了更大的可行性。即使通货膨胀了，房价也不会涨，因为已经不再是市场化环境了。所以本质上，房价在缓缓下跌。</p>
<p>需要优化财富分配，医疗养老之类的福利会加强，教育科研投入会加大，贫困人口持续扶贫。</p>
<p>大量高科技依然掌握在欧美手中。中国的普及教育很好，但是高等教育与欧美差距较大，教育改革也是困难重重。仅从本人专业来看，编程语言、操作系统、深度学习框架，都是美国人的。</p>
<p>生产过剩、人工智能，会影响就业率。新增人口下滑，老龄化加速。</p>
<p>创业会越来越难，基本上属于解决就业的公益事业。体制内、大平台的职工相当于新的铁饭碗，同时临时工也会越来越多。</p>
<p>创业的最佳阶段是GDP高速增长的阶段，只要入局基本都有得赚。如果宏观上的财富没有增加，那么创业相当于是在存量市场与其他人竞争。除非有压倒性的武器，找到可以战胜的敌人，否则不要创业。海外市场机会较大。</p>
<p>创新是有闲阶级特权，有闲不一定是非常富有，但需要有一份保证基本生活的、时间投入少的收入，即“睡后收入”。如果创新可立刻获得回报，则可以进入增强回路循环。</p>
<p>随着机器人取代越来越多的低端就业，UBI即无条件基本收入的概念会被越来越多的人接受。今年参加美国总统竞选的杨安泽使用了UBI的口号，别人问他钱从哪来，他说向富人收税。桑德斯再次参加大选，美国民主社会主义近年快速崛起。</p>
<p>传统发达国家因为后发国家的追赶，导致本国产业受到影响，就业率下滑。从而产生越来越多的社会运动，希望获得更高的社会福利，有的地方甚至为了几毛钱的地铁涨价就闹了起来。但这些国家由于债务压力已经很大，经济环境恶化，无法给到更多的社会福利。富人们则利用国际避税手段，来躲避社会责任。一场全球性的萧条和左派革命正在酝酿，那些小政府的发达地区问题最严重（比如香港、韩国是小政府、新加坡是大政府）。</p>
<p>中美的竞争不会转化为热战，因为中国不想打，美国打不赢。中美握手后，将联手打击国际避税，共治天下。</p>
<p>资本因其可以转移，在国际共运中，虽然一部分资产被没收，但更多的则逃到了避风港中。苏联解体、改革开放也使大量的公有资产再次被私有化。美国衰落将使资本最大的避风港消失，所有的资本都将处于监管之下。</p>
<p>中国国内思想分歧增大。近几十年的高速增长，右派认为前人有罪，自己有功，而问题则留给后人处理，大力宣传，所以右派思想居主流。近十年文革一代领导人上台，左派思想逐渐兴起，加之贫富差距、环保、贪腐等问题的存在，助推了左派思潮的壮大。我也是因为近几年的反腐、扶贫、强军才开始重新认识这个国家。</p>
<p>如果说这一代领导人的思想是在文革中形成的话，那么下一届领导人的思想又是在何时形成的呢？文革给中国续了命，修正主义则给苏联送了命。戈尔巴乔夫说“我们是苏共二十大的孩子，苏联六十年代的历史对我们影响很大”，六十年代的苏联就相当于八十年代的中国，赫鲁晓夫教出了戈尔巴乔夫，戈尔巴乔夫一手送走了苏联。下一届领导人是左还是右？改革开放中形成的利益集团是否已经可以左右政治格局，形成类似日韩的财阀统治？这是未来最大的不确定因素。</p>
<p>正因为这是未来最大的不确定因素，所以伟人在晚年才要发动文革。不断受人非议的文革，完成了一代人的政治教育，奠定了一代人的思想基础，顶住了世界范围的社会主义颠覆潮。伟人看得太远了，我们无法企及。但是我们虽然顶住了颠覆，却也经历了改革，大量国有资产被私有化，形成了利益集团。几十年过去了，人心不古。既得利益者一定会想方设法垄断政治权力，不加控制结局与明朝无异。解决方案伟人都说过了，而我们首先要做的就是正确评价文革。反对文革的人常拿文革中的极端案例来举例，犯罪案件每个时代都有，我们要注意到文革时期的犯罪率是远低于改革之后的，即便把群众运动中极端案件算上，也比改革之后的犯罪率低。就连当时“受迫害”的人都没有反对文革（比如现任领导人），那些不了解文革的人又凭什么反对呢？妖魔化文革的本质就是妖魔化群众运动，从而顺理成章的剥夺了民众的政治权力。</p>
<p>扯远了，看来可以写的主题有很多。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/25/2019-new-thinking/" class="archive-article-date">
  	<time datetime="2019-12-25T02:38:44.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-25</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/%E9%9A%8F%E7%AC%94/">随笔</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-torch-output-loss-optimizer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/24/torch-output-loss-optimizer/">Torch的损失函数和优化器</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。</p>
<h1 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h1><p>pytorch 损失函数的基本用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = LossCriterion(参数)</span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure>
<p>Mean Absolute Error<br>torch.nn.L1Loss<br>Measures the mean absolute error.</p>
<h2 id="Mean-Absolute-Error-L1Loss"><a href="#Mean-Absolute-Error-L1Loss" class="headerlink" title="Mean Absolute Error/ L1Loss"></a>Mean Absolute Error/ L1Loss</h2><p>nn.L1Loss<br><img src="/img/loss/l1loss.png" alt=""><br>很少使用</p>
<h2 id="Mean-Square-Error-Loss"><a href="#Mean-Square-Error-Loss" class="headerlink" title="Mean Square Error Loss"></a>Mean Square Error Loss</h2><p>nn.MSELoss<br><img src="/img/loss/mseloss.png" alt=""><br>针对数值不大的回归问题。</p>
<h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><p>nn.SmoothL1Loss<br><img src="/img/loss/smoothl1loss.png" alt=""><br>它在绝对差值大于1时不求平方，可以避免梯度爆炸。大部分回归问题都可以适用，尤其是数值比较大的时候。</p>
<h2 id="Negative-Log-Likelihood-Loss"><a href="#Negative-Log-Likelihood-Loss" class="headerlink" title="Negative Log-Likelihood Loss"></a>Negative Log-Likelihood Loss</h2><p>torch.nn.NLLLoss，一般与 LogSoftmax 成对使用。使用时 <code>loss(softmaxTarget, target)</code>。用于处理多分类问题。<br><img src="/img/loss/nllloss.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line"><span class="comment"># input is of size N x C = 3 x 5， C为分类数</span></span><br><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># each element in target has to have 0 &lt;= value &lt; C</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>nn.CrossEntropyLoss 将 LogSoftmax 和 NLLLoss 绑定到了一起。所以无需再对结果使用Softmax</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)</span><br><span class="line">target &#x3D; torch.empty(3, dtype&#x3D;torch.long).random_(5)</span><br><span class="line">output &#x3D; loss(input, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h2><p>二分类问题的CrossEntropyLoss。输入、目标结构是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line">input = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="Margin-Ranking-Loss"><a href="#Margin-Ranking-Loss" class="headerlink" title="Margin Ranking Loss"></a>Margin Ranking Loss</h2><p><img src="/img/loss/marginrankingloss.png" alt=""></p>
<p>常用户增强学习、对抗生成网络、排序任务。给定输入x1，x2，y的值是1或-1，如果y==1表示x1应该比x2的排名更高，y==-1则相反。如果y值与x1、x2顺序一致，那么loss为0，否则错误为 y*(x1-x2)</p>
<h2 id="Hinge-Embedding-Loss"><a href="#Hinge-Embedding-Loss" class="headerlink" title="Hinge Embedding Loss"></a>Hinge Embedding Loss</h2><p>y的值是1或-1，用于衡量两个输入是否相似或不相似。</p>
<h2 id="Cosine-Embedding-Loss"><a href="#Cosine-Embedding-Loss" class="headerlink" title="Cosine Embedding Loss"></a>Cosine Embedding Loss</h2><p>给定两个输入x1，x2，y的值是1或-1，用于衡量x1和x2是否相似。<br><img src="/img/loss/cosineembeddingloss.png" alt=""><br>其中cos(x1, x2)表示相似度<br><img src="/img/loss/cossim.png" alt=""></p>
<h1 id="各种优化器"><a href="#各种优化器" class="headerlink" title="各种优化器"></a>各种优化器</h1><p>大多数情况Adam能够取得比较好的效果。SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 所以说并不是越先进的优化器, 结果越佳.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD 就是随机梯度下降</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line"><span class="comment"># momentum 动量加速,在SGD函数里指定momentum的值即可</span></span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># RMSprop 指定参数alpha</span></span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># Adam 参数betas=(0.9, 0.99)</span></span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/24/torch-output-loss-optimizer/" class="archive-article-date">
  	<time datetime="2019-12-24T14:05:59.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-24</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-understanding-cnn" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/24/understanding-cnn/">理解CNN参数及PyTorch实例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。</p>
<h2 id="卷积核-kernel"><a href="#卷积核-kernel" class="headerlink" title="卷积核 kernel"></a>卷积核 kernel</h2><p>Kernel，卷积核，有时也称为filter。在迭代过程中，学习的结果就保存在kernel里面。深度学习，学习的就是一个权重。kernel的尺寸越小，计算量越小，一般选择3x3，更小就没有意义了。<br><img src="/img/cnn/kernel_2.png" alt=""></p>
<p>结果是对卷积核与一小块输入数据的点积。</p>
<h2 id="层数-Channels"><a href="#层数-Channels" class="headerlink" title="层数 Channels"></a>层数 Channels</h2><p><img src="/img/cnn/channel_1.png" alt=""></p>
<p>所有位置的点积构成一个激活层。</p>
<p><img src="/img/cnn/channel_2.png" alt=""></p>
<p>如果我们有6个卷积核，我们就会有6个激活层。</p>
<h2 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h2><p><img src="/img/cnn/kernel.gif" alt=""><br>上图是每次向右移动一格，一行结束向下移动一行，所以stride是1x1，如果是移动2格2行则是2x2。</p>
<h2 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h2><p>Padding的作用是为了获取图片上下左右边缘的特征。<br><img src="/img/cnn/pad.jpg" alt=""></p>
<h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>卷积层为了提取特征，但是卷积层提取完特征后特征图层依然很大。为了减少计算量，我们可以用padding的方式来减小特征图层。Pooling的方法有MaxPooling核AveragePooling。<br><img src="/img/cnn/pooling.jpg" alt=""></p>
<p>推荐看一下李飞飞的<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf" target="_blank" rel="noopener">这篇slide</a></p>
<h2 id="PyTorch-中的相关方法"><a href="#PyTorch-中的相关方法" class="headerlink" title="PyTorch 中的相关方法"></a>PyTorch 中的相关方法</h2><ul>
<li><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=’zeros’)</p>
</li>
<li><p>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</p>
<ul>
<li>stride 默认与kernel_size相等</li>
</ul>
</li>
<li><p>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</p>
</li>
<li><p>Tensor.view(*shape) -&gt; Tensor</p>
<ul>
<li>用于将卷积层展开为全连接层<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; torch.randn(4, 4)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y &#x3D; x.view(16)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z &#x3D; x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="MNIST例子"><a href="#MNIST例子" class="headerlink" title="MNIST例子"></a>MNIST例子</h2><p>MNIST 数据集的输入是 1x28x28 的数据集。在实际开发中必须要清楚每一次的输出结构。</p>
<ul>
<li>我们第一层使用 5x5的卷积核，步长为1，padding为0，28-5+1 = 24，那么输出就是 24x24。计算方法是 (input_size - kernel_size)/ stride + 1。</li>
<li>我们第二层使用 2x2的MaxPool，那么输出为 12x12.</li>
<li>第三层再使用5x5，卷积核，输出则为 12-5+1，即 8x8。</li>
<li>再使用 2x2 MaxPool，输出则为 4x4。</li>
</ul>
<p><img src="/img/cnn/mnist_convet.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""ConvNet -&gt; Max_Pool -&gt; RELU -&gt; ConvNet -&gt; Max_Pool -&gt; RELU -&gt; FC -&gt; RELU -&gt; FC -&gt; SOFTMAX"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>以上代码摘自 <a href="https://github.com/floydhub/mnist" target="_blank" rel="noopener">https://github.com/floydhub/mnist</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/24/understanding-cnn/" class="archive-article-date">
  	<time datetime="2019-12-24T07:56:21.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-24</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
  
    <nav id="page-nav">
      <span class="page-number current">1</span><a class="page-number" href="/page/2/">2</a><a class="page-number" href="/page/3/">3</a><span class="space">&hellip;</span><a class="page-number" href="/page/13/">13</a><a class="extend next" rel="next" href="/page/2/">Next &amp;raquo;</a>
    </nav>
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2020 桂糊涂
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: true,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: false,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>


<script src="/./main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/3D/" style="font-size: 10px;">3D</a> <a href="/tags/AI/" style="font-size: 11.25px;">AI</a> <a href="/tags/CI/" style="font-size: 12.5px;">CI</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/IM/" style="font-size: 10px;">IM</a> <a href="/tags/Objective-C/" style="font-size: 10px;">Objective-C</a> <a href="/tags/React/" style="font-size: 10px;">React</a> <a href="/tags/UI/" style="font-size: 10px;">UI</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/algorithms/" style="font-size: 11.25px;">algorithms</a> <a href="/tags/android/" style="font-size: 12.5px;">android</a> <a href="/tags/architecture/" style="font-size: 18.75px;">architecture</a> <a href="/tags/books/" style="font-size: 10px;">books</a> <a href="/tags/cloud-services/" style="font-size: 10px;">cloud-services</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a> <a href="/tags/data/" style="font-size: 10px;">data</a> <a href="/tags/data-format/" style="font-size: 12.5px;">data-format</a> <a href="/tags/editor/" style="font-size: 15px;">editor</a> <a href="/tags/education/" style="font-size: 10px;">education</a> <a href="/tags/engineering/" style="font-size: 10px;">engineering</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/font/" style="font-size: 10px;">font</a> <a href="/tags/game/" style="font-size: 15px;">game</a> <a href="/tags/game-dev/" style="font-size: 17.5px;">game-dev</a> <a href="/tags/game-dev-books/" style="font-size: 10px;">game-dev, books</a> <a href="/tags/generator/" style="font-size: 12.5px;">generator</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/go/" style="font-size: 13.75px;">go</a> <a href="/tags/gossip/" style="font-size: 10px;">gossip</a> <a href="/tags/greek/" style="font-size: 10px;">greek</a> <a href="/tags/hack/" style="font-size: 11.25px;">hack</a> <a href="/tags/hash/" style="font-size: 10px;">hash</a> <a href="/tags/iOS/" style="font-size: 10px;">iOS</a> <a href="/tags/input-methods/" style="font-size: 10px;">input-methods</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/js/" style="font-size: 12.5px;">js</a> <a href="/tags/live-dev/" style="font-size: 10px;">live-dev</a> <a href="/tags/lua/" style="font-size: 10px;">lua</a> <a href="/tags/machine-learning/" style="font-size: 12.5px;">machine-learning</a> <a href="/tags/makefile/" style="font-size: 10px;">makefile</a> <a href="/tags/marketing/" style="font-size: 11.25px;">marketing</a> <a href="/tags/mathematics/" style="font-size: 13.75px;">mathematics</a> <a href="/tags/mysql/" style="font-size: 12.5px;">mysql</a> <a href="/tags/network/" style="font-size: 16.25px;">network</a> <a href="/tags/nginx/" style="font-size: 11.25px;">nginx</a> <a href="/tags/node-js/" style="font-size: 12.5px;">node.js</a> <a href="/tags/nosql/" style="font-size: 10px;">nosql</a> <a href="/tags/ops/" style="font-size: 20px;">ops</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/redis/" style="font-size: 11.25px;">redis</a> <a href="/tags/security/" style="font-size: 11.25px;">security</a> <a href="/tags/slide/" style="font-size: 10px;">slide</a> <a href="/tags/svn/" style="font-size: 10px;">svn</a> <a href="/tags/tcp/" style="font-size: 12.5px;">tcp</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/tools/" style="font-size: 12.5px;">tools</a> <a href="/tags/udp/" style="font-size: 10px;">udp</a> <a href="/tags/video/" style="font-size: 10px;">video</a> <a href="/tags/vim/" style="font-size: 11.25px;">vim</a> <a href="/tags/web/" style="font-size: 17.5px;">web</a>
    			</div>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">联系我：&lt;br&gt; guileen AT qq DOT com</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>