---

title: CloudNative架构原则
tags:
---

道法术器。目标是道，原则是法，措施是术，工具是器。让大象跳舞。

公司文化是看不见的，但是是客观存在的。所谓工程师文化本质上是一种民主文化。老板所认为的工程师文化，应该是员工自觉自愿的加班加点完成工作，亲密无间的合作，不要甩锅推诿。而工程师所认为的工程师文化则是，领导从谏如流，不瞎指挥，产品经理不要拍脑袋，自己可以尝试新技术，容忍失败。这种认识上的差异是如此之大甚至是相互矛盾的。怎么样才算是好的企业文化，什么又是工程师文化？为什么建设工程师文化那么难？

事实上，建设工程师文化并不难，真正难的是领导者是否愿意去实施工程师文化。因为实施工程师文化意味着对权威的损害。当然，这有一个度的问题，度也就是阴阳关系问题。本质上，工程师文化绝对不仅仅是一个技术问题，而是一个政治学问题。他牵涉到议事程序、职场礼仪、民主与独裁、员工与资本、左和右。

![image-20210508165651664](/Users/admin/Library/Application Support/typora-user-images/image-20210508165651664.png)

对于一家公司而言一个好的工程师文化意味着：

* 更好的人才加入你的公司
* 工作与生活平衡，每个人自我驱动
* 一线员工的细节创新给公司带来了持续增长

但同时也意味着：

* 你看到衣着奇怪、不修边幅的员工走来走去
* 迟到、不加班的现象比比皆是
* 作为领导你感觉不到足够的敬畏，似乎总有人要挑战你，你的想法屡次遭到否定
* 员工将他的代码分享到开源社区，你感觉公司所支付的薪水做了公益，甚至这可能泄露了公司的知识产权
* 员工的能力进步很快，如果不给他们加薪，他们就可能被其他公司挖走
* 你在与不在，手下人都干得很好，作为领导者，你存在的必要性似乎不大
* 最可怕的是，作为中层领导者，你感觉自己已经没有存在的价值，随时可能被替换。

因此只有一种人可以真正建设好工程师文化：不用担心被辞退的人！符合这个条件的人只有一个人，就是公司的老板。这个人必须是这个公司的实际控制人，而不是仅仅是一名职业经理人。如果实际经理人期望职业经理人来协助自己完成企业文化的建设，必须给予经理人足够的安全感。这样经理人才能感受到公司价值与自身价值的统一，不再以官僚主义的立场来"绑架"公司。也必须选择正确的经理人，经理人的思想与目标文化的契合度非常重要。

* 





读《持续演进的Cloud Native》笔记。

2010/5/28 WSO2的CTO Paul Fremantle 在博客首次提出Cloud Native。

* 分布式
* 弹性
* 多租户
* 自服务
* 按需计量、计费
* 增量部署和测试

2013 Netflix云架构师（2016年成为AWS VP）Adrian Cockcroft在Yow Confernce上介绍了Netflix在AWS的Cloud Native的应用，目标、原则、措施。

**目标**

* 可扩展性
* 高可用性
* 敏捷
* 效率（效率高的人比效率低的人不只是高出百分之二十，甚至是几十倍、上百倍）

**五大原则**

* 不变性。实例一旦创建不可改变，若改变只能通过创建新节点。
* 关注点分离。通过微服务架构实现关注点分离，避免出现“决策瓶颈”
* 反脆弱性。默认所有的依赖都可能失效，在设计阶段就要考虑到如何处理这些失效问题。为了让系统更强壮，Netflix会不断地攻击自己、主动破坏，以提醒系统要进行反脆弱性设计。
* 高信任组织。倡导给基层员工自主决策权！！！
* 共享。透明管理，共享促进技术人员成长。

**措施**

* 利用AWS实现可扩展性、敏捷、共享
* 利用非标准化数据实现关注点分离。
* 利用猴子工程师实现反脆弱性。
* 利用默认开源实现敏捷、共享。
* 利用持续部署实现敏捷、不变性。
* 利用DevOps实现高信任组织和共享。
* 利用运行自己写的代码实现反脆弱性开发演进。

Pivotal的Matt Stine在电子书《Migrating to Cloud Native Application Architectures》认为在单体架构向Cloud Native迁移的过程中，需要文化、组织、技术共同变革。该书把Cloud Native描述为一组最佳实践，包含如下几个重要内容。

* 十二因子。
* 微服务。
* 自服务敏捷基础设施。
* 基于API的协作。
* 反脆弱性。

----

**如何衡量CN的能力**

| 成熟度 | 描述                                                         | 关键技术点                                                   | 效果                                                         |
| ------ | ------------------------------------------------------------ | ------------------------------------------------------------ | ------------------------------------------------------------ |
| 第一级 | 单体架构或者大粒度的拆分；<br />应用运行在虚拟化环境重；<br />应用可以通过镜像或脚本自动化部署 | 负载均衡技术<br />模块化<br />虚拟化隔离                     | 瀑布式开发<br />系统重构基本是革命<br />可用性严重依赖运维人员 |
| 第二级 | 微服务架构，无状态、自治、隔离<br />服务根据业务划分等级<br />非核心业务可以实现快速降级<br />不受失败服务的影响，快速隔离 | 微服务框架<br />持续交付流水线<br />调用链分析<br />分布式配置<br />自动化测试<br />监控系统 | 交付周期提升明显<br />小团队开发、沟通效率提升<br />对测试人员依赖降低 |
| 第三级 | 可编程基础设施<br />中间件作为后端服务提供（开发、测试、部署、维护都有专门团队负责，只需要通过接口调用，轻松满足多租户需求）<br />任何时刻业务无中断<br />实现1%-10%-100%的灰度发布流程<br />自动扩/缩容方式，包括有状态的数据库（自动化负载均衡、分库分表）、缓存<br />自动化降级<br />开发、测试、生产环境统一<br />全球化的业务发布能力、异地多活 | 分布式数据库<br />分布式存储<br />分布式缓存<br />分布式消息队列<br />灰度发布平台<br />全局容错方案<br />全局一致性方案<br />混沌测试<br />容器<br />资源调度平台 | 不需要择时发布<br />开发人员专注于业务，架构能力由基础设施承载<br />公共基础服务共享重用 |
| 第四级 | 系统具有自学习、自诊断、自调整、自恢复能力<br />高度可视化、自动化<br />实现自动发布（AI选择发布时间）、自动降级、自动回滚；<br />可根据AI自动调整参数，如超时时间<br />所有公共服务形成统一的整体，通过接口实现数据共享，实时调整。例如监控可以反压流控 | 人工智能服务进行决策<br />强大的公共基础服务<br />智能化运维<br />高度自动化能力<br />可实现Serverless架构 | AIOps/NoOps<br />资源利用率极大提升<br />瞬间扩展能力<br />强大的可用性<br />强一致性 |

原则（王启军）：

* 为失败设计原则
* 不变性原则（每个组件都可以自动安装、部署、不需要人工干预）
* 去中心化原则（包含研发流程、技术架构）
* 标准化原则（通过框架固化标准）(SonarQube检查代码)(独立自主与标准化相平衡)
* 迭代速度优先原则（迭代速度优于资源使用效率）
* 简化设计原则
* 自动化驱动原则（服务的创建、开发、测试、部署、运维的重复性工作都要自动化）
* 演进式设计原则

### 微服务架构

微服务架构相比单体架构有交付速度快、可用性高、架构灵活、可扩展性强等优点，但也要注意**其缺点有：一致性实现成本高、时延高、资源成本高、关联查询复杂、开发人员要求高、运维复杂度高、对rpc、服务治理以及其他的工具依赖更多**。

![image-20210506163615151](/Users/admin/work/guileen.github.com/hexo/source/img/cloudnative/microvsmono.png)

**几乎在所有我听说过从一开始就构建为微服务架构的故事中，最终都有人遇到了巨大的麻烦。**在服务划分之前，应该保证基础设施及公共基础服务已经准备完毕。可以通过监控快速定位故障，通过工具自动化部署、管理服务，通过服务化框架降低服务开发的复杂度，通过灰度发布提升可用性，通过资源调度服务快速申请、释放资源，通过弹性伸缩快速扩展应用。

在决定是否要进一步拆分微服务时，首先考虑团队规模因素，其次考虑交付速度要求，而资源利用率、性能等要求则最后考虑。团队变大后，会出现决策瓶颈点，所有决策都依赖某个人，而没有人愿意承担责任，效率底下。

**微服务设计原则**：

* 垂直划分优先原则
* 持续演进原则
* 服务自治、接口隔离原则
* 自动化驱动原则

**微服务先决条件**

* 研发环境和流程上的转变
  * 自动化工具链
  * 微服务框架
  * 快速申请资源
  * 故障发现反馈机制
  * 研发流程上的转变
* 拆分前先做好解耦
  * 状态外置
    * 定时任务
    * 本地存储
    * 本地缓存
  * 去触发器、存储过程
  * 通过接口隔离

**微服务划分模式**

![image-20210506170957898](/Users/admin/work/guileen.github.com/hexo/source/img/cloudnative/domainvstable.png)

主要考虑两个因素：业务复杂度；团队对领域驱动的熟悉程度。

通常基于数据驱动划分服务的步骤如下。

* （1）需求分析。通过领域专家（或者产品经理）确定目标，然后总结User Story，确定核心的业务流程；通过工具呈现比较粗糙的界面，进行内部讨论；不断迭代此环节，直到满意为止。
* （2）抽象数据结构。根据需求总结Use Case，协助分析需求，从中抽象数据结构。
* （3）划分服务。分析数据结构，识别服务——服务应该满足高内聚、低耦合、单一职责等特征。
* （4）确定服务调用关系。先分析出主要流程，根据请求需要调用的服务确定服务调用关系。如果存在问题，则需要回到（1）重新开始。
* （5）业务流程验证。重新回到User Story，以服务为粒度实现时序图，注意此阶段重点是验证服务划分是否合适，要关注如下问题。
  * 一次更新操作如果要跨越更多服务，那么一致性的要求是什么。
  * 跨服务查询时，是否要做关联查询，一个服务内是否能解决问题。
  * 性能是否能满足要求。
  * 成本是否满足要求。
* （6）持续优化。

通常基于领域驱动划分服务的步骤如下。

* （1）通过模型和领域专家建立统一语言。建立统一语言是为了更深入地理解需求。通用语言尽量以业务语言为主，而非技术语言；通用语言和代码一样，需要不断地重构。
* （2）业务分析。确定核心的业务流程，然后逐步扩展到全部。最好通过工具呈现比较粗糙的界面，供内部讨论。
* （3）寻找聚合。显式地定义领域模型的边界。最近比较热门的事件风暴[5]，是一种基于领域驱动分析业务、划分服务的方法。事件风暴就是把所有的关键参与者都召集到一个很宽敞的屋子里来开会，并且使用便利贴来描述系统中发生的事情，事件风暴用桔黄色的便利贴代表领域事件，在上面用一句话描述曾经发生过什事情。用蓝色的便利贴代表命令。命令的发起者可能是人，也可能是注入系统中的外部事件，或者定时器等。用黄色的便利贴代表聚合。聚合是一组相关领域对象的集合，高内聚、低耦合是其基本要求，聚合内还要保证数据一致性。

![image-20210506171914269](/Users/admin/work/guileen.github.com/hexo/source/img/cloudnative/eventstorm.png) 

* （4）确定服务调用关系。先分析出主要流程，根据一次请求需要调用的服务来确定服务调用关系。如果存在水平划分，则需要根据服务依赖原则确定关系。如果存在问题，则需要回到（1）重新开始。
* （5）业务流程验证。以服务为粒度实现时序图，注意此阶段重点是要验证服务划分是否合适，主要关注如下问题。一次更新操作如果要跨越更多服务，那么一致性的要求是什么。跨服务查询时，是否要做关联查询，一个服务内是否能解决问题。性能是否能满足要求。成本是否满足要求。
* （6）持续优化。

从已有单体架构中逐步划分服务

* （1）通常前后端分离是拆分的第一步
* （2）提取公共基础服务，如单点登录。拆分可以遵循逻辑分离和物理分离两种方法。另外随着系统压力的增加，可能会用到消息中间件、分布式缓存等服务。
* （3）不断地从老系统中抽象出服务，垂直划分优先
* （4）当业务越来越复杂的时候，API Gateway做了太多的事情，会成为一个瓶颈点，服务之间的依赖关系也会变得越来越复杂，此时，需要适当地进行水平切分

 微服务拆分策略：.优先抽象通用服务,边界比较明显的服务,抽象核心服务...采用绞杀者模式，在遗留系统外围，随着时间的推移，让新的服务逐渐“绞杀”老的系统。在这种情况下，复杂度往往体现在如何灰度发布、迁移数据，以及如何保障服务不中断

衡量划分是否合理：

* 一个小功能的修改从需求到上线需要多长时间？
* 大多数功能修改是否可以在一个服务内完成？
* 是否要频繁修改接口？频繁修改接口有可能是接口设计不合理导致的，也有可能是服务划分的问题导致的，说明服务之间的边界并不是特别明确和稳定。
* 响应时间是否能满足要求？
* 是否存在大量的跨服务更新？是否存在大量的跨服务的关联查询？

**微服务框架**

* 服务治理
* 容量规划
* 高效通信
* 负载均衡

**微服务错误应用**

* 先实施组件化，再实施微服务架构。？？
* 用传统方式构建微服务。流程、KPI的习惯没有改变。微服务自动化发布、灰度发布、Design For Failure、自动化测试、故障隔离、自愈，这些思想没有改变，团队充满质疑。
* 组织架构不改变。决策瓶颈
* 习惯于领导安排工作（不愿承担责任）
* 以大规模拆分服务开始
* 高估架构可移植性

### 基础设施

基础设施即代码（Infrastructure as Code）或者可编程基础设施（Programmable Infrastructure）。某旅游互联网公司的策略是让有代码能力的运维人员去开发云平台和工具，让没有代码能力的运维人员做基础运维，也就是传说中的“搬机器”，所有的开发流程都是由全栈工程师主导，取得了比较好的效果。

基于容器的敏捷基础设施

* 标准化。所有的基础设施最好都是标准的，没有个性化配置。开发环境、测试环境及生产环境无差异。
* 可替换。任意节点都能够被轻易地创建、销毁、替换。
* 自动化。所有的操作都通过工具自动化完成，无须人工干预。
* 可视化。当前环境要做到可控，就需要对当前的环境状况可视。
  * 博主注：[滴滴夜莺](https://github.com/didi/nightingale)
* 可追溯。所有的配置统一作为代码进行版本化管理，所有的操作都可以追溯。
* 快速。资源申请及释放要求秒级完成，以适应弹性伸缩和故障切换的要求。

目前比较常用的基础设施自动化工具包括Ansible、Chef、SaltStack、Terraform等，可以使用DSL定义、描述环境，更容易理解和维护。

**告警系统：**

* 数据采集

  * 直接上报
  * 通过日志上报
  * 通过Agent上报

* 数据接收：

  * （1）推模式。业务服务主动通过Monitor的API与其建立连接，当产生数据时，通过连接直接发送数据。
  * （2）拉模式。业务服务到Monitor上注册标准的数据收集API接口和相应配置（包括调用时间、频率等）,Monitor根据配置调用业务服务的API获取数据。

* 通过时间序列数据库存储

  * 基于时间序列存储监控数据是目前大型互联网公司的一个普遍做法。目前比较流行的时序数据库包括InfluxDB、OpenTSDB、Druid、Graphite等。
  * 写多读少，大部分时间是写入，写入是顺序的，但是读的时候并发量也有可能很高。
  * 数据量较大，内存一般放不下，属于IO密集型。
  * 读操作的时候需要按照时间排序，升序或降序。

* Prometheus

  * 特点：
    * 时序数据库存储监控数据能够存储更大量的数据，不依赖于其他存储系统，安装非常简单。
      * 据官方介绍，每个采样数据只占3.5byte左右，上百万条时间序列，30s间隔，保留60天，大概只占用200GB
    * 灵活的查询语言（PromQl）。
    * 通过基于HTTP的pull方式采集时序数据，可以通过Pushgateway进行时序列数据推送。
      * Prometheus支持通过配置文件、文本文件、ZooKeeper、Consul、DNS SRVlookup等方式指定抓取目标
    * 多种可视化和仪表盘支持。
    * Alertmanager是独立于Prometheus之外的一个组件，提供十分灵活的报警方式。另外，可以通过Web UI、Grafana、API clients等可视化界面查询数据。
  * 四种数据类型。
    * Counter：单调递增数据，只能增加，不能减少。例如记录请求次数、错误数。
    * Guage：当前的状态，常规值，可增可减。例如内存、CPU的使用情况。
    * Histogram：主要用于在指定分布范围内（Buckets）记录大小或者事件发生的次数。
    * Summary：客户端定义的数据分布统计图。
  * Prometheus配合Grafana
  * Thanos扩展存储。

  

**分布式消息中间件**

* 解耦
* 削峰填谷
* 选型：
  * RocketMQ。有一些比较好的功能，如：服务端过滤，定时消息，事务消息。
  * Kafka。性能非常高！0.8以后保证可靠性。
    * 消息堆积能力强
    * 保留一段时间后，会被批量删除，无论是否消费过。
    * 每个Partition只能被一个消费者订阅，一个消费者可以订阅多个Partition，用这种方式避免一定的重复消费。如果认为消费者的能力受到了限制，则可以通过增加Partition 的方式实现扩展，当一个消费者挂掉之后，会重新进行负载均衡。如果网络不稳定，则会导致频繁的重新负载均衡。
    * 顺序写磁盘——媲美内存随机访问
      * 使用6个7200rpm SATA RAID-5阵列的JBOD配置上的线性写入性能约为600MB/s，但是随机写入的性能大约只有100KB/s
    * 零拷贝——减少上下文切换及拷贝次数
      * 操作系统将数据从磁盘读取到内核空间中的Page Cache过程大致如下。
        * （1）业务应用从内核空间读取数据到用户空间缓冲区。
        * （2）业务应用将数据写入内核空间到socket缓冲区。
        * （3）操作系统将数据从socket缓冲区复制到通过网络发送的NIC缓冲区。
        * 整个过程经历了四次数据复制，两次上下文切换。
        * Linux也能够做到在数据传输的过程中，避免数据在操作系统内核态buffer和用户态buffer之间进行复制。Linux中提供类似的系统调用函数主要有mmap（）、sendfile（）及splice（）,Kafka采用的是Linux系统的函数sendfile（），允许操作系统将数据从Page Cache直接发送到网络，以此来避免数据复制。
    * Broker>Topic>Partition>Segment
    * 如何保证不丢消息
      * ACK
      * 复制。Topic/Broker 设置 min.insync.replicas, 当且仅当request.required.acks参数设置为−1时，此参数才生效
      * 默认7天删除
      * producer.type=async 可改为 sync
      * 消费消息显式提交Offset。需要幂等性。
    * 跨数据中心：MirrorMaker

**分布式缓存** 略 Redis 4.0 新特性

**分布式任务调度服务**

* 要求
  * 不重复的执行任务
  * 不遗漏的执行任务
* Tbschedule
* Elastic-Job

**分布式ID**

* SnowFlake
* Ticket Server（MySQL自增 REPLACE INTO 语句）

### 可用性设计

* 可用性公式：A=Uptime /（Uptime+Downtime）。其中，Uptime是可用时间，Downtime是不可用时间。
* 可靠性公式：A=MTBF /（MTBF+MTTR）。其中，MTBF的全称是Mean Time Between Failure，即平均无故障工作时间，指上一次故障恢复后开始正常运行到这次故障的时间平均值。MTTR的全称是Mean Time To Repair，即平均故障修复时间，是指从出现故障到完全恢复的这段时间。
* 可用性等级：
  * 99% 年停机时间 87.6小时 简单的负载均衡
  * 99.9% 年停机时间 8.8小时 灰度发布、自动化发布、自动化测试、快速回滚
  * 99.99% 年停机时间 53分钟 微服务、相关中间件自动扩展（数据库自动扩展、缓存自动扩展）、容错、监控、弹性伸缩
  * 99.999% 年停机时间 5分钟 异地多活、智能运维
* 降低了可用性的因素
  * 发布！当需要迁移数据时，通常会暂时中断服务。
  * 故障。
  * 压力
  * 外部强依赖
* 高可用设计：
  * 20倍设计、10倍开发、5倍部署
  * Design for failure，预测可能发生的问题，做好预案。如限流、伸缩、隔离故障节点。
* 逐步切换
  * 影子测试：TCPCopy、MQ添加新消费者，对比数据库验证。需要额外资源。
  * 蓝绿部署：除了正在运行的环境（蓝色环境），需要额外冗余另外一份相同的环境（绿色环境）。测试通过后，可以把负载均衡器/反向代理/路由指向绿色环境，一旦发生故障需要回退时，只需要切换到蓝色环境即可
    * 最好有自动化的基础设施作为支撑。全面的监控，发生故障马上发送告警。
    * 两套环境隔离问题，有相互影响的风险。
    * 难点是**当数据结构发生变化时**，如何同步数据，发生故障时，如何回滚。
    * 切换时需要优雅的终止机制，禁止直接kill进程。
    * 比较**浪费资源**，需要有一套独立的闲置资源。
  * 灰度发布/金丝雀发布
    * （1）假设生产环境运行的是v1版本，从负载均衡列表中摘掉一个节点，作为“金丝雀”服务器。
    * （2）在“金丝雀”服务器上部署v2版本。
    * （3）进行自动化测试。
    * （4）将v2版本节点添加到负载均衡列表中。
    * （5）如果发生故障，则马上进行回滚。
    * （6）如果没有问题，则逐步升级剩余的其他节点。
    * 内部员工>外部1%友好用户>5%友好用户>10%友好用户>全网发布。
    * 任意维度，通常根据用户相关的属性进行切分，如用户所在的区域、用户的级别等。
    * 需要有一个过程，需要引入一套自动化的流程进行发布，需要通过快速的发布回滚机制和全面的监控报警来保证高可用。

**容错设计**

* 消除单点
* 特性开关
  * 大多数互联网公司都是基于主干开发的，包括Google和Facebook。主干开发最大的好处是避免了合并分支时的麻烦。
  * 在多人同时协作开发的时候，可能A开发的特性M已经完成，B开发的特性N还没完成，为了不影响M上线，我们可以采用特性开关关闭N。当M、N两个特性都要上线时，发现A出现bug，是否要全部回退？能否通过开关关掉A？
  * 建立一个通用的特性开关服务，每5秒轮询一次特性开关服务，或者特性开关服务出现变化时通知服务实例
* 服务分级
  * 1级服务：核心业务流程，一旦发生故障，直接导致业务遭受重大损失
  * 2级服务：用户体验受到严重影响。一旦发生故障，关键业务还可以使用。
  * 3级服务：用户体验受到轻微影响。一旦发生故障，正常业务不受影响，一些不常用功能不可用。
  * 4级服务：多为管理维护服务，用户不会受到影响，用户不会直接访问。
* 降级设计
  * 双11关闭无关紧要功能（隐藏 确认收货 按钮）
  * 关闭某个功能，页面显示不全或不能点击某个按钮。
  * 请求短路，直接返回缓存结果。
  * 简化流程，放弃某个操作，如给用户发注册成功短信。
  * 延迟执行，停止定时任务，如某些结算。
* 降级方法
  * 页面加开关，通过JS控制功能是否隐藏。
  * 关闭低级别服务前端页面
  * 关闭定时任务
  * 预先定义降级逻辑（通过头信息限制某级以下的服务不能调用）
  * 降低精确度（有货、无货，而非具体数量）
* 超时重试
  * 超时时间。
    * 由于服务x个别请求的超长执行时间导致服务线程池耗尽，不应该为了保证个别请求而降低整体的可用性，应该设置更低的超时时间。
    * AI进行预测和动态配置。
  * 重试的总次数。
  * 重试的间隔时间。
  * 重试间隔时间的衰减度。
  * 1.简单重试模式——try-catch-redo
  * 2.策略重试模式——try-catch-redo-retry
  * 3.基于Spring-tryer重试
  * 4.基于Guava-retrying重试
* 隔离策略
  * 核心业务独占隔离，线程池隔离，进程隔离，集群隔离，用户隔离，租户隔离（逻辑隔离、物理隔离、混合隔离），Hystrix实现隔离。
* 熔断器
  * 雪崩效应：当某个用户在网上提交一个订单的时候，在微服务架构中，可能这个请求要经过N个服务。假设某个库存服务实例的CPU非常繁忙，导致请求一直阻塞，我们可能想到的一个解决办法是设置超时时间，那这个超时时间设置多少合适呢？如果设置的超时时间过短，生产者（被调用的服务）没有处理完，消费者又发过来一次请求，那么最终导致生产者崩溃。如果设置的过长，那么用户发现几秒都没有响应，可能会以为网络太慢，又重新提交一次订单
  * 熔断器模式（Circuit Breaker Patten，Flop）
    * 状态：
      * 打开（open）：熔断器打开状态。消费者的请求禁止通过熔断器调用生产者，调用快速失败。
      * 闭合（closed）：熔断器闭合状态。消费者的请求可以顺利通过，到达生产者。熔断器会记录最近调用失败次数，当达到阈值时，状态切换到熔断器打开状态。然后，熔断器开始倒计时，当剩余时间为0时，则切换到半打开（half open）状态。这个倒计时实际上是给生产者一个自动恢复的时间。
      * 半打开：熔断器半打开状态。熔断器允许消费者的部分请求通过熔断器尝试调用生产者，因为前面的失败有可能是因为断网等原因导致的。如果这些请求顺利调用成功，则熔断器恢复到闭合状态；如果这些请求仍然调用失败，则切换到打开状态，重新开始计时
    * 关注如下几个问题
      * 熔断器打开时，生产者如何应对？此时应该视业务场景而定，尝试调用其他实例服务；快速失败；业务降级，不显示或者显示缓存值。
      * 和其他运维监控系统关联使用。应该在界面上预留开关，可以手动控制熔断器状态。例如，当生产者发生故障的时候，可以通过监控系统发现系统是否已经恢复，如果已经恢复，则可以直接通过预留接口恢复调用，而不必等待重试到指定阈值。当生产者因为满负荷导致无法响应的时候，应该停止重试。升级时，也可以通过开关进行手动控制。
      * 禁止通过一个熔断器控制多个服务。用一个熔断器控制多个服务，可能会导致一个服务出现问题，熔断所有服务的情况发生。

**流控设计**

* 限流算法
  * 固定窗口算法（fixed window）
  * 漏桶算法（Leaky Bucket）平滑网络上的突发流量，突发流量可以被重新分配以便为网络提供一个稳定的流量。漏桶算法能强行限制数据的传输速率，输入速率可以变化，但是输出速率保持不变。
    * 水（请求）先进入漏桶（队列）里。
    * 漏桶（队列）以一定的速度出水（请求）。
    * 水（请求）过大会直接溢出（丢弃数据包）。
  * 令牌桶算法（token bucket）
    * 每秒会有x个令牌放入桶中，或者说，每过1/x秒桶中增加一个令牌。
    * 桶中最多存放n个令牌，如果桶满了，则新放入的令牌会被丢弃。
    * 当一个m字节的数据包到达时，消耗m个令牌，然后发送该数据包。
    * 如果桶中可用令牌小于m个，则该数据包将被缓存或丢弃。
* 流控策略
  * 根据压测结果、生产环境上的表现，对每个服务进行设定。一般采用经验值，原则是宁愿设置得过小，也绝对不设置太大
  * 请求入口处。因为此处限流效果最好，后端服务不会受到影响
  * 业务服务入口处。可以依赖于微服务框架，对每个服务节点进行限流，限流并不意味着只能设置单位时间请求数，通过设置最大连接数、最大线程数等都可以实现限流。每个业务服务都应该声明SLA，包括单位时间处理请求的能力。
  * 公共基础服务处。因为公共基础服务太重要，一旦宕机后果严重，所以更需要保护，如数据库、缓存等。
  * 只针对一个实例进行限制，对外则表现为一会儿可以访问，一会儿又访问不了了。在负载均衡配置转发策略，同一IP转发到同一服务节点，这样至少对于一个用户来说是一致的，但是对于不同用户来说还是存在问题。如果有一个分布式的限流服务针对同一用户记录访问次数，那么效果会比较好，通常我们会使用Redis来实现。

**容量预估**

* 互联网公司普遍采用全链路压测的方式，如京东的 ForceBot、阿里巴巴的全链路压测平台。在请求入口进行真实流量复制，开源实现可以参考TCPCopy
* 找到核心流程。做全链路压测需要巨大的成本，不可能全做。一个系统中的核心流程可能只有20%，这是压测的目标。
* 选择隔离方式。一种是独立的环境进行压测，隔离效果好，简单方便，但是资源成本高。另一种是和生产环境混合，通过参数进行识别，在框架和服务处进行特殊处理，这种方式真实性更高，节省资源，但是隔离性不好。
* 缩小依赖服务范围。
* 思考：如果对A服务进行压测，那么A服务依赖的服务应该如何配合。

**故障演练**

* 早在2010年的一篇博文中[2],Netflix的工程师John Ciancutti就有一句经典的话：“The best way to avoidfailure is to fail constantly”，大意就是，避免失败最好的方式就是不断失败。
* 2012年，Netflix开源了Chaos Monkey,Chaos Monkey是一个在生产环境随机选择并关闭服务的工具。
* Design for failure，以确保不会因为故障对最终用户产生更大的影响。
* 可以在 Chaos Monkey 上配置执行计划，默认只在工作日的上午9点至下午3点执行。
* Chaos Monkey属于Netflix的Simian Army产品中的一员，SimianArmy由一组工具构成，包括如下成员。Chaos Monkey：随机关闭生产环境中的实例。Latency Monkey：让某台机器的请求或返回变慢，观察系统的表现，可以用来测试上游服务是否有服务降级能力，当然如果响应时间特别长，也就相当于服务不可用。Chaos Gorilla：模拟AZ故障，中断一个机房，验证是否跨可用区部署，业务容灾和恢复的能力。Conformity Monkey：查找不符合最佳实践的实例，并将其关闭。例如，如果某个实例不在自动伸缩组里，那么就该将其关闭，观察服务是否能够重新正常启动。Doctor Monkey：查找不健康实例的工具，除了运行在每个实例上的健康检查，还会监控外部健康信号，一旦发现不健康实例就会将其移出服务组。Janitor Monkey：查找不再需要的资源，并将其回收，这能在一定程度上降低云资源的浪费。Security Monkey：是Conformity Monkey的一个扩展，检查系统的安全漏洞，同时也会保证SSL和DRM证书仍然有效。10-18 Monkey：进行本地化及国际化的配置检查，确保不同地区、使用不同语言和字符集的用户都能正常使用Netflix。阿里巴巴也开始进行故障演练，它的工具名称叫MonkeyKing，为每年的“双11”活动做准备。MonkeyKing可以模拟硬件故障、API故障、分布式故障、数据库故障等。

**数据迁移**

* Code is easy,state is hard.
* 尽量不让拆分服务和数据结构的改变放在一起来做，要尽量把一次大的重构划分为几次比较小的重构，但是在很多场景中，两者一起做还是无法避免的。
* 1. 逻辑分离，物理不分离逻辑分离，物理不分离（如图4-15所示）是指新服务和老服务放在一个数据库里，建立不同的表名，从代码层面实现隔离、解耦。数据迁移可以通过触发器实现，也可以采用双写的方式实现。通过单库事务实现业务服务写数据的同时写两张表。
     * 逻辑分离，物理不分离的优点是足够简单，缺点是隔离性差，容易引发全局故障。由于触发器导致数据库的扩展受到影响，因此这个方案更多地被作为一个临时方案或者过渡方案。
  2. 物理分离（如图4-16所示）是指新服务和老服务的数据通过不同的数据库物理隔离，可以使用相同的表名，数据同步需要额外的方案实现。
     * 物理分离的优点是隔离性好，缺点是数据同步比较复杂。
     * 利用数据库同步工具通过读取binlog实现数据双向同步。
     * 在业务应用上同时双写两个数据库。
     * 老系统在写数据库的同时，发送消息到消息中间件，消费消息从消息中间件实现同步。
     * 都不可避免地会遇到一致性问题。

### 可扩展性设计

### 性能设计

### 一致性设计

### 未来方向

* Serverless并不是不需要服务，而是开发者不用关注服务。举个例子，开发一个应用，开发者需要关心缓存、MQ、Web容器。而在Serverless环境下，开发者只需要关注代码层面的东西，也就是写完代码直接提交到 Serverless 服务上，并设置相关的策略，Serverless就会帮开发者考虑好一切，包括基础设施、扩展性、性能等。如果你想用 MQ，则只需调用函数，无须关注MQ是否能承受压力，至于什么时候需要扩展，成本如何控制等问题，Serverless会为你做好一切。
* 边缘计算和Serverless的结合，因为在一个数据中心，我们可以通过云操作系统轻松管理数万个节点，而边缘的计算能力则要小得多，通常只有10～100台物理机，但是这些机器需要为上万个租户提供IaaS服务
* 简单来说，Service Mesh 帮助应用程序在海量服务、复杂的架构和网络中建立稳定的通信机制，业务所有的流量都转发到Service Mesh的代理服务中。不仅如此，Service Mesh还承担了微服务框架所有的功能，包括服务注册发现、负载均衡、熔断限流、认证鉴权、缓存加速等。不同的是，Service Mesh 强调的是通过独立的进程代理的方式，除此之外，还承担了上报日志、监控的责任
  * Linkerd
  * Envoy
  * nginMesh
  * Conduit （rust 开发）
  * Istio

### 研发流程

### 团队文化



