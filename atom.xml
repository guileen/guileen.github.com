<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>桂糊涂的博客</title>
  
  <subtitle>代码杂记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guileen.github.io/"/>
  <updated>2021-05-01T02:31:48.858Z</updated>
  <id>http://guileen.github.io/</id>
  
  <author>
    <name>桂糊涂</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>c10m单机千万连接问题及解决方案</title>
    <link href="http://guileen.github.io/2021/04/22/c10m-problem-and-solution/"/>
    <id>http://guileen.github.io/2021/04/22/c10m-problem-and-solution/</id>
    <published>2021-04-22T06:24:01.000Z</published>
    <updated>2021-05-01T02:31:48.858Z</updated>
    
    <content type="html"><![CDATA[<p>C10m是继c10k问题之后提出的新问题，指单机1000万连接问题。40gbps网卡、32核、256G内存，这样的配置理论上已经可以处理千万并发连接了。但虽然硬件已经能够满足条件了，但是软件系统依然是复杂的。这就是C10M问题。</p><p>使用Go语言开发的TCP server，可以比较轻松的保持1000万的连接。大约消耗56G内存，用于连接对象和goroutine，不包含业务对象。但仅仅保持空闲连接并不够，最重要的是拥有更高的包处理速度（Packet per second），和更低的时延。</p><p>Robert Graham的演讲<a href="https://www.cs.dartmouth.edu/~sergey/cs258/2013/C10M-Defending-the-Internet-at-Scale-Dartmouth-2013.pdf" target="_blank" rel="noopener">《C10M Defending The Internet At Scale》(pdf)</a> <a href="https://www.youtube.com/watch?v=D09jdbS6oSI" target="_blank" rel="noopener">(youtube)</a>回答了C10M的问题原因：内核不是解决方案，而是问题本身。因为内核处理数据包是经过了一个复杂的过程。对于C10m的定义如下：</p><ul><li>千万并发连接</li><li>每秒百万连接接入</li><li>每秒千万数据包</li><li>10gb/s</li><li>10微秒延迟</li><li>10微秒抖动</li><li>10核CPU并行</li></ul><p>其中最核心的指标，是前3项。而面临的挑战主要是<strong>用户态协议栈和多核并发问题</strong>。用户态协议栈可以在网关类、云原生等应用中发挥极大的价值，但是对于业务层的开发并不友好。用户态协议栈的技术有SDP、BPF、DPDK等。我们需要<strong>重点关注的是多核并发情况下的网络编程问题</strong>。</p><h2 id="为什么多核编程是复杂的？"><a href="#为什么多核编程是复杂的？" class="headerlink" title="为什么多核编程是复杂的？"></a>为什么多核编程是复杂的？</h2><p>大多数的程序在高于4核的CPU上不能发挥更好的性能，有时甚至会降低性能。其中主要的影响因素是CPU的缓存流水线。L1 cache 4个cycle（cpu时钟），L2 cache 12个cycle，L3 30 cycles，内存 300 cycles。如果命中L1、L2缓存，性能则是很高的，若缓存miss性能则会降低。在高并发的情况下，我们最好是保证程序是绑定在某个CPU上执行的。</p><p><img src="/img/c10m/cpu-cache.png" alt="img"></p><p>在千万并发情况下，因为上下文的切换过于频繁，缓存miss的情况将大大增加。在上下文切换时，如果同一个连接的处理线程不是绑定在同一个CPU上的，那么将进一步加剧缓存miss的情况。</p><p>除缓存miss情况外，cacheline还存在一种<strong>伪共享</strong>的问题，会造成性能的下降。当从内存中取单元到cache中时，会一次取一个cacheline大小的内存区域到cache中，然后存进相应的cacheline中, 所以当你读取一个变量的时候，可能会把它相邻的变量也读取到CPU的缓存中(如果正好在一个cacheline中)，因为有很大的几率你回继续访问相邻的变量，这样CPU利用缓存就可以加速对内存的访问。这本来是一种优化策略，避免加载相邻变量时多次访问内存。但是在多核并发的情况下，则可能造成性能下降。</p><p><img src="/img/c10m/cacheline-1.png" alt="preview"></p><p>当两个CPU操作两个相邻的变量时，这段相同的数据被加载到内存中。这将产生数据竞争，为了保证一致性，性能必然会下降。</p><p><img src="/img/c10m/cacheline-2.png" alt="preview"></p><p>因此，<strong>保持CPU亲和性</strong>，就成为了C10M中需要重点关注的问题。其中端口复用、prefork、taskset这些技术比较值得尝试，这些方法可以将进程与CPU相绑定。我将这些资料都整理在了 <a href="https://github.com/guileen/c10m-test" target="_blank" rel="noopener">guileen/c10m-test</a></p><p>每次内存访问都是一个cache miss，解决方案是减少指针的使用。此外，内存的数据结构，如何减少GC，也是一个重点要考虑的问题，主要优化方法在之前的《Golang GC优化策略》的文章里描述过，我认为对象池是最有效的手段。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;C10m是继c10k问题之后提出的新问题，指单机1000万连接问题。40gbps网卡、32核、256G内存，这样的配置理论上已经可以处理千万并发连接了。但虽然硬件已经能够满足条件了，但是软件系统依然是复杂的。这就是C10M问题。&lt;/p&gt;
&lt;p&gt;使用Go语言开发的TCP se
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>推荐系统架构</title>
    <link href="http://guileen.github.io/2021/04/09/recommend-system-architecture/"/>
    <id>http://guileen.github.io/2021/04/09/recommend-system-architecture/</id>
    <published>2021-04-09T14:13:09.000Z</published>
    <updated>2021-04-10T06:53:24.568Z</updated>
    
    <content type="html"><![CDATA[<h2 id="0x00-架构目标"><a href="#0x00-架构目标" class="headerlink" title="0x00 架构目标"></a>0x00 架构目标</h2><p>推荐系统的目标简而言之是从全量Item（商品、内容、人）中为用户找到一个最符合业务价值的一部分有序子集。这个最有价值，通常要综合考虑用户的兴趣以及平台的商业模式。比如对于视频网站而言，用户的观看时长是最能代表这个价值的，而影评网站则要推荐用户评分最高的那些影片。</p><p>我们关心的问题是推荐效果和算法复杂度。</p><h2 id="0x01-架构设计"><a href="#0x01-架构设计" class="headerlink" title="0x01 架构设计"></a>0x01 架构设计</h2><p>按数据来划分可分为：大数据平台 =&gt; 召回层 =&gt; 排序层 =&gt; 融合层。总数据量，亿级，召回层千级、万级，排序层百级，融合层十级。</p><p>按计算来划分可分为：离线层=&gt; 近线层 =&gt; 在线层</p><p><img src="/img/recsys/netflix.jpeg" alt=""></p><p><a href="https://www.cnblogs.com/shengyang17/p/11546299.html" target="_blank" rel="noopener">https://www.cnblogs.com/shengyang17/p/11546299.html</a></p><h2 id="0x02-召回层算法"><a href="#0x02-召回层算法" class="headerlink" title="0x02 召回层算法"></a>0x02 召回层算法</h2><p>召回算法通常采用多种模型，尽可能多的召回物品。主要采用简单高效的算法，也包含一些简单的策略如热门物品、新发布物品等。协同过滤模型不适用用户和物品的特征数据，只使用用户行为数据。</p><h3 id="矩阵分解"><a href="#矩阵分解" class="headerlink" title="矩阵分解"></a>矩阵分解</h3><p>经典的协同过滤算法，泛化能力不足，已不再流行。现在更多的是采用矩阵分解方法，将评分矩阵分解为用户矩阵和物品矩阵的叉乘，相当于有一个K维的隐含特征，用户与物品在这个K维特征上相契合，则会有很高的推荐度。优化目标是最小化评分矩阵与预测矩阵的距离。</p><p><img src="/img/recsys/mf.png" alt=""></p><p><a href="https://antkillerfarm.github.io/ml/2016/12/29/Machine_Learning_16.html" target="_blank" rel="noopener">ALS算法</a> 一个离线的可以并行的算法，但缺点是无法增量更新。<a href="https://asset-pdf.scinapse.io/prod/30495595/30495595.pdf" target="_blank" rel="noopener">Fast incremental matrix factorization forrecommendation with positive-only feedback</a> 这篇论文介绍了一种使用 ISGD 算法进行矩阵分解的增量更新算法。</p><p><img src="/img/recsys/ISGD.png" alt=""></p><p>这个算法仅考虑了正样本，其中的 $ err_(ui)=1-A_uB_i^T $ 可以改进为 $ err_(ui)=R_(ui)-A_uB_i^T $。采用这个算法可以使我们的模型增量更新。</p><h2 id="0x03-排序层算法"><a href="#0x03-排序层算法" class="headerlink" title="0x03 排序层算法"></a>0x03 排序层算法</h2><p>在矩阵分解中，我们没有使用用户和物品的特征，这无疑是对信息的浪费。在排序层中，我们将问题转化为一个2分类问题，即 P(y=1|X)，用户喜欢一个物品的概率。其中X为 {用户特征、物品特征、场景特征}。这里有大量的机器学习方法可以使用，常用的有 LR（线性回归），FM/FFM（隐向量特征交叉），GBDT+LR（其中GBDT完成了特征工程部分，LR完成了回归部分）。</p><p>深度学习在这一层也有越来越多的应用，Embedding技术主要使用在特征工程的阶段。</p><h2 id="0x04-如何抽象一个推荐系统"><a href="#0x04-如何抽象一个推荐系统" class="headerlink" title="0x04 如何抽象一个推荐系统"></a>0x04 如何抽象一个推荐系统</h2>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;0x00-架构目标&quot;&gt;&lt;a href=&quot;#0x00-架构目标&quot; class=&quot;headerlink&quot; title=&quot;0x00 架构目标&quot;&gt;&lt;/a&gt;0x00 架构目标&lt;/h2&gt;&lt;p&gt;推荐系统的目标简而言之是从全量Item（商品、内容、人）中为用户找到一个最符合业务价
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>分布式事务</title>
    <link href="http://guileen.github.io/2021/03/11/distributed-transaction/"/>
    <id>http://guileen.github.io/2021/03/11/distributed-transaction/</id>
    <published>2021-03-11T13:24:22.000Z</published>
    <updated>2021-03-19T15:20:50.109Z</updated>
    
    <content type="html"><![CDATA[<p>微服务架构中，分布式事务、全链路跟踪、监控报警、限流降级、灰度发布、服务网关等等都很重要，大多是比较简单的工程性问题，有成熟的解决方案。其中在理论上比较复杂的，主要就是分布式事务了。</p><h2 id="异步场景的分布式事务"><a href="#异步场景的分布式事务" class="headerlink" title="异步场景的分布式事务"></a>异步场景的分布式事务</h2><p>主服务完成事务后将结果用事件（消息队列）通知从服务。从服务消费完成事务后，将事件删除（否则将持续收到事件通知）。这一模式的主要问题是，消息队列与主事务如何保持一致。解决方案是主事务中增加一个本地Msg表，事件投递成功后，删除本地Msg。若事件投递失败，由消息补偿定时任务将未投递消息写入消息服务。</p><p>目前看来是比较完美的，但是这一方案对主业务有很大的侵入性。因此可以考虑将Msg持久化独立为一个服务。在开始主事务前，先将Msg置为Prepare状态，然后主事务完成后，Commit Msg。如果Prepare Msg失败，则主事务不会开始，如果Msg Prepare失败，但没有Commit Msg，则Msg服务会向主服务回调检测任务是否完成。RocketMQ实现了类似的机制。这一模式的主要缺点是需要写一个回调检查方法。这种方法也被成为<strong>半投递</strong>。</p><p><img src="/img/dtx/half-message.jpg" alt=""></p><h2 id="同步场景分布式事务"><a href="#同步场景分布式事务" class="headerlink" title="同步场景分布式事务"></a>同步场景分布式事务</h2><h3 id="二阶段提交协议（2-Phase-Commit）（XA）"><a href="#二阶段提交协议（2-Phase-Commit）（XA）" class="headerlink" title="二阶段提交协议（2 Phase Commit）（XA）"></a>二阶段提交协议（2 Phase Commit）（XA）</h3><p><img src="/img/dtx/2pc.png" alt=""></p><p>2PC协议中，用户与协调者通信。事务的执行分为准备阶段和提交阶段。在准备阶段，完成资源的锁定。协调者收到所有的投票都为Yes后，通知所有参与者提交事务，否则通知参与者取消事务。为了完成事务，需要实现以下几个接口</p><ul><li>[参与者]canCommit(trans) -&gt; Yes/No  协调者询问参与者能否开始任务。参与者若返回yes，需要锁定相关资源。</li><li>[参与者]doCommit(trans)  协调者通知参与者执行他的事务。</li><li>[参与者]doAbort(trans) 协调者通知参与者放弃事务。</li><li>[协调者]haveCommitted(trans) 参与者调用协调者的该接口，通知协调者，自己已经完成了任务。</li><li>[协调者]getDecision(trans) 参与者投Yes后一段时间未收到通知，参与者主动询问表决结果，主动恢复事务。</li></ul><p>我们必须要考虑超时的情况：</p><ul><li>每一个网络动作都要包含一个超时的动作，超时并不意味失败。</li><li>当协调者调用canCommit超时时，事务将不会开始，协调者将向所有参与者发送doAbort。</li><li>当参与者在canCommit返回了No之后，参与者终止事务（没有超时）</li><li>当参与者在canCommit返回了Yes之后超时，我们称其进入了不确定状态，参与者需要调用getDecision来决定下一步的动作。如果协调者发生故障，需要继续检测getDecision，等待协调者恢复后则可恢复事务。（可能持续的等待）</li></ul><p>我们还需要考虑到进程崩溃的情况：</p><ul><li>当参与者回复了Yes之后崩溃，后继服务需要从数据库中恢复该事务。因此参与者在回复Yes之前必须将事务状态写入数据库。</li><li>当协调者崩溃后，需要正确的处理getDecision</li></ul><p>缺点：</p><ul><li>在一切正常的情况下，2PC的性能是2N次请求。但如果出现了异常，则可能出现长时间的等待，并锁定了相关资源。3PC用来解决这样的问题。</li></ul><h3 id="三阶段提交协议（3-Phase-Commit）"><a href="#三阶段提交协议（3-Phase-Commit）" class="headerlink" title="三阶段提交协议（3 Phase Commit）"></a>三阶段提交协议（3 Phase Commit）</h3><p><img src="/img/dtx/3pc.png" alt=""></p><p>3PC与2PC的异同：</p><ul><li>3PC将2PC的准备阶段拆分为『询问』和『准备』两个阶段。</li><li>在询问阶段，参与者不锁定资源，只返回是否可以执行。这一步避免了2PC最终表决为No却锁定了资源的情况。</li><li>当询问所有参与者都可以执行的情况下，才要求参与者进行准备，锁定资源。</li><li>询问、准备必须都成功，才会执行Commit，否则执行Abort。这与2PC是类似的。</li></ul><h3 id="TCC协议"><a href="#TCC协议" class="headerlink" title="TCC协议"></a>TCC协议</h3><p>TCC本质上依然是2PC，他们的区别是TCC是服务级别的，而2PC是资源级别的（也可以是服务级别的）。在2PC、3PC中，都会对资源进行长时间的占用，同一时间只能有一个事务执行，有一个锁竞争的问题。为了解决这个问题，TCC在Try阶段，就将事务所需的资源进行预留，后续的锁只发生在预留的资源上。</p><p><img src="/img/dtx/tcc.png" alt=""></p><p>为了解释这个问题，我们先来想象这样一种场景，用户在电商网站购买商品1000元，使用余额支付800元，使用红包支付200元。我们看一下在 2PC 中的流程：</p><p>Prepare 阶段：</p><ul><li>下单系统插入一条订单记录，不提交</li><li>余额系统减 800 元，给记录加锁，写 redo 和 undo 日志，不提交</li><li>红包系统减 200 元，给记录加锁，写 redo 和 undo 日志，不提交</li></ul><p>Commit 阶段：</p><ul><li>下单系统提交订单记录</li><li>余额系统提交，释放锁</li><li>红包系统提交，释放锁</li></ul><p>我们在事务执行过程中，锁定了整个用户账户。而TCC 在该场景中的流程：</p><p>Try操作</p><ul><li>tryX 下单系统创建待支付订单</li><li>tryY 冻结账户红包 200 元</li><li>tryZ 冻结资金账户 800 元</li></ul><p>Confirm操作</p><ul><li>confirmX 订单更新为支付成功</li><li>confirmY 扣减账户红包 200 元</li><li>confirmZ 扣减资金账户 800 元</li></ul><p>Cancel操作</p><ul><li>cancelX 订单处理异常，资金红包退回，订单支付失败</li><li>cancelY 冻结红包失败，账户余额退回，订单支付失败</li><li>cancelZ 冻结余额失败，账户红包退回，订单支付失败</li></ul><p>我们只对参与事务的部分资源进行了锁定，因此极大的降低了锁竞争的情况，也就提升了系统的性能。缺点是，TCC的实现对业务的侵入性较强，必须由开发人员来编写。而2PC、3PC则可以抽象为统一的框架。</p><h2 id="AT-模式"><a href="#AT-模式" class="headerlink" title="AT 模式"></a>AT 模式</h2><p>本地关系型数据库   [Seata AT模式]<a href="http://seata.io/zh-cn/docs/dev/mode/at-mode.html" target="_blank" rel="noopener">http://seata.io/zh-cn/docs/dev/mode/at-mode.html</a></p><p>两阶段提交协议的演变：</p><ul><li>一阶段：业务数据和回滚日志记录在同一个本地事务中提交，释放本地锁和连接资源。</li><li>二阶段：<ul><li>提交异步化，非常快速地完成。</li><li>回滚通过一阶段的回滚日志进行反向补偿。</li></ul></li></ul><p>TODO</p><h2 id="SAGA"><a href="#SAGA" class="headerlink" title="SAGA"></a>SAGA</h2><p>长事务解决方案，订机票的例子</p><p>TODO</p><p><a href="https://huzb.me/2019/06/30/%E5%88%86%E5%B8%83%E5%BC%8F%E4%BA%8B%E5%8A%A1%E2%80%94%E2%80%942PC%E3%80%813PC%E5%92%8CTCC/#%E4%BA%94%E3%80%81TCC-%E5%8D%8F%E8%AE%AE" target="_blank" rel="noopener">[1]分布式事务——2PC、3PC 和 TCC</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;微服务架构中，分布式事务、全链路跟踪、监控报警、限流降级、灰度发布、服务网关等等都很重要，大多是比较简单的工程性问题，有成熟的解决方案。其中在理论上比较复杂的，主要就是分布式事务了。&lt;/p&gt;
&lt;h2 id=&quot;异步场景的分布式事务&quot;&gt;&lt;a href=&quot;#异步场景的分布式事务&quot;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>为什么『太极』是一门好语言？</title>
    <link href="http://guileen.github.io/2021/02/23/why-taichi-is-good/"/>
    <id>http://guileen.github.io/2021/02/23/why-taichi-is-good/</id>
    <published>2021-02-23T14:08:00.000Z</published>
    <updated>2021-03-07T15:05:12.999Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/taichi-dev/taichi" target="_blank" rel="noopener">太极</a> 是由MIT的中国小伙胡渊鸣开发的一门编程语言。这不仅仅是又一个新的编程语言，我实在已经厌倦了不断有人造新的语言，来满足每个作者不同的语言怪癖。有许多的编程语言是没有存在的价值的，他们继续存在往往只是因为遗留历史项目还在维护。而『太极』是有开创性和工程价值的。</p><p>我认为太极有以下几个关键的<strong>有价值</strong>的特性：</p><ol><li>跨平台的GPU加速支持。同时支持CUDA、OpenGL、Metal，在Windows、Linux、MacOSX上都能很好的运行。我此前发现大部分的深度学习框架的GPU加速都是基于CUDA，这意味着我的macbook的GPU无法发挥作用。</li><li>性能卓越，比PyTorch快13.4倍、Tensorflow要快188倍<a href="https://www.qbitai.com/2020/01/10534.html" target="_blank" rel="noopener">[1]</a>。这两点都有着实实在在的经济价值。</li><li>简洁易学。作者并没有想要特地标新立异，而是基于python的语法来开发，与python完全兼容。这是非常务实的，但并非没有追求的，代码是会被静态编译执行的，性能与C++代码并没有太多差别。</li></ol><p>我认为太极在以下领域会有很好的应用：</p><ol><li>学术研究。</li><li>机器学习、图形学、物理引擎、游戏编程的教学和实验。</li><li>工具开发。可能作为最终产品的打包发布仍要探索，但制作工具是效率极高的。</li><li>深度学习、离线渲染等离线任务的工程级应用。</li></ol><p>对于一名仍在不断学习的中老年人而言，我不希望仅仅是学会一两个重复的工具，而是能够更高效的去实践、探索未知的世界。与其学习tensorflow这种集成式的深度学习框架，不如用太极自己实现一个，这样我们才能更深刻的理解其中的过程。我们还可以用太极将数据可视化的展示出来。</p><p>学习API是非常无趣的，但对于每一个学习编程的孩子来说，却是一道很高的门槛。很多人都无法实现在屏幕上绘制一个像素，这使我们哪怕拥有了足够的知识，依然缺乏表现力。有了太极，就仿佛打开了一道科研的大门。在科研过程中，我们需要的是快速实验、快速试错。太极就是为此而生的。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/taichi-dev/taichi&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;太极&lt;/a&gt; 是由MIT的中国小伙胡渊鸣开发的一门编程语言。这不仅仅是又一个新的编程语言，我实在已经厌倦了不断有人造新
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>设计一个3D游戏引擎</title>
    <link href="http://guileen.github.io/2021/02/20/design-a-3d-game-engine/"/>
    <id>http://guileen.github.io/2021/02/20/design-a-3d-game-engine/</id>
    <published>2021-02-20T13:22:04.000Z</published>
    <updated>2021-03-07T15:05:12.999Z</updated>
    
    <content type="html"><![CDATA[<p>最近学习了一些计算机图形学的知识，重新点燃了自己想要学习游戏开发的热情。但是，计算机图形学与游戏开发还是有很大区别的。计算机图形学关心的是如何高质量的还原真实世界的视觉效果，而游戏开发则要求一个可编程性更强的系统。因此游戏引擎，首先是为开发服务的。</p><p>游戏引擎，应该允许游戏开发者，轻松的定义实体，操控这些实体的位置、模型、滤镜，最终通过渲染引擎渲染到屏幕上。甚至要提供一些易用的图形化制作工具，比如关卡编辑器，地图编辑器，实体编辑器，脚本语言，配置表，这些输出成数据文件。这些数据将被加载到内存，通过渲染引擎输出到屏幕。</p><p><strong>[制作工具]</strong>–&gt;静态数据–&gt;<strong>[用户输入处理、AI update、物理引擎]</strong>–&gt;动态数据–&gt;<strong>[渲染引擎]</strong>–&gt;最终画面。</p><p>游戏引擎=制作工具+程序API+渲染引擎。</p><p>而游戏引擎的关键，就在于处理数据。</p><p>哪怕一个最简单的<a href="https://github.com/louis-gui/louis-gui.github.io/blob/main/likeasnakegame.cpp" target="_blank" rel="noopener">终端贪食蛇游戏</a>(这是我11岁的儿子写的)也需要符合这个标准。这个终端游戏制作工具不需要，他的素材是一些终端字符，google担任了这个制作工具的角色。程序API更新贪食蛇的动态数据，然后渲染引擎将动态数据映射成特定的终端字符打印在屏幕上。</p><p>开发一个3D游戏，数据结构更加复杂一些。理清了这个结构之后，3D游戏可能比2D游戏更容易开发一些，因为3D数据结构毕竟更接近真实世界。我们将整个待渲染的世界，称为【场景】，场景中摆放着各种【模型】。对于物理引擎部分，只需要模型的数据就可以了，而对于渲染引擎，则还需要【光源】和【摄像机】。</p><h2 id="物理引擎"><a href="#物理引擎" class="headerlink" title="物理引擎"></a>物理引擎</h2><p>一个粒子由*位置position、速度velocity、质量mass、力force（加速度acceleration）、阻尼damping（简化一个物体在一个环境中的摩擦力如0.999）等属性，还需要考虑地心引力gravity的影响。</p><p>用反质量invertMass模拟无限质量？</p><p><em>位置更新公式：</em>$$ p’=p+dot p t + 1/2 ddot p t^2 $$ 其中，$dot p$表示速度，$ddot p$表示加速度。在30fps时，t=0.033 后一项可以忽略。简化为 $$ p’=p+dot p t$$</p><p><em>速度更新公式:</em> $$ dot p’ = dot p d^t + ddot p t $$  d为阻尼，</p><p>以上模拟对于慢速物体没有问题，但对于子弹、炮弹这种高速物体则不适用，因为可能在一帧之内子弹已穿过物体，无法与目标发生碰撞。因此对于这种情况，我们则需要将子弹想象为一个激光（或抛物线）。对于目标所受到的冲击，则需要通过动量守恒、能量守恒公式进行计算（TODO）。</p><p>重力是恒定的，但还有很多其他的力是动态生成的，爆炸、发射、风。因此我们要创造一个『力生成器』在每一帧执行它的updateForce方法，来设定物体受到的外力。</p><p><strong>弹簧</strong>是一种普遍的模型，那些柔软的材质都可以抽象为大量的弹簧。根据胡克定理$$f=-k Delta l$$，我们可以写一个弹簧力生成器。</p><p>除了弹簧系统外，还有一类硬约束是紧密连接的对象，比如铁链、四肢。他们的关键在于连接点的速度是一致的。</p><p>然后我们要将所有的东西整合到一起成为一个物理引擎，在每一帧updatePhysics()</p><p>我们要将物理引擎从点升级到体，刚体的碰撞盒与他的质点。刚体的旋转。</p><p>碰撞检测（TODO）</p><p>参考：</p><p>《Game Physics Engine Development》2nd Edition by Ian Millington</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近学习了一些计算机图形学的知识，重新点燃了自己想要学习游戏开发的热情。但是，计算机图形学与游戏开发还是有很大区别的。计算机图形学关心的是如何高质量的还原真实世界的视觉效果，而游戏开发则要求一个可编程性更强的系统。因此游戏引擎，首先是为开发服务的。&lt;/p&gt;
&lt;p&gt;游戏引擎，
      
    
    </summary>
    
    
      <category term="OpenGL,Game development" scheme="http://guileen.github.io/tags/OpenGL-Game-development/"/>
    
  </entry>
  
  <entry>
    <title>BRDF双向反射分布函数</title>
    <link href="http://guileen.github.io/2021/02/18/brdf-pbr/"/>
    <id>http://guileen.github.io/2021/02/18/brdf-pbr/</id>
    <published>2021-02-18T13:12:37.000Z</published>
    <updated>2021-02-18T15:48:31.636Z</updated>
    
    <content type="html"><![CDATA[<p>双向反射分布函数（bidirectional reflectance distribution function）$f_r(omega_i,omega_r)$是一个计算光照反射量的函数。$omega_i$表示输入光角度，$omega_r$表示反射光角度，函数返回反射光辐射率。$omega$由球面坐标系的$phi$,$theta$角度表示，因此brdf函数共有4个参数。brdf的单位是每立体角$sr^(-1)$。</p><img src="/img/brdf/spherical-coordinates.png" style="width:50%;" /><img src="/img/brdf/solid-angle-1sr.png" style="width:45%;margin-top:5%;" /><h3 id="辐射度量学-Radiometry"><a href="#辐射度量学-Radiometry" class="headerlink" title="辐射度量学(Radiometry)"></a>辐射度量学(Radiometry)</h3><table><thead><tr><th>物理量</th><th>符号</th><th>公式</th><th>国际单位制</th><th>单位符号</th><th>注释</th></tr></thead><tbody><tr><td><a href="https://zh.wikipedia.org/wiki/辐射能" target="_blank" rel="noopener">辐射能</a>（Radiant energy）</td><td>$Q_e$</td><td></td><td><a href="https://zh.wikipedia.org/wiki/焦耳" target="_blank" rel="noopener">焦耳</a></td><td>$J$</td><td>能量。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射通量" target="_blank" rel="noopener">辐射通量</a>（Radiant flux）</td><td>$Phi_e$</td><td>$Phi=(dQ)/(dt)$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a></td><td>$W$</td><td>每单位时间的辐射能量，亦作“辐射功率”。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射强度" target="_blank" rel="noopener">辐射强度</a>（Radiant intensity）</td><td>$I_e$</td><td>$I=(dPhi)/(d omega)</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每<a href="https://zh.wikipedia.org/wiki/球面度" target="_blank" rel="noopener">球面度</a></td><td>$W*sr^(-1)$</td><td>每单位<a href="https://zh.wikipedia.org/wiki/立體角" target="_blank" rel="noopener">立体角</a>的辐射通量。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/輻照度" target="_blank" rel="noopener">辐照度</a>（Irradiance）（辉度）</td><td>$E_e$</td><td>$E=(dPhi)/(dA)=int_(Omega)  L(omega)cos theta d omega$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每平方米</td><td>$W*m^(-2)$</td><td>入射表面的辐射通量</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射率" target="_blank" rel="noopener">辐射率</a>（Radiance）(光亮度）</td><td>$L_e$</td><td>$(d^2Phi)/(dAcos theta d omega)$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每<a href="https://zh.wikipedia.org/wiki/球面度" target="_blank" rel="noopener">球面度</a>每平方米</td><td>$W*sr^(-1)*m^(-2)$</td><td>每单位<a href="https://zh.wikipedia.org/wiki/立體角" target="_blank" rel="noopener">立体角</a>每单位投射表面的<a href="https://zh.wikipedia.org/wiki/辐射通量" target="_blank" rel="noopener">辐射通量</a>。<strong>相当于辐射强度在dA上的微分</strong></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>BRDF由Fred Nicodemus在1965年提出，函数如下：</p><p>$$f_r(omega_i,omega_r) = (dL_r(omega_r))/(dE_i(omega_i)) = (dL_r(omega_r))/(L_i(omega_i)cos theta_i d omega_i) $$</p><img src="/Users/admin/work/guileen.github.com/hexo/source/img/brdf/brdf-diagram.png" style="width:50%;" /><p>这个公式之所以定义为辐射率（radiance）和辐照度（irradiance）之比，而不是radiance和radiance之比，或irradiance和irradiance之比。是因为当考虑入射时，我们需要考虑入射光在面积上的分量，所以irradiance译为辐<strong>照</strong>度。当考虑反射时，我们需要考虑每立体角的辐射通量，并且这个辐射通量最终投影在屏幕（视网膜）面积上的辐射通量，因此我们用辐射率。如果我们用点光源，入射光的计算似乎也是可以用辐射率的，但有时我们还要考虑平行光的情况，那么对于入射光就不存在每立体角的概念了，因此对于入射光照我们用辐照度，反射我们用辐射率。</p><h3 id="基于物理的BRDF模型-PBR，Physically-based-rendering"><a href="#基于物理的BRDF模型-PBR，Physically-based-rendering" class="headerlink" title="基于物理的BRDF模型(PBR，Physically-based rendering)"></a>基于物理的BRDF模型(PBR，Physically-based rendering)</h3><h4 id="次表面散射（Subsurface-scattering）"><a href="#次表面散射（Subsurface-scattering）" class="headerlink" title="次表面散射（Subsurface scattering）"></a>次表面散射（Subsurface scattering）</h4><p>是一些半透明物质比如皮肤、玉石、大理石、塑料等。当光入射到材料表面后，一部分被反射、一部分被吸收、还有一部分经历透射，透射光在材料内部进行多次不规则的反射之后，又从不同角度反射了回来。</p><h4 id="菲涅尔反射（Fresnel-Reflectance）"><a href="#菲涅尔反射（Fresnel-Reflectance）" class="headerlink" title="菲涅尔反射（Fresnel Reflectance）"></a>菲涅尔反射（Fresnel Reflectance）</h4><p>当光从一种折射率为$n_1$的介质向另一种折射率为$n_2$的介质传播时，在两者的交界处可能会同时发生光的反射和折射。<a href="https://zh.wikipedia.org/wiki/%E8%8F%B2%E6%B6%85%E8%80%B3%E6%96%B9%E7%A8%8B" target="_blank" rel="noopener">菲涅尔方程</a>描述了光波的不同分量被折射和反射的情况，也描述了波反射时的相变。光线会随着我们的观察角度而反射不同的亮度，当我们以垂直与水面的角度观察池塘时，我们可以看到池塘的底部，但当我们以平行于水面的角度观察水面时，反射光则会很强我们无法看到池底。</p><h4 id="微表面理论（Microfacet-Theory）"><a href="#微表面理论（Microfacet-Theory）" class="headerlink" title="微表面理论（Microfacet Theory）"></a>微表面理论（Microfacet Theory）</h4><p>微表面理论假设材质的表面是由不同方向的微小细节平面（microfacet）所构成，反射光线由这些微表面的法线分布决定。我们用法线分布函数（Normal Distribution Function，NDF），D(h) 来描述表面的法线分布概率。h表示视角与入射光角度之间的半程向量。</p><p><img src="/img/brdf/microfacet.jpg" alt=""></p><p>$$f(i,o) = (F(i,h)G(i,o,h)D(h))/(4(n,i)(n,o))$$ </p><p>其中F(i,h)表示菲涅尔项，表示所有反射的比例。G(i,o,h) 表示自投影项，当光线几乎平射于微表面时，光线则将被粗糙的表面自我遮挡掉。D(h)表示法线分布。</p><p>参考:</p><p><a href="https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function" target="_blank" rel="noopener">Wikipedia:《Bidirectional reflectance distribution function》</a></p><p>《Real-Time Rendering, 4th edition》</p><p><a href="https://github.com/QianMo/Real-Time-Rendering-3rd-CN-Summary-Ebook" target="_blank" rel="noopener">《Real-Time Rendering 3rd》提炼总结</a></p><p><a href="https://zhuanlan.zhihu.com/p/20119162" target="_blank" rel="noopener"> Microfacet材质和多层材质——文刀秋二</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;双向反射分布函数（bidirectional reflectance distribution function）$f_r(omega_i,omega_r)$是一个计算光照反射量的函数。$omega_i$表示输入光角度，$omega_r$表示反射光角度，函数返回反射光辐射率
      
    
    </summary>
    
    
      <category term="computer-graphics" scheme="http://guileen.github.io/tags/computer-graphics/"/>
    
  </entry>
  
  <entry>
    <title>线性代数的本质</title>
    <link href="http://guileen.github.io/2021/02/13/the-essence-of-linear-algebra/"/>
    <id>http://guileen.github.io/2021/02/13/the-essence-of-linear-algebra/</id>
    <published>2021-02-13T04:03:51.000Z</published>
    <updated>2021-03-07T15:05:12.999Z</updated>
    
    <content type="html"><![CDATA[<p>当我在大学中学习线性代数的时候，我不知所云且不以为然。然后随着不断的学习，我发现不懂线性代数是没法在更深的技术领域里混的。比如机器学习、计算机图形学等等，对于其他的科研领域也都是同样的。如果学不好线性代数既不是我的问题，也不是线性代数的问题，那到底是什么问题？最近学些了<a href="https://www.bilibili.com/video/BV1X7411F744" target="_blank" rel="noopener">现代计算机图形学入门-闫令琪</a> <a href="https://www.bilibili.com/video/BV1ib411t7YR" target="_blank" rel="noopener">线性代数的本质-3blue1brown</a> 这两个系列视频使我对线性代数多了更多的感性认识，而<a href="https://www.marxists.org/chinese/maozedong/marxist.org-chinese-mao-193707.htm" target="_blank" rel="noopener">《实践论》</a> 告诉我们，<strong>理性认识是要依赖于感性认识的</strong>。传统的线性代数教材则希望构建一套完全自洽的纯粹的数学理论，它不依赖于现实世界的知识。这或许可以满足一些数学家的成就感，但这种马后炮式的“创造”是脱离历史的。线性代数就像其他的学科一样，不可能是仅凭想象产生的，虽然数学家可以伪装成不依赖历史发展而独立自洽，但这除了是一种智力游戏外，对于认识世界、传播知识并没有任何帮助。我们就在这种缺乏感性认识的数学教育中丧失了对数学理论的兴趣，岂不哀哉？</p><p>吐槽结束，进入正题。谈谈我此刻对线性代数的理解，探讨一下他的本质到底是什么。我们是否会问自己，加减乘除的本质到底是什么？我们之所以不这么问，是因为我们已经理解了四则运算的本质。我们不会问关于十进制的本质，因为日常生活中已经给我们建立了足够多的经验。但我们在学龄前的阶段，我们则可能对十进制和加减乘除充满了困惑。但我们接触线性代数太晚，我们并没有足够的练习和日常应用使我们建立起感性认识。当我们在商店里消费的时候四则运算不断强化着我们的认知，但线性代数缺少这样的机会。当我们被教授四则运算时，老师把我们当作一个普通的人类，会告诉我们3个苹果+2个苹果=5个苹果，这种现实世界的例子帮助我们更好的理解了四则运算。但当我们学习线性代数时，我们则变成了一个个抽象的理性机器，这个系统只告诉我们各种定义、运算规则，然后要求我们像计算机一样的运行，计算出结果。What are we doing？我们怎么可能不懵圈呢？线性代数就是一个增强版的加减乘除，但没有足够的案例使我们不知道我们的计算究竟代表着什么？AlphaGo就算赢了李世石，但他不知道自己在干什么。我们作为人类的尊严在哪里？我又没控制好自己的情绪，让我们回到正题。《线性代数及其应用》是一本很好的教材，他和国内教材最大的区别就在于“应用”上，这本书中列举了大量的例子来说明线性代数的应用。这本书的开头说道“<strong>线性代数是一门语言，必须用学习外语的方法每天学习这种语言</strong>”。</p><h3 id="鸡兔同笼与线性变换"><a href="#鸡兔同笼与线性变换" class="headerlink" title="鸡兔同笼与线性变换"></a>鸡兔同笼与线性变换</h3><p>我们从鸡兔同笼来举个例子。鸡兔同笼是小学阶段的奥数题，也就是在小学的数学语言中，这是一道很难描述的题。到了中学阶段我们可以用未知数x表示鸡的数量，未知数y表示兔的数量，并列出方程。而对于线性代数的语言，我们用向量$$((a),(b))$$表示鸡和兔的数量，如果我们有非常多的未知数，我们不希望定义太多的未知数符号，我们直接用$$x$$表示这个n维变量。我们有一个变换矩阵$$[[1,1],[2,4]]$$ 表示鸡有1个头2只脚，兔有1个头4只脚 。如果有3只鸡5只兔则 $$[[1,1],[2,4]]*[[3],[5]]=[[3*1+5*1],[3*2+5*4]]=[[8],[26]]$$，它代表着我们将一个“鸡兔向量”映射到了“头脚向量”的空间中，共有8只头，26只脚。</p><p>我们知道函数是一种映射，$$f(x)=y$$代表将$x$到$y$的映射关系。矩阵乘法叫做线性变换，线性变换是一种函数映射，但函数映射不一定是线性变化。因此线性变换是符合函数的性质的。如果函数是可逆的，则有$$x=f^(-1)(y)$$，同样的，对于矩阵而言，$$若A是可逆的，且Ax=b，则x=A^(-1)b。设A=[[a,b],[c,d]]，则A^(-1)=1/(ad-bc)[[d,-b],[-c,a]]$$。</p><p>对于鸡兔同笼问题，$$A=[[1,1],[2,4]]，则A^(-1)=1/2[[4,-1],[-2,1]]$$。$$当有8头26脚时，x=1/2[[4,-1],[-2,1]]*[[8],[26]]=1/2[[4*8-26],[-2*8+26]]=1/2[[6],[10]]=[[3],[5]]$$，即3只鸡5只兔。最重要的是，这整个计算过程，计算机可以轻松的完成，并且可以用定义标准化的操作，因为操作标准化，计算机可以被设计的更加擅长处理这类操作。这就是线性代数得到广泛应用的一个最重要原因。</p><p>线性变换可能进行多次，就像映射可以进行多次一样。因为矩阵的乘法就是一种特殊的函数，函数满足结合律$$g(f(h(x)))=((g @ f)(h(x)))$$，所以矩阵乘法也符合结合律$$A(BC)=(AB)C$$。多次映射之后是一个新的映射，多次变换之后也是一个新的变换，所以我们可以将这些变换矩阵预先乘好，以增加每次计算的效率。也可以将一个复杂变换拆解为多个简单变换，使我们能更好的理解其性质。</p><p>我们可以从鸡兔变换到头脚，我们也可以从产量变换到成本收益（这是经济学的应用），我们也可以从速度变换到阻力（这是空气动力学的应用），我们也可以将3D空间变换到3D或2D空间（这是计算机图形学的应用），我们也可以将用户行为维度变换到兴趣标签维度（这是机器学习推荐系统的应用）。这都说明了线性代数是一门“语言”，是一个工具。并不是因为线性代数，所以这些定理存在，而是因为这些规律本身存在，才能有线性代数这门工具。人类是巧妙的“发明”了线性代数，而不是“发现”了线性代数。线性代数这门语言可以使我们避免基本代数语言的变量名爆炸。人类日常语言中有你我他这那等代词，中文有甲乙丙丁这种天干地支可以用作代词，英文则有a,b,cd可以使用。日常的代词使用是随意的，很多时候是不严谨的。代数学的代词在使用前则需要对它进行准确的定义。而线性代数则将最常使用的一些操作提取了出来，使我们免于重复的定义大量性质类似的代词。在线性代数中，鸡兔数量，头脚数量与空间中点的xyz轴位置都是同一种性质的。研究飞机表面的气流的过程包含了反复求解大型的线性方程组$$Ax=b$$，涉及的变量个数达到2百万个。可以说线性代数的发展完全是随着其应用发展的，如果有一本按照历史发展顺序描述线性代数的书，一定可以达到很好的教学效果。</p><h3 id="行列式（determinant）"><a href="#行列式（determinant）" class="headerlink" title="行列式（determinant）"></a>行列式（determinant）</h3><p>行列式的出现是远早于矩阵的。Determinant是决定的意思，（下文称之为决定值，“行列式”翻译的无味，既没有描述其决定性质，也没有说明其结果是特定的值）：决定值是否不为0决定了一个线性方程组是否有唯一解。所谓线性方程组，就是一次方程组。这个结论最早见于《九章算术》（有一种观点认为很多思想是在明朝由中国传至欧洲，而清朝恰好是中国的一个倒退，而欧洲则顺势伪造了一套其文明独立发展的历史叙事，这里不展开描述了）。决定值的这个性质在欧洲由莱布尼茨最早提出（莱布尼茨在中学西传中扮演着重要角色）。高斯首先使用了determinant这个词，他在数论理论中大量用到了决定值。后来这个词就更多的指一个特殊的函数，即某个表达式，因此中文将其翻译为行列式。</p><p>观察我们在鸡兔同笼中得出的结论：$$设A=[[a,b],[c,d]]，则A^(-1)=1/(ad-bc)[[d,-b],[-c,a]]$$。因此矩阵A当且仅当$$ad-bc!=0$$时存在逆矩阵，我们记为$$det(A)=|A|=|[a,b],[c,d]|=ad-bc$$。我们继续推广研究$3xx3$矩阵，可以得到：</p><p>$$|[a,b,c],[d,e,f],[h,i,j]|=a|[e,f],[h,i]|-b|[d,f],[g,i]|+c|[d,e],[g,h]|=aei+bfg+cdh-ceg-bdi-afh=|[a,d,h],[b,e,i],[c,f,j]|$$</p><p>对$4xx4$矩阵则有：</p><p>$$|[a,b,c,d],[e,f,g,h],[i,j,k,l],[m,n,o,p]|=a|[f,g,h],[j,k,l],[n,o,p]|-b|[e,g,h],[i,k,l],[m,o,p]|+c|[e,f,h],[i,j,l],[m,n,p]|-d|[e,f,g],[i,j,k],[m,n,o]|$$</p><p>依此类推，决定值的公式是一个递归。决定值的几何意义代表代表$1xx1$矩阵变换后的平行四边形的面积，或者说是矩阵变换后面积的放大倍数，即变换矩阵围成的四边形的面积。简单的证明如下图：$S=(a+c)(b+d)-ab-cd-2bc=ad-bc$。推广到3维则表示变换矩阵的围成的立方体的体积。</p><img src="/img/math/transform-determinant-opt.png" style="width:45%;" /><img src="/img/math/determinant-area.gif" style="width:45%;" /><p>如果determinant=0，则说明变换之后由面变为了线，或由体变为了面。而线或面无法再变换回面或体。我们同样以鸡兔同笼问题来举例，我们把题目中的兔换成鸭，则变换矩阵为$$A=[[1,1],[2,2]], det(A)=0$$。这个方程组是没有唯一解的，因为无论鸡鸭的比例如何，头脚的比例都是$1:2$。对于这样的矩阵，我们称他的秩为1，秩代表矩阵的维数。一个二维矩阵，可能秩为1，一个三维矩阵可能秩为2也就是一个面，也可能秩为1也就是一条线，甚至秩为0。</p><p>TODO 叉乘表示面积和垂直与平面的向量，特征值与特征向量，表示空间中在线性变换中保持稳定的轴，最小二乘法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当我在大学中学习线性代数的时候，我不知所云且不以为然。然后随着不断的学习，我发现不懂线性代数是没法在更深的技术领域里混的。比如机器学习、计算机图形学等等，对于其他的科研领域也都是同样的。如果学不好线性代数既不是我的问题，也不是线性代数的问题，那到底是什么问题？最近学些了&lt;a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>从余弦定理到冯氏光照模型</title>
    <link href="http://guileen.github.io/2021/02/07/law-of-cosines-and-phong-reflection-model/"/>
    <id>http://guileen.github.io/2021/02/07/law-of-cosines-and-phong-reflection-model/</id>
    <published>2021-02-06T17:01:33.000Z</published>
    <updated>2021-02-07T00:12:00.889Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-勾股定理——宇宙的密码"><a href="#1-勾股定理——宇宙的密码" class="headerlink" title="1.勾股定理——宇宙的密码"></a>1.勾股定理——宇宙的密码</h3><p>$a^2+b^2=c^2$。下图是勾股定理的一个直观证明。</p><p><img src="/img/math/ggdl.png" alt=""></p><h3 id="2-余弦定理，角与边的关系"><a href="#2-余弦定理，角与边的关系" class="headerlink" title="2. 余弦定理，角与边的关系"></a>2. 余弦定理，角与边的关系</h3><img src="/img/math/q-cosine.svg.png" width="30%"><img src="/img/math/cosine-1.png" width="30%"> <img src="/img/math/cos-sin.png" width="30%"><p>三条边可以确定一个三角形，已知三角形的三条边长，如何求出其角度呢？</p><p>由$cos,sin$定义可知<br>  $$ c = a * cos beta + b * cos alpha $$<br>  两边同乘c得：<br>$$ c^2 = ac * cos beta + bc * cos alpha $$<br>  同理可得：<br>$$ a^2 = ac cos beta + ab * cos gamma $$<br>$$ b^2 = bc cos alpha + ab cos gamma $$<br>  故：$$ a^2+b^2-c^2 = 2abcosgamma $$<br>  可得：$$ c^2 = a^2 + b^2 - 2ab cos gamma $$<br></p><h3 id="3-向量的定义（方向）"><a href="#3-向量的定义（方向）" class="headerlink" title="3. 向量的定义（方向）"></a>3. 向量的定义（方向）</h3><img src="/img/math/vector_subtraction.svg.png" width="45%"><img src="/img/math/vector_addition.svg.png" width="45%"><p>$$令 vec c = vec a - vec b$$, $$theta$$为$$vec a$$ $$vec b$$ 的夹角。余弦定理可以用向量形式写成 $$ | vec c |^2 = |vec a|^2 + |vec b|^2 -  2 |vec a| |vec b| cos theta $$ </p><h3 id="4-点积（dot-product）的代数定义"><a href="#4-点积（dot-product）的代数定义" class="headerlink" title="4. 点积（dot product）的代数定义"></a>4. 点积（dot product）的代数定义</h3><p>两个向量的点积是一个标量。向量$$vec a=[a_1, a_2, … a_n]$$与向量$$vec b=[b_1, b_2, … b_n]$$的点积定义为: $$ vec a * vec b = sum_(i=1)^n a_i b_i = a_1 b_1 + a_2 b_2 + … a_n b_n $$。</p><p>点积有以下性质（证略）：</p><ol><li>满足交换律 $$vec a * vec b = vec b * vec a$$</li><li>满足分配律 $$vec a * (vec b + vec c) = vec a * vec b + vec a * vec c$$</li><li>乘以标量时满足 $$ (c_1 vec a) * (c_2 vec b) = (c_1 c_2)(vec a * vec b)$$</li><li>不满足结合律。因为标量 $$ vec a * vec b $$ 与向量 $$ vec c $$ 的点积没有定义，所以$$(vec a * vec b) * vec c=vec a * (vec b * vec c)$$ 没有意义。</li></ol><p>点积的代数定义简单实用，易于表示，也易于使用计算机程序处理。是线性代数的基本操作之一。</p><h3 id="5-点积的几何意义"><a href="#5-点积的几何意义" class="headerlink" title="5. 点积的几何意义"></a>5. 点积的几何意义</h3><p>对于任何一个n维向量有 $|vec a|^2=a_1^2+a_2^2+…+a_n^2$。根据勾股定理，这是很显然的。换个角度<strong>说如果没有勾股定理，这一步就不存在，后面的内容也不存在了。而勾股定理不是由代数方法证明的，而是独立于代数系统之外的空间基本性质。而空间和时间是宇宙最根本的本质。这就是勾股定理最神奇的地方</strong>。</p><p>我们根据点积的定义可知：$$ vec a * vec a = a_1 * a_1 + a_2 * a_2 + … a_n * a_n = |vec a|^2$$ 即 $$ vec a * vec a == |vec a|^2$$</p><p>我们根据余弦定理的向量表示可得：$$ vec c * vec c = vec a * vec a + vec b * vec b - 2 |vec a| |vec b| cos theta . (1)$$ </p><p>根据向量的定义 $$ vec c = vec a - vec b $$ 有 $$ vec c * vec c = (vec a - vec b) * (vec a - vec b) = vec a * vec a + vec b * vec b - 2 vec a * vec b . (2)$$</p><p>结合等式$$(1)$$、$$(2)$$有 $$vec a * vec b = |vec a| |vec b| cos theta$$。一个看似简单的代数点积操作，竟然和夹角余弦相关，真是不可思议。</p><p>点积的几何意义是什么呢？关键就在这个$cos theta$，如果$$|vec b|$$为1时候，我们可以将$$vec a * vec b$$视为$$vec a $$在$$vec b$$方向上的投影长度。</p><p><img src="/img/math/dot_product_1.png" width="45%"> <img src="/img/math/dot_product_2.png" width="45%"></p><h3 id="6-点积的物理意义（从数学到宇宙）"><a href="#6-点积的物理意义（从数学到宇宙）" class="headerlink" title="6. 点积的物理意义（从数学到宇宙）"></a>6. 点积的物理意义（从数学到宇宙）</h3><p>点积的物理意义就是向量在某方向上的投影长度。这在物理上可以表达力在某方向上的投影，光在某方向的投影，速度、加速度在某方向的投影。而点积的操作，可以使我们只需要关心这些物理量的向量表示，而不需要去关心夹角，不需要去计算三角函数。而在统计学、机器学习等方面，余弦可以表示两个向量之间的相似性，比如两个词向量，两个用户的兴趣向量等，应用非常广泛。下面就以计算机图形学举例来说明点积的应用。</p><p>冯氏光照模型将一个物体的光照分解为环境光+漫反射光+镜面反射光。</p><p><img src="/img/math/Phong_components_version_4.png" alt=""></p><p>环境光比较简单就是一个常量。而漫反射光，则为光照强度在平面的法线方向的投影，与法线方向一致则光照最强。镜面反射光则为反射光方向在视角方向上的投影，与视角完全一致，则反射光最强。</p><p><img src="/img/math/diffuse_light.png" width="45%"> <img src="/img/math/specular_light.png" width="45%"></p><p>OpenGL的shader大致如下：</p><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="type">vec3</span> CalcDirLight(DirLight light, <span class="type">vec3</span> normal, <span class="type">vec3</span> viewDir) &#123;</span><br><span class="line">    <span class="comment">// normalize 归一化，使法线向量的长度为1</span></span><br><span class="line">    <span class="type">vec3</span> lightDir = <span class="built_in">normalize</span>(-light.direction);</span><br><span class="line">    <span class="comment">// 漫反射光. dot 计算cos* 强度， max把负值最多降到0，表示全黑。</span></span><br><span class="line">    <span class="type">float</span> diff = <span class="built_in">max</span>(<span class="built_in">dot</span>(normal, lightDir), <span class="number">0.0</span>);</span><br><span class="line">    <span class="type">vec3</span> reflectDir = <span class="built_in">reflect</span>(-lightDir, normal);</span><br><span class="line">    <span class="comment">// dot 计算反射光在视角上的cos分量，至少为0。使用pow，模拟镜面光焦点分布集中度，shininess越高要求反射分量越接近于1</span></span><br><span class="line">    <span class="comment">// 反射分量==1 表示必须视角恰巧与反射角完全一致才能看到反射光，也就是绝对镜面</span></span><br><span class="line">    <span class="type">float</span> spec = <span class="built_in">pow</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(viewDir, reflectDir), <span class="number">0.0</span>), material.shininess);</span><br><span class="line">    <span class="comment">// 合并冯氏光照结果</span></span><br><span class="line">    <span class="type">vec3</span> ambient = light.ambient * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.diffuse, TexCoords));</span><br><span class="line">    <span class="type">vec3</span> diffuse = light.diffuse * diff * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.diffuse, TexCoords));</span><br><span class="line">    <span class="type">vec3</span> specular = light.specular * spec * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.specular, TexCoords));</span><br><span class="line">    <span class="keyword">return</span> (ambient + diffuse + specular);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-勾股定理——宇宙的密码&quot;&gt;&lt;a href=&quot;#1-勾股定理——宇宙的密码&quot; class=&quot;headerlink&quot; title=&quot;1.勾股定理——宇宙的密码&quot;&gt;&lt;/a&gt;1.勾股定理——宇宙的密码&lt;/h3&gt;&lt;p&gt;$a^2+b^2=c^2$。下图是勾股定理的一个直观
      
    
    </summary>
    
    
      <category term="OpenGL,math" scheme="http://guileen.github.io/tags/OpenGL-math/"/>
    
  </entry>
  
  <entry>
    <title>学习CMake</title>
    <link href="http://guileen.github.io/2021/01/11/learn-cmake/"/>
    <id>http://guileen.github.io/2021/01/11/learn-cmake/</id>
    <published>2021-01-11T12:56:32.000Z</published>
    <updated>2021-02-06T12:30:37.115Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://hsf-training.github.io/hsf-training-cmake-webpage/" target="_blank" rel="noopener">https://hsf-training.github.io/hsf-training-cmake-webpage/</a></p><h2 id="1-构建"><a href="#1-构建" class="headerlink" title="1. 构建"></a>1. 构建</h2><h3 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/CLIUtils/CLI11.git</span><br><span class="line">cd CLI11</span><br><span class="line">cmake -S . -B build</span><br><span class="line">cmake --build build</span><br><span class="line">cmake --build build --target test</span><br></pre></td></tr></table></figure><h3 id="另一种构建"><a href="#另一种构建" class="headerlink" title="另一种构建"></a>另一种构建</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make test</span><br></pre></td></tr></table></figure><p><strong>永远不要</strong>在源码目录直接 <code>cmake .</code> 这样会污染源码目录。</p><h3 id="选择编译器"><a href="#选择编译器" class="headerlink" title="选择编译器"></a>选择编译器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CC=clang CXX=clang++ cmake -S . -B build</span><br></pre></td></tr></table></figure><h3 id="设置选项-D-列出选项-L"><a href="#设置选项-D-列出选项-L" class="headerlink" title="设置选项 -D 列出选项 -L"></a>设置选项 <code>-D</code> 列出选项 <code>-L</code></h3><ul><li><a href="https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html" target="_blank" rel="noopener"><code>CMAKE_BUILD_TYPE</code></a>:  <code>Release</code>, <code>RelWithDebInfo</code>, <code>Debug</code>, 或其他编译选项。</li><li><a href="https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html" target="_blank" rel="noopener"><code>CMAKE_INSTALL_PREFIX</code></a>: 安装位置，Unix上默认是 <code>/usr/local</code> , 用户安装目录常是 <code>~/.local</code> </li><li><a href="https://cmake.org/cmake/help/latest/variable/BUILD_SHARED_LIBS.html" target="_blank" rel="noopener"><code>BUILD_SHARED_LIBS</code></a>:  <code>ON</code> or <code>OFF</code> </li><li><a href="https://cmake.org/cmake/help/latest/module/CTest.html" target="_blank" rel="noopener"><code>BUILD_TESTING</code></a>: </li></ul><h3 id="调试Cmake-files-在source目录执行下面的命令："><a href="#调试Cmake-files-在source目录执行下面的命令：" class="headerlink" title="调试Cmake files 在source目录执行下面的命令："></a>调试Cmake files 在source目录执行下面的命令：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake build --trace-source="CMakeLists.txt"</span><br></pre></td></tr></table></figure><h2 id="2-编写-CMakeLists-txt"><a href="#2-编写-CMakeLists-txt" class="headerlink" title="2. 编写 CMakeLists.txt"></a>2. 编写 CMakeLists.txt</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>)</span><br><span class="line"><span class="comment"># 项目名称，未设置LANGUAGES的话，则是 C, CXX 的混合项目</span></span><br><span class="line"><span class="keyword">project</span>(MyProject)</span><br><span class="line"><span class="comment"># 至少一个 add_executeable 或 add_library 来作为target。</span></span><br><span class="line"><span class="keyword">add_executable</span>(myexample simple.cpp)</span><br></pre></td></tr></table></figure><p>可以设置更多信息</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最小。。最大版本</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>...<span class="number">3.18</span>)</span><br><span class="line"><span class="comment"># 更详细的项目信息</span></span><br><span class="line"><span class="keyword">project</span>(MyProject</span><br><span class="line">  VERSION</span><br><span class="line">    <span class="number">1.0</span></span><br><span class="line">  DESCRIPTION</span><br><span class="line">    <span class="string">"Very nice project"</span></span><br><span class="line">  LANGUAGES</span><br><span class="line">    CXX</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可添加 STATIC, SHARED, or MODULE; 默认通过 BUILD_SHARED_LIBS 选择.</span></span><br><span class="line"><span class="keyword">add_library</span>(mylibrary simplelib.cpp)</span><br></pre></td></tr></table></figure><h3 id="Target可用的设置"><a href="#Target可用的设置" class="headerlink" title="Target可用的设置"></a>Target可用的设置</h3><ul><li><a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html" target="_blank" rel="noopener"><code>target_link_libraries</code></a>: Other targets; can also pass library names directly</li><li><a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html" target="_blank" rel="noopener"><code>target_include_directories</code></a>: Include directories</li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_features.html" target="_blank" rel="noopener"><code>target_compile_features</code></a>: The compiler features you need activated, like <code>cxx_std_11</code></li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_definitions.html" target="_blank" rel="noopener"><code>target_compile_definitions</code></a>: Definitions</li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_options.html" target="_blank" rel="noopener"><code>target_compile_options</code></a>: More general compile flags</li><li><a href="https://cmake.org/cmake/help/latest/command/target_link_directories.html" target="_blank" rel="noopener"><code>target_link_directories</code></a>: Don’t use, give full paths instead (CMake 3.13+)</li><li><a href="https://cmake.org/cmake/help/latest/command/target_link_options.html" target="_blank" rel="noopener"><code>target_link_options</code></a>: General link flags (CMake 3.13+)</li><li><a href="https://cmake.org/cmake/help/latest/command/target_sources.html" target="_blank" rel="noopener"><code>target_sources</code></a>: Add source files</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不需要添加任何source，导出一个 header-only library。</span></span><br><span class="line"><span class="keyword">add_library</span>(some_header_only_lib INTERFACE)</span><br></pre></td></tr></table></figure><h4 id="什么是-INTERFACE-IMPORETED？？"><a href="#什么是-INTERFACE-IMPORETED？？" class="headerlink" title="什么是 INTERFACE IMPORETED？？"></a>什么是 INTERFACE IMPORETED？？</h4><h3 id="Script"><a href="#Script" class="headerlink" title="Script"></a>Script</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cache.cmake</span></span><br><span class="line"><span class="comment"># 设置变量</span></span><br><span class="line"><span class="keyword">set</span>(MY_VARIABLE <span class="string">"I am a variable"</span>)</span><br><span class="line"><span class="keyword">message</span>(STATUS <span class="string">"$&#123;MY_VARIABLE&#125;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(MY_CACHE_VAR <span class="string">"I am a cached variable"</span> CACHE <span class="keyword">STRING</span> <span class="string">"Description"</span>)</span><br><span class="line"><span class="keyword">message</span>(STATUS <span class="string">"$&#123;MY_CACHE_VAR&#125;"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DMY_CACHE_VAR="command line" -P cache.cmake</span><br></pre></td></tr></table></figure><p>Try setting a cached variable using <code>-DMY_VARIABLE=something</code> <em>before</em> the <code>-P</code>. Which variable is shown?</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">option</span>(MY_OPTION <span class="string">"On or off"</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="comment"># $ENV&#123;name&#125;</span></span><br><span class="line"><span class="comment"># if(DEFINED ENV&#123;name&#125;)  </span></span><br><span class="line"><span class="keyword">file</span>(GLOB OUTPUT_VAR *.cxx)</span><br><span class="line"><span class="keyword">file</span>(GLOB_RECURSE  OUTPU_VAR *.cxx)</span><br></pre></td></tr></table></figure><h2 id="3-项目结构"><a href="#3-项目结构" class="headerlink" title="3. 项目结构"></a>3. 项目结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">code&#x2F;03-structure&#x2F;</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── README.md</span><br><span class="line">├── apps</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── app.cpp</span><br><span class="line">├── cmake</span><br><span class="line">│   └── FindSomeLib.cmake</span><br><span class="line">├── docs</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── mainpage.md</span><br><span class="line">├── include</span><br><span class="line">│   └── modern</span><br><span class="line">│       └── lib.hpp</span><br><span class="line">├── src</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── lib.cpp</span><br><span class="line">└── tests</span><br><span class="line">    ├── CMakeLists.txt</span><br><span class="line">    └── testlib.cpp</span><br></pre></td></tr></table></figure><h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="/CMakeLists.txt"></a>/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Works with 3.14 and tested through 3.18</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>...<span class="number">3.18</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Project name and a few useful settings. Other commands can pick up the results</span></span><br><span class="line"><span class="keyword">project</span>(</span><br><span class="line">  ModernCMakeExample</span><br><span class="line">  VERSION <span class="number">0.1</span></span><br><span class="line">  DESCRIPTION <span class="string">"An example project with CMake"</span></span><br><span class="line">  LANGUAGES CXX)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅在主项目中运行，若是子项目中 add_subdirectory 则忽略</span></span><br><span class="line"><span class="keyword">if</span>(CMAKE_PROJECT_NAME <span class="keyword">STREQUAL</span> PROJECT_NAME)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optionally set things like CMAKE_CXX_STANDARD,</span></span><br><span class="line">  <span class="comment"># CMAKE_POSITION_INDEPENDENT_CODE here</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Let's ensure -std=c++xx instead of -std=g++xx</span></span><br><span class="line">  <span class="keyword">set</span>(CMAKE_CXX_EXTENSIONS <span class="keyword">OFF</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Let's nicely support folders in IDE's</span></span><br><span class="line">  <span class="keyword">set_property</span>(GLOBAL PROPERTY USE_FOLDERS <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Testing only available if this is the main app. Note this needs to be done</span></span><br><span class="line">  <span class="comment"># in the main CMakeLists since it calls enable_testing, which must be in the</span></span><br><span class="line">  <span class="comment"># main CMakeLists.</span></span><br><span class="line">  <span class="keyword">include</span>(CTest)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Docs only available if this is the main app</span></span><br><span class="line">  <span class="keyword">find_package</span>(Doxygen)</span><br><span class="line">  <span class="keyword">if</span>(Doxygen_FOUND)</span><br><span class="line">    <span class="keyword">add_subdirectory</span>(docs)</span><br><span class="line">  <span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">message</span>(STATUS <span class="string">"Doxygen not found, not building docs"</span>)</span><br><span class="line">  <span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FetchContent added in CMake 3.11, downloads during the configure step</span></span><br><span class="line"><span class="comment"># FetchContent_MakeAvailable was not added until CMake 3.14</span></span><br><span class="line"><span class="keyword">include</span>(FetchContent)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accumulator library This is header only, so could be replaced with git</span></span><br><span class="line"><span class="comment"># submodules or FetchContent</span></span><br><span class="line"><span class="keyword">find_package</span>(Boost REQUIRED)</span><br><span class="line"><span class="comment"># Adds Boost::boost / Boost::headers (newer FindBoost / BoostConfig 3.15 name)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Formatting library, adds fmt::fmt</span></span><br><span class="line">FetchContent_Declare(</span><br><span class="line">  fmtlib</span><br><span class="line">  GIT_REPOSITORY https://github.com/fmtlib/fmt.git</span><br><span class="line">  GIT_TAG <span class="number">7.0</span>.<span class="number">2</span>)</span><br><span class="line">FetchContent_MakeAvailable(fmtlib)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The compiled library code is here</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(src)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The executable code is here</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(apps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing only available if this is the main app</span></span><br><span class="line"><span class="keyword">if</span>(BUILD_TESTING)</span><br><span class="line">  <span class="keyword">add_subdirectory</span>(tests)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure><h3 id="src-CMakeLists-txt"><a href="#src-CMakeLists-txt" class="headerlink" title="/src/CMakeLists.txt"></a>/src/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note that headers are optional, and do not affect add_library, but they will</span></span><br><span class="line"><span class="comment"># not show up in IDEs unless they are listed in add_library.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optionally glob, but only for CMake 3.12 or later: file(GLOB HEADER_LIST</span></span><br><span class="line"><span class="comment"># CONFIGURE_DEPENDS "$&#123;ModernCMakeExample_SOURCE_DIR&#125;/include/modern/*.hpp")</span></span><br><span class="line"><span class="keyword">set</span>(HEADER_LIST <span class="string">"$&#123;ModernCMakeExample_SOURCE_DIR&#125;/include/modern/lib.hpp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make an automatic library - will be static or dynamic based on user setting</span></span><br><span class="line"><span class="keyword">add_library</span>(modern_library lib.cpp <span class="variable">$&#123;HEADER_LIST&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need this directory, and users of our library will need it too</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(modern_library PUBLIC ../<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This depends on (header only) boost</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(modern_library PRIVATE Boost::boost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># All users of this library will need at least C++11</span></span><br><span class="line"><span class="keyword">target_compile_features</span>(modern_library PUBLIC cxx_std_11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># IDEs should put the headers in a nice place</span></span><br><span class="line"><span class="keyword">source_group</span>(</span><br><span class="line">  TREE <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/include"</span></span><br><span class="line">  PREFIX <span class="string">"Header Files"</span></span><br><span class="line">  FILES <span class="variable">$&#123;HEADER_LIST&#125;</span>)</span><br></pre></td></tr></table></figure><h3 id="apps-CMakeLists-txt"><a href="#apps-CMakeLists-txt" class="headerlink" title="/apps/CMakeLists.txt"></a>/apps/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(app app.cpp)</span><br><span class="line"><span class="keyword">target_compile_features</span>(app PRIVATE cxx_std_17)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(app PRIVATE modern_library fmt::fmt)</span><br></pre></td></tr></table></figure><h3 id="docs-CMakeLists-txt"><a href="#docs-CMakeLists-txt" class="headerlink" title="/docs/CMakeLists.txt"></a>/docs/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">set</span>(DOXYGEN_EXTRACT_ALL YES)</span><br><span class="line"><span class="keyword">set</span>(DOXYGEN_BUILTIN_STL_SUPPORT YES)</span><br><span class="line"></span><br><span class="line">doxygen_add_docs(docs modern/lib.hpp <span class="string">"$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/mainpage.md"</span></span><br><span class="line">                 WORKING_DIRECTORY <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/include"</span>)</span><br></pre></td></tr></table></figure><h3 id="tests-CMakeLists-txt"><a href="#tests-CMakeLists-txt" class="headerlink" title="/tests/CMakeLists.txt"></a>/tests/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing library</span></span><br><span class="line">FetchContent_Declare(</span><br><span class="line">  catch2</span><br><span class="line">  GIT_REPOSITORY https://github.com/catchorg/Catch2.git</span><br><span class="line">  GIT_TAG v2.<span class="number">9.1</span>)</span><br><span class="line">FetchContent_MakeAvailable(catch2)</span><br><span class="line"><span class="comment"># Adds Catch2::Catch2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tests need to be added as executables first</span></span><br><span class="line"><span class="keyword">add_executable</span>(testlib testlib.cpp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># I'm using C++17 in the test</span></span><br><span class="line"><span class="keyword">target_compile_features</span>(testlib PRIVATE cxx_std_17)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Should be linked to the main library, as well as the Catch2 testing library</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(testlib PRIVATE modern_library Catch2::Catch2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you register a test, then ctest and make test will run it. You can also run</span></span><br><span class="line"><span class="comment"># examples and check the output, as well.</span></span><br><span class="line"><span class="keyword">add_test</span>(NAME testlibtest <span class="keyword">COMMAND</span> testlib) <span class="comment"># Command can be a target</span></span><br></pre></td></tr></table></figure><p><a href="https://hsf-training.github.io/hsf-training-cmake-webpage/" target="_blank" rel="noopener">更多内容</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://hsf-training.github.io/hsf-training-cmake-webpage/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hsf-training.github.io/hsf-tr
      
    
    </summary>
    
    
      <category term="cpp" scheme="http://guileen.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>C++构建工具比较</title>
    <link href="http://guileen.github.io/2021/01/08/cpp-build-systems/"/>
    <id>http://guileen.github.io/2021/01/08/cpp-build-systems/</id>
    <published>2021-01-08T08:26:33.000Z</published>
    <updated>2021-02-06T12:30:37.114Z</updated>
    
    <content type="html"><![CDATA[<p>近日，又对C++产生了兴趣。</p><p>C++可以说是我内心的阴影，之所以是内心的阴影，倒不是C++的语言本身使我无法理解，或不喜欢C++的某些特性。相反我一直对C++怀有很高的敬意。之所以说C++是我的阴影，主要是因为C++的工具链实在是太长太杂了。我不介意多花点时间来学习C++的语言特性，但是我实在不想浪费时间在工具的学习上面。我屡屡在尝试用C++做点东西的时候，都被环境的配置而感到厌烦而放弃。</p><p>最近，感觉 <a href="https://github.com/ocornut/imgui" target="_blank" rel="noopener">Dear ImGui</a> 这个项目有点意思，可以用来做点有趣的事情。于是我想再次挑战一下C++的项目，顺便看看社区是否有新的构建工具出来简化我的工作。</p><p>此前的工具 Make 缺点是项目越大越复杂，需要学习很多东西。还有 autotools , scons，也有一定学习成本。</p><p>最近出现的 google的Bazel、facebook的Buck 两个build system都可以用来构建C++。看了些对比主要缺点是对原有的生态兼容性比较差，往往需要源码导入。</p><p>可以选择的有CMake和Meson。目前看来是比较合适的选择，我决定有空尝试一下meson。</p><p>找到两个template项目 <a href="https://github.com/tiernemi/meson-sample-project" target="_blank" rel="noopener">meson-sample-project</a>  、 <a href="https://github.com/kigster/cmake-project-template" target="_blank" rel="noopener">cmake-project-template</a> 。拿这个模板直接改一改就可以创建一个c++项目了，这样我内心的恐惧感减少了很多。 </p><p>— 2021-01-26 更新 —<br>最终我选择了 <a href="https://github.com/kigster/cmake-project-template" target="_blank" rel="noopener">cmake-project-template</a> 来作为当前的C++<br>项目模板。今天github的发现页给我推荐了<a href="https://github.com/lefticus/cpp_starter_project" target="_blank" rel="noopener">cpp_starter_project</a> ，还能直接使用 github 的 use this template 功能，貌似不错，下次试试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日，又对C++产生了兴趣。&lt;/p&gt;
&lt;p&gt;C++可以说是我内心的阴影，之所以是内心的阴影，倒不是C++的语言本身使我无法理解，或不喜欢C++的某些特性。相反我一直对C++怀有很高的敬意。之所以说C++是我的阴影，主要是因为C++的工具链实在是太长太杂了。我不介意多花点时间
      
    
    </summary>
    
    
      <category term="cpp" scheme="http://guileen.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>强化学习在长时间运行系统中的限制</title>
    <link href="http://guileen.github.io/2020/04/05/limitation-of-rl-in-long-term/"/>
    <id>http://guileen.github.io/2020/04/05/limitation-of-rl-in-long-term/</id>
    <published>2020-04-04T16:43:34.000Z</published>
    <updated>2020-04-04T16:43:34.648Z</updated>
    
    <content type="html"><![CDATA[<p>近日尝试使用强化学习解决8x8大小的2048游戏。游戏环境是一个8x8的方格，环境每回合随机生成2个方块，方块的值为2或4，AI的决策空间是上、下、左、右四个操作，所有方块都将向所指示的方向堆叠，如果两个相邻的值相同方块在堆叠方向挤压，则这两个方块合并为1个方块，值是两个方块值的和。如果任何方向都无法移动方块，则游戏结束。游戏的目标是尽可能久的玩下去，合并出最大的方块。</p><p>最初我尝试用PG来训练这个看似简单的游戏。每一步，都视为1点奖励，如果失败则给予-1000惩罚。算法很快习得了一个偷懒的方法，每一步都进行无效的移动，以此来苟延残喘。于是将无效的移动操作，视为重大的失误，也同样给予-1000的惩罚。算法很快学会了在一个局面下的有效移动操作。但这个游戏，哪怕只是随机的移动也能够取得一个普通的结果，如果要突破极限，则需要使用一些特殊的策略，我期待算法是否能在训练中学会这些策略。</p><p><img src="/img/2048/pg.gif" alt="初次训练，类似随机运动"></p><p>对于这个游戏，达到2048，需要大约500次移动，达到4096，则需要1000，达到1M，也就大约需要20多万次移动，达到4M，则需要上百万次移动。每到达一个新的难度，面临的局面都不同，之前所习得的经验就不一定继续有效了。2048游戏是一个比围棋要简单很多的游戏，围棋拥有更多的选点，2048只有4个操作选择。他们的主要区别在于围棋一般在100多手内结束，而2048的游戏时间则近乎无限长。理想的游戏结果如下图所示：</p><p><img src="/img/2048/perfect.png" alt=""></p><p>这一游戏是存在理想玩法的，经过很多局的游戏，我已总结出一些经验。但是这些经验是感性的，很难使用逻辑规则表达出来，很多时候是凭直觉的。我手段操作达到了512K的结果，虽然我依然可以挑战更高的游戏记录，但显然我不能将如此多的时间浪费在滑动手指上。这也是我要编程解决这个问题的初衷，但是强化学习算法，只能在一次次的失败中得到教训，可是这个游戏的特点是，训练的越好，游戏时间越长，获得失败经验的成本就越大。所以无论该算法在理论上是多么的正确，但在实际操作过程中已经变得不可行了。</p><p>DQN、PG等强化学习算法的基本过程是根据系统给予的奖励，努力最大化收益。但是对于一个没有明确获胜终点的系统，如果验证训练结果的有效性却是一个非常大的问题。由于强化学习本质上是通过过往的经验来调整自己的策略的，如果有明显的获胜路径，则算法可以有充分的胜利经验可供借鉴。但如果目标是永远安全的运行下去，没有获胜的路径，只有失败的惩罚，那么算法只能在有限的教训中得到学习。假设我们正在训练一个自动飞行系统，获取每一个经验教训的成本都非常巨大，强化学习在这一方面的应用，必须要搭配一些人类的理性评估作为辅助，但是将人类的意识转化为可以实施的程序逻辑又是非常复杂的事情。</p><p>如果我们训练的是一个自动驾驶系统呢？在未来无人驾驶会应用的越来越多。无人驾驶的安全性会很快超越人类，随即人们期望可以进一步提升驾驶的平均速度或其他一些智能驾驶的指标。因为无人驾驶的安全性已经超越了人类，所以无法再依赖于人类的驾驶经验给予其帮助，只能依赖于自身驾驶中的经验（尤其是事故）作为训练依据。那么这时这个系统还可能是安全的吗？</p><p>在未来的相关强化学习领域，一个好的环境模拟系统、事故全息信息的采集和共享系统，才是提升人工智能的关键，而不是算法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日尝试使用强化学习解决8x8大小的2048游戏。游戏环境是一个8x8的方格，环境每回合随机生成2个方块，方块的值为2或4，AI的决策空间是上、下、左、右四个操作，所有方块都将向所指示的方向堆叠，如果两个相邻的值相同方块在堆叠方向挤压，则这两个方块合并为1个方块，值是两个方
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（六）：策略梯度实例</title>
    <link href="http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/"/>
    <id>http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/</id>
    <published>2020-01-03T06:40:51.000Z</published>
    <updated>2020-01-03T09:26:04.672Z</updated>
    
    <content type="html"><![CDATA[<p>和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网络在各个动作的Q值是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(PolicyNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.1</span>))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出层需要使用softmax</span></span><br><span class="line">        <span class="keyword">return</span> F.softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>不要忘了输出层的SoftMax。</p><h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><p>相对于DQN，我们也不需要额外的目标网络和参数复制操作，只需要一个策略网络即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">GAMMA = <span class="number">0.99</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">policy_net = PolicyNet(input_size, HIDDEN_SIZE, n_actions)</span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="选择动作"><a href="#选择动作" class="headerlink" title="选择动作"></a>选择动作</h2><p>在选择动作时，我们不再需要特地设置探索概率，因为输出结果就是各个动作的概率分布。我们使用<code>torch.distributions.categorical.Categorical</code> 来进行取样。在每次选择动作时，我们同时记录对应的概率，以便后续使用。这个概率就是 `ln pi_theta(S_t,A_t)`</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">log_probs = []</span><br><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state),<span class="number">0</span>)</span><br><span class="line">    probs = policy_net(x)</span><br><span class="line">    c = Categorical(probs)</span><br><span class="line">    action = c.sample()</span><br><span class="line">    <span class="comment"># log action probs to plt</span></span><br><span class="line">    prob = c.log_prob(action)</span><br><span class="line">    log_probs.append(prob)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h2><p>为了更新参数，我们首先需要计算`v_t`，这在后续参数迭代中需要用到。</p><ul><li>` v_t = r_(t+1) + gamma * v_(t+1) `</li></ul><p>在模拟执行的时候，我们记录了每一步的reward，我们需要计算每一步的`v_t`，其顺序与执行顺序一致。根据公式我们需要倒序的计算`v_t`，然后将计算好的结果倒序排列，就形成了`v_1,v_2…v_t`的序列。最后我们需要将数据标准化。(TODO: 这里可能存在一个序列对应的问题，其中每一个状态的累计收益，是后续状态收益之和，不包含本轮收益)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">values = []</span><br><span class="line">v = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> reward <span class="keyword">in</span> reversed(rewards):</span><br><span class="line">    v = v * GAMMA + reward</span><br><span class="line">    values.insert(<span class="number">0</span>, v)</span><br><span class="line">mean = np.mean(values)</span><br><span class="line">std = np.std(values)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">    values[i] = (values[i] - mean) / std</span><br></pre></td></tr></table></figure><p>接下来我们需要更新参数，参数更新的公式为：</p><ul><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul><p>我们将其转换为损失函数形式:</p><ul><li>` L(theta) = - v_t ln pi_theta(A_t|S_t) `</li></ul><p>这个损失函数的形式可以帮助我们更好的理解策略梯度的原理。如果一个动作价值为负值，但是其选择概率为正，则损失较大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.random.choice(size, n):</span><br><span class="line">    loss.append(- values[i] * log_probs[i])</span><br><span class="line">loss = torch.cat(loss).sum()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">    param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>训练循环需要在一局结束之后进行。并清除rewards、log_probs缓存。对于cartpole-v1环境，要注意他的每一步奖励都是1，很显然在最后一步代表着游戏失败，我们需要施加一定的惩罚，我们将最后一步的奖励设为-100。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">5000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state)</span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward, done,_ = env.step(action.item())</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt;= <span class="number">2500</span>:</span><br><span class="line">            optimize_model()</span><br><span class="line">            print(<span class="string">'EP'</span>, i_episode)</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            rewards = []</span><br><span class="line">            log_probs = []</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/files/cart-pg.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（五）：策略梯度Policy Gradient</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/</id>
    <published>2019-12-30T06:37:07.000Z</published>
    <updated>2020-01-03T09:32:49.981Z</updated>
    
    <content type="html"><![CDATA[<p>DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问题，而Q值的目的也是为了选择动作。我们可以将增加学习的问题简化为选择动作的问题。那么我们可否使用深度学习直接做出动作选择呢？显然，我们可以定义一个网络`pi_theta`，其中输入为状态`s`，输出为每个动作`a`的概率。</p><p><img src="/img/rl-5/1.png" alt="策略梯度"></p><p>因为这个网络与策略函数的定义一样，所以被称为策略网络。`pi_theta(a|s)`，表示在`s`状态下选择动作`a`的概率。只要这个网络能够收敛，我们就可以直接得到最佳策略。这个网络的奖励函数也就是最终游戏的总奖励。</p><p>`J(theta) = sum_(s in S)d^pi(s)V^pi(s) = sum_(s in S)d^pi(s)sum_(a in A)pi_theta(a|s)Q^pi(s, a)`</p><p>`d^pi(s)`指状态`s`在马尔科夫链上的稳定分布，`d^pi(s) = lim_(t-&gt;oo)P(s_t=s|s_0,pi_theta)`。</p><p>但是这个表达式看上去是不可能计算的，因为状态的分布和Q值都是随着策略的更新而不断变化的。但是我们并不需要计算`J(theta)`，在梯度下降法中我们只需要计算梯度`grad_(theta)J(theta)`即可</p><p>`grad_(theta)V^pi(s)`<br>`= grad_(theta)(sum_(a in A)pi_theta(a|s)Q^pi(s, a))`<br>根据导数乘法规则<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_thetaQ^pi(s, a))`<br>展开`Q^pi(s,a)`为各各种可能的下一状态奖励之和<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_(theta)sum_(s’,r)P(s’,r|s,a)(r+V^pi(s’)))`<br>而其中状态转移函数`P(s’,r|s,a)`、奖励`r`由环境决定，与`grad_theta`无关，所以<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’,r)P(s’,r|s,a)grad_(theta)V^pi(s’))`<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>现在我们有了一个形式非常好的递归表达式：<br>`grad_(theta)V^pi(s) = sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>设 `rho^pi(s-&gt;x, k)` 表示在策略`pi^theta`下，`k`步以后状态`s`转移到状态`x`的概率。有：</p><ul><li>`rho^pi(s-&gt;s, k=0)=1`</li><li>`rho^pi(s-&gt;s’, k=1)=sum_(a)pi_(theta)(a|s)P(s’|s,a)`</li><li>`rho^pi(s-&gt;x, k+1) = sum_(s’)rho^pi(s-&gt;s’, k)rho^pi(s’-&gt;x, 1)`</li></ul><p>为了简化计算，令 `phi(s)=sum_(a in A)grad_(theta)pi_theta(a|s)Q^pi(s,a)`</p><p>`grad_(theta)V^pi(s)`<br>`= phi(s) + sum_(a in A)pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)sum_(a in A)pi_(theta)(a|s)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)(phi(s’) + sum_(s’’)rho^pi(s’-&gt;s’’,1)grad_(theta)V^pi(s’’)) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)grad_(theta)V^pi(s’’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)phi(s’’) + sum_(s’’’)rho^pi(s-&gt;s’’’,3)grad_(theta)V^pi(s’’’) `<br>`= …`<br>`= sum_(x in S)sum_(k=0)^(oo)rho^pi(s-&gt;x, k)phi(x)`</p><p>令 `eta(s)=sum_(k=0)^(oo)rho^pi(s_0-&gt;s, k)`</p><p>`grad_(theta)J(theta)=grad_(theta)V^pi(s_0)`<br>`= sum_(s)sum_(k=0)^(oo)rho^pi(s_0-&gt;s,k)phi(s)`<br>`= sum_(s)eta(s)phi(s)`<br>`= (sum_(s)eta(s))sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `sum_(s)eta(s)` 属于常数，对于求梯度而言常数可以忽略。<br>`prop sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `eta(s)/(sum_(s)eta(s))`表示`s`的稳定分布<br>`= sum_(s)d^pi(s)sum_a grad_(theta)pi_(theta)(a|s)Q^pi(s,a)`<br>`= sum_(s)d^pi(s)sum_a pi_(theta)(a|s)Q^pi(s,a)(grad_(theta)pi_(theta)(a|s))/(pi_(theta)(a|s))`<br>因 ` (ln x)’ = 1/x `<br>`= Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)]`</p><p>所以得出策略梯度最重要的定理：</p><p>` grad_(theta)J(theta)=Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)] `</p><p>其中的`Q^pi(s,a)`也就是状态s的累计收益，可以在一次完整的动作轨迹中累计计算得出。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>该算法被称为 REINFORCE</p><ul><li>随机初始化`theta`</li><li>生成一个完整的策略`pi_theta`的轨迹: `S1,A1,R2,S2,A2,…,ST`。</li><li>For t=1, 2, … , T-1:<ul><li>` v_t = sum_(i=0)^(oo) gamma^i R_(t+i+1) `</li><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul></li></ul><p>参考：<br><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Lilian Weng:Policy Gradient Algorithms</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（四）：DQN实战</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/</id>
    <published>2019-12-30T06:37:03.000Z</published>
    <updated>2020-01-03T09:26:21.490Z</updated>
    
    <content type="html"><![CDATA[<p>我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用<a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank" rel="noopener">CartPole-v1</a>。我们的输入是一幅图片，动作是施加一个向左向右的力量，我们需要尽可能的保持木棍的平衡。</p><p><img src="/files/cartpole-v1.gif" alt="CartPole-v1"></p><p>对于这个环境，尝试了很多次，总是不能达到很好的效果，一度怀疑自己的代码写的有问题。后来仔细看了这个环境的奖励，是每一帧返回奖励1，哪怕是最后一帧也是返回1 的奖励。这里很明显是不合理的俄。我们需要重新定义这个奖励函数，也就是在游戏结束的时候，给一个比较大的惩罚，r=-100。很快可以达到收敛。</p><h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>, (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.mem = []</span><br><span class="line">        self.pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Save a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.mem) &lt; self.cap:</span><br><span class="line">            self.mem.append(<span class="keyword">None</span>)</span><br><span class="line">        self.mem[self.pos] = Transition(*args)</span><br><span class="line">        self.pos = (self.pos + <span class="number">1</span>) % self.cap</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.mem, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.mem)</span><br></pre></td></tr></table></figure><h2 id="Q网络"><a href="#Q网络" class="headerlink" title="Q网络"></a>Q网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="初始化参数和状态"><a href="#初始化参数和状态" class="headerlink" title="初始化参数和状态"></a>初始化参数和状态</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络</span></span><br><span class="line">policy_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络</span></span><br><span class="line">target_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络从策略网络复制参数</span></span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="探索和选择最佳动作"><a href="#探索和选择最佳动作" class="headerlink" title="探索和选择最佳动作"></a>探索和选择最佳动作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state, no_explore=False)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold <span class="keyword">or</span> no_explore:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(x).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)</span><br></pre></td></tr></table></figure><h2 id="优化模型-关键代码"><a href="#优化模型-关键代码" class="headerlink" title="优化模型(关键代码)"></a>优化模型(关键代码)</h2><p>这里主要是抽样、目标值计算、损失计算的部分。损失计算采用Huber loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span>  s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>,</span><br><span class="line">                        batch.next_state)), dtype=torch.uint8)</span><br><span class="line">    non_final_next_states = torch.FloatTensor([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>])</span><br><span class="line">    state_batch = torch.FloatTensor(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.FloatTensor(batch.reward)</span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 限制网络更新的幅度，可以大幅提升训练的效果。</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>这里主要有主循环、获取输入、记录回放、训练、复制参数等环节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state, i_episode%<span class="number">50</span>==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(i_episode%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward,done,_ = env.step(action.item())</span><br><span class="line">        <span class="comment"># reward = torch.tensor([reward])</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt; <span class="number">2500</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                optimize_model()</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br></pre></td></tr></table></figure><p><img src="/files/dqn2.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用&lt;a href=&quot;https://gym.openai.com/envs/CartPole-v1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CartPole-v1&lt;
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（三）：DQN</title>
    <link href="http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/"/>
    <id>http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/</id>
    <published>2019-12-28T03:53:37.000Z</published>
    <updated>2019-12-30T10:49:04.704Z</updated>
    
    <content type="html"><![CDATA[<p>上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学习就有了施展的空间了。</p><p>Q表存储着状态s和动作a、奖励r的信息。我们知道深度神经网络，也具有存储信息的能力。DQN算法就是将Q-table存储结构替换为神经网络来存储信息。我们定义神经网络`f(s, w) ~~ Q(s)`，输出为一个向量`[Q(s, a_1), Q(s, a_2), Q(s, a_3), …, Q(s, a_n)]`。经过这样的改造，我们就可以用Q-learing的算法思路来解决更复杂的状态空间的问题了。我们可以通过下面两张图来对比Q-learning和DQN的异同。</p><p><img src="/img/rl-3/1.png" alt="Q-learning"></p><p><img src="/img/rl-3/2.png" alt="Deep-Q-learning"></p><p>网络结构要根据具体问题来设计。在神经网络训练的过程中，损失函数是关键。我们采用MSE来计算error。</p><p>`L(w) = (ubrace(r + argmax_aQ(s’, a’))_(目标值) - ubrace(Q(s, a))_(预测值))^2`</p><h2 id="基本算法描述"><a href="#基本算法描述" class="headerlink" title="基本算法描述"></a>基本算法描述</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">使用随机参数初始化网络Q</span><br><span class="line"><span class="keyword">while</span> 未收敛:</span><br><span class="line">  action 按一定概率随机选取，其余使用argmax Q(s)选取</span><br><span class="line">  模拟执行 action 获取 状态 s_, 奖励 r, 是否完成 done</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    target = r</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    target = r + gamma * argmax Q(s_)</span><br><span class="line">  loss = MSE(target, Q(s, a))</span><br><span class="line">  使用loss更新网络Q</span><br><span class="line">  s = s_</span><br></pre></td></tr></table></figure><p>但是，通过实验我们会发现，训练过程非常的不稳定。稳定性是强化学习所面临的主要问题之一，为了达到稳定的训练我们需要运用一些优化的手段。</p><h2 id="环境的稳定性"><a href="#环境的稳定性" class="headerlink" title="环境的稳定性"></a>环境的稳定性</h2><p>Agent生活在环境之中，并根据环境的反馈进行学习，但环境是否是稳定的呢？假设agent在学习出门穿衣的技能，它需要学会在冬天多穿，夏天少穿。但是这个agent只会根据当天的反馈来修正自己的行为，也就是说这个agent是没有记忆的。那么这个agent就会在多次失败后终于在冬天学会了多穿衣，但转眼之间到了夏天他又会陷入不断的失败，最终他在夏天学会了少穿衣之后，又会在冬天陷入失败，如此循环不断，永远不会收敛。如果要能够很好的训练，这个agent至少要有一整年的记忆空间，每一批都要从过去的记忆中抽取记忆来进行训练，就可以避免遗忘过去的教训。</p><p>在DeepMind的Atari 论文中提到</p><blockquote><p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p></blockquote><p>意思是，受生物学启发，他们采用了一种叫做经验回放（experience replay）的机制，随机抽取数据来到达“移除观察序列的相关性，平滑数据分布的改变”的目的。<br><img src="/img/rl-3/4.png" alt="DQN with Atari"></p><p>我们已经理解了要有经验回放的记忆，但是为什么一定要随机抽取呢？对此论文认为这个随机抽取可以移除序列相关性、平滑分布中的改变。当如何理解呢？简单的说就是在我们不清楚合理的周期的情况下，能够保证采样的合理性。我们仍然以四季穿衣举例，假设我们不使用随机采样，我们必须在每次训练中都采用365天左右的数据，才能使我们的数据样本分布合理。可是agent并不清楚一年365天这个规律，这恰恰是我们所要学习的内容。采用随机采用，就可以自然的做到数据的分布合理，而且只需要使用记忆中的部分数据，减少单次迭代的计算量。</p><p>在这个记忆里，我们并不记录当时的网络参数（分析过程），我们只记录（状态s，动作a，新状态s’, 单步奖励r)。显然，记忆的尺寸不可能无限大。当记忆体增大到一定程度之后，我们采用滚动的方式用最新的记忆替换掉最老的记忆。就像在学习围棋的过程中，有初学者阶段的对局记忆，也有高手阶段的对局记忆，在提升棋艺的角度来看，高手阶段的记忆显然比初学者阶段的记忆更有价值。</p><p>说句题外话，其实对于一个民族而言也是一样的。我们这个民族拥有一个非常好的传统，就是记述历史，也就是等于我们这个民族拥有足够大的记忆量，这是我们胜于其他民族的。但是这个历史记录中，掺杂了历史上不同阶段的评价，这些评价是根据当时的经验得出的。而根据DQN的算法描述来看，对我们最有价值的部分其实是原始信息，而不是那些附加在之上的评价，这些评价有正确的部分，也有错误的部分，我们不用去过多关心。我们只需要在今天的认知（也就是最新的训练结果）基础上，对历史原始信息（旧状态、动作、新状态、单步奖励）进行随机的抽样分析即可。</p><h2 id="网络稳定性"><a href="#网络稳定性" class="headerlink" title="网络稳定性"></a>网络稳定性</h2><p>DQN另一个稳定性问题与目标值计算有关。因为`target = r + gamma * argmax Q(s’)`，所以目标值与网络参数本身是相关，而参数在训练中是不断变化的，所以这会造成训练中的不稳定。一个神经网络可以自动收敛，取决于存在一个稳定的目标，如果目标本身在不断的游移变动，那么想要达到稳定就比较困难。这就像站在平地上的人很容易平衡，但如果让人站在一个不断晃动的木板上，就很难达到平衡。为了解决这个问题，我们需要构建一个稳定的目标函数。</p><p>解决的方法是采用两个网络代替一个网络。一个网络用于训练调整参数，称之为策略网络，另一个专门用于计算目标，称之为目标网络。目标网络与策略网络拥有完全一样的网络结构，在训练的过程中目标网络的参数是固定的。执行一小批训练之后，将策略网络最新的参数复制到目标网络中。</p><p><img src="/img/rl-3/3.png" alt="目标网络"></p><p>经验回放和目标网络的效果见下表（引用自Nature 论文）：<br><img src="/img/rl-3/5.png" alt="优化对比"></p><h2 id="其他DQN优化"><a href="#其他DQN优化" class="headerlink" title="其他DQN优化"></a>其他DQN优化</h2><p>关于DQN的优化，这篇文章描述的比较全面 <a href="https://zhuanlan.zhihu.com/p/21547911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21547911</a>。在之后的实践中考虑是否进一步深入。主要介绍3个改进：</p><p><img src="/img/rl-3/6.png" alt="DQN优化"></p><ul><li>Double DQN：对目标值计算的优化，a’使用策略网络选择的动作来代替目标网络选择的动作。</li><li>Prioritised replay：使用优先队列（priority queue）来存储经验，避免丢弃早期的重要经验。使用error作为优先级，仿生学技巧，类似于生物对可怕往事的记忆。</li><li>Dueling Network：将Q网络分成两个通道，一个输出V，一个输出A，最后再合起来得到Q。如下图所示（引用自Dueling Network论文）。这个方法主要是idea很简单但是很难想到，然后效果一级棒，因此也成为了ICML的best paper。</li></ul><p><img src="/img/rl-3/7.png" alt="Dueling Network"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（二）：Q-learning实战</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/</id>
    <published>2019-12-27T08:02:19.000Z</published>
    <updated>2019-12-27T10:37:57.211Z</updated>
    
    <content type="html"><![CDATA[<p>我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用<code>gym</code>来作为我们对实验环境。安装方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym</span><br></pre></td></tr></table></figure><h2 id="初识环境"><a href="#初识环境" class="headerlink" title="初识环境"></a>初识环境</h2><p>我们的实验环境是一个冰湖滑行游戏，你将控制一个agent在冰面到达目标终点，前进方向并不总受你的控制，你还需要躲过冰窟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># 构造游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"><span class="comment"># 动作空间-&gt; Discrete(4)</span></span><br><span class="line">print(env.action_space)</span><br><span class="line"><span class="comment"># 状态空间-&gt; Discrete(16)</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line"><span class="comment"># 初始化游戏环境，并得到状态s</span></span><br><span class="line">s = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 渲染游戏画面</span></span><br><span class="line">    env.render()</span><br><span class="line">    <span class="comment"># 从动作空间中随机选择一个动作a</span></span><br><span class="line">    a = env.action_space.sample()</span><br><span class="line">    <span class="comment"># 执行动作a，得到新状态s，奖励r，是否完成done</span></span><br><span class="line">    s, r, done, info = env.step(a) <span class="comment"># take a random action</span></span><br><span class="line">    print(s, r, done, info)</span><br><span class="line"><span class="comment"># 关闭环境</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>游戏画面示意如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SFFF       (S: 起点，安全)</span><br><span class="line">FHFH       (F: 冰面，安全)</span><br><span class="line">FFFH       (H: 冰窟，进入则失败)</span><br><span class="line">HFFG       (G: 终点，到达则成功)</span><br></pre></td></tr></table></figure><h2 id="Agent结构"><a href="#Agent结构" class="headerlink" title="Agent结构"></a>Agent结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLAgent</span><span class="params">()</span>:</span></span><br><span class="line">    q = <span class="keyword">None</span></span><br><span class="line">    action_space = <span class="keyword">None</span></span><br><span class="line">    epsilon = <span class="number">0.1</span> <span class="comment"># 探索率</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># 衰减率</span></span><br><span class="line">    lr = <span class="number">0.1</span> <span class="comment"># 学习率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, action_space, state_count, epsilon=<span class="number">0.1</span>, lr=<span class="number">0.1</span>, gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.q = [[<span class="number">0.</span> <span class="keyword">for</span> a <span class="keyword">in</span> range(action_space.n)] <span class="keyword">for</span> s <span class="keyword">in</span> range(state_count)]</span><br><span class="line">        self.action_space = action_space</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据状态s，选择动作a</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新状态变化并学习，状态s执行了a动作，得到了奖励r，状态转移到了s_</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这是一个Agent的一般结构，主要由初始化、选择动作、更新状态变化，三个方法构成。后续的其他算法将依然采用该结构。q表数据使用一个二维数组表示，其大小为 state_count action_count，对于这个项目而言是一个 `16*4` 的大小。</p><h2 id="添加Q-table的辅助方法"><a href="#添加Q-table的辅助方法" class="headerlink" title="添加Q-table的辅助方法"></a>添加Q-table的辅助方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回状态s的最佳动作a、及其r值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    max_r = -math.inf</span><br><span class="line">    max_a = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(self.action_space.n):</span><br><span class="line">        r = self.q_get(s, a)</span><br><span class="line">        <span class="keyword">if</span> r &gt; max_r:</span><br><span class="line">            max_a = a</span><br><span class="line">            max_r = r</span><br><span class="line">    <span class="keyword">return</span> max_a, max_r</span><br><span class="line"><span class="comment"># 获得 状态s，动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_get</span><span class="params">(self, s, a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.q[s][a]</span><br><span class="line"><span class="comment"># 更新 状态s 动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_put</span><span class="params">(self, s, a, v)</span>:</span></span><br><span class="line">    self.q[s][a] = v</span><br></pre></td></tr></table></figure><h2 id="Q-learning的关键步骤"><a href="#Q-learning的关键步骤" class="headerlink" title="Q-learning的关键步骤"></a>Q-learning的关键步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="comment"># 按一定概率进行随机探索</span></span><br><span class="line">        <span class="keyword">return</span> self.action_space.sample()</span><br><span class="line">    <span class="comment"># 返回最佳动作</span></span><br><span class="line">    a, _ = self.argmax(s)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">    q = self.q_get(s, a)</span><br><span class="line">    _, r_ = self.argmax(s_)</span><br><span class="line">    <span class="comment"># Q &lt;- Q + a(Q' - Q)</span></span><br><span class="line">    <span class="comment"># &lt;=&gt; Q &lt;- (1-a)Q + a(Q')</span></span><br><span class="line">    q = (<span class="number">1</span>-self.lr) * q + self.lr * (r + self.gamma * r_)</span><br><span class="line">    self.q_put(s, a, q)</span><br></pre></td></tr></table></figure><h2 id="训练主循环"><a href="#训练主循环" class="headerlink" title="训练主循环"></a>训练主循环</h2><p>我们进行10000局游戏的训练，每局游戏执行直到完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line">agent = QLAgent(env.action_space, env.observation_space.n)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># env.render()  # 训练过程不需要渲染</span></span><br><span class="line">        a = agent.choose_action(s) <span class="comment"># 选择动作</span></span><br><span class="line">        s_, r, done, info = env.step(a) <span class="comment"># 执行动作</span></span><br><span class="line">        agent.update_transition(s, a, r, s_) <span class="comment"># 更新状态变化</span></span><br><span class="line">        s = s_</span><br><span class="line"><span class="comment"># 显示训练后的Q表</span></span><br><span class="line">print(agent.q)</span><br></pre></td></tr></table></figure><h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><p>在测试中，我们只选择最佳策略，不再探索，也不再更新Q表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获胜次数</span></span><br><span class="line">total_win = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># 选择最佳策略</span></span><br><span class="line">        a, _ = agent.argmax(s)</span><br><span class="line">        <span class="comment"># 执行动作 a</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">and</span> r == <span class="number">1</span>:</span><br><span class="line">            total_win += <span class="number">1</span></span><br><span class="line">        s = s_</span><br><span class="line">print(<span class="string">'Total win='</span>, total_win)</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>最终测试的效果是在1万局中获胜了7284次，说明达到了不错的实验效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用&lt;code&gt;gym&lt;/code&gt;来作为我们对实验环境。安装方法：&lt;/p&gt;
&lt;figure cla
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（一）：Q-learning</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/</id>
    <published>2019-12-27T03:28:16.000Z</published>
    <updated>2020-01-03T15:38:11.056Z</updated>
    
    <content type="html"><![CDATA[<p>完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。</p><p><img src="/img/rl-1/4.png" alt="强化学习图示"></p><h2 id="K摇臂赌博机"><a href="#K摇臂赌博机" class="headerlink" title="K摇臂赌博机"></a>K摇臂赌博机</h2><p>我们可以考虑一个最简单的环境：一个动作可立刻获得奖励，目标是使每一个动作的奖励最大化。对这种单步强化学习任务，可以设计一个理论模型——“K-摇臂赌博机”。这个赌博机有K个摇臂，赌徒在投入一个硬币后可一选择按下一个摇臂，每个摇臂以一定概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。最终所获得的总奖励被称为累计奖励。</p><p><img src="/img/rl-1/2.jpg" alt="K摇臂赌博机"></p><p>对于这个简单模型，若要知道每个摇臂的概率，我们只需要进行足够多的尝试即可，这是“仅探索”策略；若要奖励最大化，则需要执行奖赏概率最大的动作即可，这是“仅利用”策略。但在更复杂的环境中，我们不可能对每一个状态的每个动作都进行足够多的探索。比如围棋，我们后续的探索是需要依赖于之前的探索的。因此我们需要在探索和利用之间进行平衡。我们在学习的过程中，必须要保持一定的概率`epsilon`进行探索，其余时候则执行学习到的策略。</p><h2 id="基本概念术语"><a href="#基本概念术语" class="headerlink" title="基本概念术语"></a>基本概念术语</h2><p>为了便于分析讨论，我们定义一些术语。</p><ul><li><p>机器agent处于环境`E`中。</p></li><li><p>状态空间为`S`，每个状态`s in S`是机器感知到的环境描述。</p></li><li><p>机器能够采取的可采取的动作a的集合即动作空间`A`，`a in A`。</p></li><li><p>转移函数`P`表示：当机器执行了一个动作`a`后，环境有一定概率从状态`s`改变为新的状态`s’`。即：`s’=P(s, a)`</p></li><li><p>奖赏函数`R`则表示了执行动作可能获得的奖赏`r`。即：`r=R(s,a)`。</p></li><li><p>环境可以描述为`E=&lt;&lt;S, A, P, R&gt;&gt;`。</p></li><li><p>强化学习的任务是习得一个策略（policy）`pi`，使机器在状态`s`下选择到最佳的`a`。策略有两种描述方法：</p><ol><li>`a=pi(s)` 表示状态`s`下将执行动作`a`，是一种确定性的表示法。</li><li>`pi(s, a)` 表示状态`s`下执行动作`a`的概率。这里有 `sum_a pi(s,a)=1`</li></ol></li><li><p>累计奖励指连续的执行一串动作之后的奖励总和。</p></li><li><p>`Q^(pi)(s, a)`表示在状态`s`下，执行动作`a`，再策略`pi`的累计奖励。为方便讨论后续直接写为`Q(s,a)`。</p></li><li><p>`V^(pi)(s)` 表示在状态`s`下，使用策略`pi`的累计奖励。为方便讨论后续直接写为`V(s)`。</p></li></ul><p>强化学习往往不会立刻得到奖励，而是在很多步之后才能得到一个诸如成功/失败的奖励，这是我们的算法需要反思之前所有的动作来学习。所以强化学习可以视作一种延迟标记的监督学习。</p><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>对于我们要学习的`Q(s,a)`函数，我们可以使用一个Q-table来表示。Q-table是一个二维表，记录每个状态`s in S, a in A`的`Q`值。Q表被初始化为0的状态。在状态`s`执行了动作`a`之后，得到状态`s’`，奖励`r`。我们将潜在的`Q`函数记为`Q_(real)`，其值为当前奖励r与后续状态`s’`的最佳累计奖励之和。则有：</p><p>` Q_(real)(s, a) = r + gamma * argmax_aQ(s’, a) `<br>` err = Q_(real)(s, a) - Q(s, a) `</p><p>其中`gamma`为`Q`函数的偏差，`err`为误差，`alpha`为学习率。 可得出更新公式为：</p><p>` Q(s, a) leftarrow Q(s, a) + alpha*err `<br>即：<br>` Q(s,a) leftarrow (1-alpha)Q(s,a) + alpha(r + gamma * argmax_aQ(s’, a)) `</p><p>以上公式即为Q-learning的关键</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化Q表</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    Q(s, a) = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">  <span class="keyword">if</span> rand() &lt; epsilon:</span><br><span class="line">    a = 从 A 中随机选取一个动作</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    a = 从 A 中选取使 Q(a) 最大的a</span><br><span class="line">  <span class="comment"># 从环境中获得反馈</span></span><br><span class="line">  r = R(s, a)</span><br><span class="line">  s1 = P(s, a)</span><br><span class="line">  <span class="comment"># 更新Q表</span></span><br><span class="line">  Q(s,a) = (<span class="number">1</span>-alpha)*Q(s,a) + alpha * (r + gamma * argmax Q(s1, a) - Q(s, a))</span><br><span class="line">  s = s1</span><br></pre></td></tr></table></figure><p>下图是一个Q表内存结构，在经过一定的学习后，Q表的内容将能够不断逼近每个状态的每个动作的累计收益。<br><img src="/img/rl-1/3.png" alt="Q-table"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/rl-1/4.png&quot; alt=&quot;强化学习图示&quot;&gt;&lt;/p
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>2019年度回顾：新思潮的前夜</title>
    <link href="http://guileen.github.io/2019/12/25/2019-new-thinking/"/>
    <id>http://guileen.github.io/2019/12/25/2019-new-thinking/</id>
    <published>2019-12-25T02:38:44.000Z</published>
    <updated>2019-12-26T10:05:58.289Z</updated>
    
    <content type="html"><![CDATA[<p>过去一年，事业上没多少进步，思想上有贯通之感。这些“无用之学”是一年的无所事事换来，所以需要记录一下。</p><p>今年不再听得到，改听微信读书。办了张图书馆的借书证。主要知识来源：书、知乎、B站。排序与阅读顺序无关</p><p>与同学论战，论战过程中不断提高自己的理论水平，发现有很多东西虽然有概念，如果缺乏实证没有说服力。世界银行提供了很多经济数据，很有价值，如GDP、PPP、Gini系数等。</p><p>读冯友兰的《中国哲学史》，思考中国哲学所具有的普世意义。温习《庄子》《墨子》的部分内容。尝试论证人生的意义，一个可以被自己接受的人生意义。</p><p>B站看秦晖讲的《中国哲学史》，其以群己界限、乡举里选、儒法斗争为叙述线索，似有可取之处，但其对民族性论述不以为然。新文化运动以来诸如胡适、鲁迅，皆有可取之处，却其对民族性的贬低又有精日、西奴之嫌。</p><p>读了《夏禹神话研究》。了解相关知识，对五帝、夏禹历史尝试进行分析，梳理一个叙事结构。录了几个关于五帝抖音视频，暂停了。</p><p>进一步了解了地理环境决定论，读《枪炮病菌与钢铁》。</p><p>了解加州学派的主要观点，读了《白银资本》《火枪与账簿》全球史视角下的东西方对比（明清时期）。</p><p>了解了明史，尤其是晚明史，读了《万历十五年》，《剑桥中国明代史》，B站看张西平、商传等学者的视频，重新认识了明朝。</p><p>了解东林党的历史和评价，褒贬不一。猜想：东林岳麓两家书院，传承中华文明，他们的学生从明末到民国，风格迥异。常凯申是江浙人，属于东林一脉，毛主席是湖南人，属于岳麓一脉。</p><p>了解了中学西传，启蒙运动与中国哲学的关系。西方近代哲学主要发端于启蒙运动，那么西方哲学到底是继承自中学西传，还是继承自基督教，还是继承自古希腊？这个问题引人深思。</p><p>读了白云先生的文章，虽有偏激之辞，但格局极大，有打通任督二脉之感。尤其是关于韩愈-陈抟-朱熹-宋明理学-王夫之-曾国藩、杨昌济、毛泽东的中华道统传承的论述。</p><p>了解王夫之生平，对此先贤之前无甚了解。他的《读通鉴论》看了一节。钦佩。读《曾国藩传》。</p><p>读《天朝的崩溃》，思考满清的农奴式统治的影响，新文化运动批评的民族性，其实是满遗余孽。</p><p>在知乎上回答了李约瑟难题，基本认为李约瑟难题是个伪命题。</p><p>读了《毛泽东自传》《毛泽东传》，一个越了解越觉得伟大的人，千古无二，真神，远胜佛祖、摩西之流。相信毛主义会再次回归神坛。</p><p>读了温铁军的《八次危机》，认识到前三十年的巨大成就和困难，以及后三十年的巨大经济与社会问题。</p><p>在读《临高启明》，一群工业党写的。了解现代国家形成的过程，工业化的巨大困难，以及工业化完成后将带来财富激增。</p><p>看了《无悔追踪》《一年又一年》《天道》这三部连续剧可以串起中国几十年民众的生活状态。</p><p>读了《战后日本经济史》，德国、苏联、日本 的崛起都有社会主义/国家资本主义的原因，与自由市场经济无关。而那些鼓吹自由市场理论的，都是在完成工业化后，才开始采用自由市场，工业化前都有压榨式的原始积累过程。上学时没学好马哲，现在要补习。</p><p>读了《大国悲剧：苏联解体的前因后果》，看了HBO电视剧《切尔诺贝利》。</p><p>了解中国在冷战中的地位与策略。战后历史就是中美苏的斗争历史，其他国家忙着搭便车。</p><p>看了《切腹》《寄生虫》《美国工厂》这几部电影，无产阶级无处不在啊。</p><h2 id="今年"><a href="#今年" class="headerlink" title="今年"></a>今年</h2><p>华为被视为民族英雄，联想被视作美帝买办。</p><p>非洲猪瘟，猪肉价格上涨。</p><p>香港动乱。</p><p>王健林内贷外投，转移财产。</p><p>国庆大阅兵，民心振奋。</p><p>华为胡玲发帖、251事件。</p><p>抖音上忽然走红 沈巍（流浪的大师）、李子柒（世外桃源）、牧马人（文革晚期、改革初期的淳朴生活）。</p><p>中美贸易战接近收官。</p><p>世界各地区社会运动不断。</p><h2 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h2><p>我们正站在世界巨变的前夜，这个巨变将以我们意向不到的方式出现。</p><p>美国会倒下，但我们不知道美国将会如何倒下。中美最根本的战争在金融领域，人民币国际化就是在争夺全球铸币权份额。美元可能会崩溃，但中国拥有的大量美元资产需要安全置换。如果美元霸权能够缓缓的衰落，对中国的影响最小，如果美元霸权骤然终结，也将对中国带来冲击。</p><p>在现在的国际环境下，依靠出口拉动经济的方式是不可持续的，最重要的是提振内需，但目前老百姓手里没钱，内需也拉不动。所以现在必须要平衡贫富差距。缩小贫富差距就要有人掏钱，因此有钱的人都想着抽逃资金。严打不仅稳定社会治安，也会收回大量不法收入。</p><p>房价不会大涨也不会大跌，上涨会让还没买房的人不满意，下跌会让已经买了房的人不满意，所以就锁住，不涨不跌。房子已经进入了计划时代，就像火车票必须凭身份证购买来抑制黄牛是一样的。信息化使计划经济具有了更大的可行性。即使通货膨胀了，房价也不会涨，因为已经不再是市场化环境了。所以本质上，房价在缓缓下跌。</p><p>需要优化财富分配，医疗养老之类的福利会加强，教育科研投入会加大，贫困人口持续扶贫。</p><p>大量高科技依然掌握在欧美手中。中国的普及教育很好，但是高等教育与欧美差距较大，教育改革也是困难重重。仅从本人专业来看，编程语言、操作系统、深度学习框架，都是美国人的。</p><p>生产过剩、人工智能，会影响就业率。新增人口下滑，老龄化加速。</p><p>创业会越来越难，基本上属于解决就业的公益事业。体制内、大平台的职工相当于新的铁饭碗，同时临时工也会越来越多。</p><p>创业的最佳阶段是GDP高速增长的阶段，只要入局基本都有得赚。如果宏观上的财富没有增加，那么创业相当于是在存量市场与其他人竞争。除非有压倒性的武器，找到可以战胜的敌人，否则不要创业。海外市场机会较大。</p><p>创新是有闲阶级特权，有闲不一定是非常富有，但需要有一份保证基本生活的、时间投入少的收入，即“睡后收入”。如果创新可立刻获得回报，则可以进入增强回路循环。</p><p>随着机器人取代越来越多的低端就业，UBI即无条件基本收入的概念会被越来越多的人接受。今年参加美国总统竞选的杨安泽使用了UBI的口号，别人问他钱从哪来，他说向富人收税。桑德斯再次参加大选，美国民主社会主义近年快速崛起。</p><p>传统发达国家因为后发国家的追赶，导致本国产业受到影响，就业率下滑。从而产生越来越多的社会运动，希望获得更高的社会福利，有的地方甚至为了几毛钱的地铁涨价就闹了起来。但这些国家由于债务压力已经很大，经济环境恶化，无法给到更多的社会福利。富人们则利用国际避税手段，来躲避社会责任。一场全球性的萧条和左派革命正在酝酿，那些小政府的发达地区问题最严重（比如香港、韩国是小政府、新加坡是大政府）。</p><p>中美的竞争不会转化为热战，因为中国不想打，美国打不赢。中美握手后，将联手打击国际避税，共治天下。</p><p>资本因其可以转移，在国际共运中，虽然一部分资产被没收，但更多的则逃到了避风港中。苏联解体、改革开放也使大量的公有资产再次被私有化。美国衰落将使资本最大的避风港消失，所有的资本都将处于监管之下。</p><p>中国国内思想分歧增大。近几十年的高速增长，右派认为前人有罪，自己有功，而问题则留给后人处理，大力宣传，所以右派思想居主流。近十年文革一代领导人上台，左派思想逐渐兴起，加之贫富差距、环保、贪腐等问题的存在，助推了左派思潮的壮大。我也是因为近几年的反腐、扶贫、强军才开始重新认识这个国家。</p><p>如果说这一代领导人的思想是在文革中形成的话，那么下一届领导人的思想又是在何时形成的呢？文革给中国续了命，修正主义则给苏联送了命。戈尔巴乔夫说“我们是苏共二十大的孩子，苏联六十年代的历史对我们影响很大”，六十年代的苏联就相当于八十年代的中国，赫鲁晓夫教出了戈尔巴乔夫，戈尔巴乔夫一手送走了苏联。下一届领导人是左还是右？改革开放中形成的利益集团是否已经可以左右政治格局，形成类似日韩的财阀统治？这是未来最大的不确定因素。</p><p>正因为这是未来最大的不确定因素，所以伟人在晚年才要发动文革。不断受人非议的文革，完成了一代人的政治教育，奠定了一代人的思想基础，顶住了世界范围的社会主义颠覆潮。伟人看得太远了，我们无法企及。但是我们虽然顶住了颠覆，却也经历了改革，大量国有资产被私有化，形成了利益集团。几十年过去了，人心不古。既得利益者一定会想方设法垄断政治权力，不加控制结局与明朝无异。解决方案伟人都说过了，而我们首先要做的就是正确评价文革。反对文革的人常拿文革中的极端案例来举例，犯罪案件每个时代都有，我们要注意到文革时期的犯罪率是远低于改革之后的，即便把群众运动中极端案件算上，也比改革之后的犯罪率低。就连当时“受迫害”的人都没有反对文革（比如现任领导人），那些不了解文革的人又凭什么反对呢？妖魔化文革的本质就是妖魔化群众运动，从而顺理成章的剥夺了民众的政治权力。</p><p>扯远了，看来可以写的主题有很多。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过去一年，事业上没多少进步，思想上有贯通之感。这些“无用之学”是一年的无所事事换来，所以需要记录一下。&lt;/p&gt;
&lt;p&gt;今年不再听得到，改听微信读书。办了张图书馆的借书证。主要知识来源：书、知乎、B站。排序与阅读顺序无关&lt;/p&gt;
&lt;p&gt;与同学论战，论战过程中不断提高自己的理论
      
    
    </summary>
    
      <category term="随笔" scheme="http://guileen.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Torch的损失函数和优化器</title>
    <link href="http://guileen.github.io/2019/12/24/torch-output-loss-optimizer/"/>
    <id>http://guileen.github.io/2019/12/24/torch-output-loss-optimizer/</id>
    <published>2019-12-24T14:05:59.000Z</published>
    <updated>2019-12-25T03:43:30.327Z</updated>
    
    <content type="html"><![CDATA[<p>深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。</p><h1 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h1><p>pytorch 损失函数的基本用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = LossCriterion(参数)</span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure><p>Mean Absolute Error<br>torch.nn.L1Loss<br>Measures the mean absolute error.</p><h2 id="Mean-Absolute-Error-L1Loss"><a href="#Mean-Absolute-Error-L1Loss" class="headerlink" title="Mean Absolute Error/ L1Loss"></a>Mean Absolute Error/ L1Loss</h2><p>nn.L1Loss<br><img src="/img/loss/l1loss.png" alt=""><br>很少使用</p><h2 id="Mean-Square-Error-Loss"><a href="#Mean-Square-Error-Loss" class="headerlink" title="Mean Square Error Loss"></a>Mean Square Error Loss</h2><p>nn.MSELoss<br><img src="/img/loss/mseloss.png" alt=""><br>针对数值不大的回归问题。</p><h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><p>nn.SmoothL1Loss<br><img src="/img/loss/smoothl1loss.png" alt=""><br>它在绝对差值大于1时不求平方，可以避免梯度爆炸。大部分回归问题都可以适用，尤其是数值比较大的时候。</p><h2 id="Negative-Log-Likelihood-Loss"><a href="#Negative-Log-Likelihood-Loss" class="headerlink" title="Negative Log-Likelihood Loss"></a>Negative Log-Likelihood Loss</h2><p>torch.nn.NLLLoss，一般与 LogSoftmax 成对使用。使用时 <code>loss(softmaxTarget, target)</code>。用于处理多分类问题。<br><img src="/img/loss/nllloss.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line"><span class="comment"># input is of size N x C = 3 x 5， C为分类数</span></span><br><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># each element in target has to have 0 &lt;= value &lt; C</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>nn.CrossEntropyLoss 将 LogSoftmax 和 NLLLoss 绑定到了一起。所以无需再对结果使用Softmax</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)</span><br><span class="line">target &#x3D; torch.empty(3, dtype&#x3D;torch.long).random_(5)</span><br><span class="line">output &#x3D; loss(input, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h2><p>二分类问题的CrossEntropyLoss。输入、目标结构是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line">input = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="Margin-Ranking-Loss"><a href="#Margin-Ranking-Loss" class="headerlink" title="Margin Ranking Loss"></a>Margin Ranking Loss</h2><p><img src="/img/loss/marginrankingloss.png" alt=""></p><p>常用户增强学习、对抗生成网络、排序任务。给定输入x1，x2，y的值是1或-1，如果y==1表示x1应该比x2的排名更高，y==-1则相反。如果y值与x1、x2顺序一致，那么loss为0，否则错误为 y*(x1-x2)</p><h2 id="Hinge-Embedding-Loss"><a href="#Hinge-Embedding-Loss" class="headerlink" title="Hinge Embedding Loss"></a>Hinge Embedding Loss</h2><p>y的值是1或-1，用于衡量两个输入是否相似或不相似。</p><h2 id="Cosine-Embedding-Loss"><a href="#Cosine-Embedding-Loss" class="headerlink" title="Cosine Embedding Loss"></a>Cosine Embedding Loss</h2><p>给定两个输入x1，x2，y的值是1或-1，用于衡量x1和x2是否相似。<br><img src="/img/loss/cosineembeddingloss.png" alt=""><br>其中cos(x1, x2)表示相似度<br><img src="/img/loss/cossim.png" alt=""></p><h1 id="各种优化器"><a href="#各种优化器" class="headerlink" title="各种优化器"></a>各种优化器</h1><p>大多数情况Adam能够取得比较好的效果。SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 所以说并不是越先进的优化器, 结果越佳.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD 就是随机梯度下降</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line"><span class="comment"># momentum 动量加速,在SGD函数里指定momentum的值即可</span></span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># RMSprop 指定参数alpha</span></span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># Adam 参数betas=(0.9, 0.99)</span></span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。&lt;/p&gt;
&lt;h1 id=&quot;常用损失函数&quot;&gt;&lt;a href=&quot;#常用损失函数&quot; class=&quot;headerlink&quot; title=&quot;常用损失函数&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>理解CNN参数及PyTorch实例</title>
    <link href="http://guileen.github.io/2019/12/24/understanding-cnn/"/>
    <id>http://guileen.github.io/2019/12/24/understanding-cnn/</id>
    <published>2019-12-24T07:56:21.000Z</published>
    <updated>2019-12-25T03:43:17.534Z</updated>
    
    <content type="html"><![CDATA[<p>本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。</p><h2 id="卷积核-kernel"><a href="#卷积核-kernel" class="headerlink" title="卷积核 kernel"></a>卷积核 kernel</h2><p>Kernel，卷积核，有时也称为filter。在迭代过程中，学习的结果就保存在kernel里面。深度学习，学习的就是一个权重。kernel的尺寸越小，计算量越小，一般选择3x3，更小就没有意义了。<br><img src="/img/cnn/kernel_2.png" alt=""></p><p>结果是对卷积核与一小块输入数据的点积。</p><h2 id="层数-Channels"><a href="#层数-Channels" class="headerlink" title="层数 Channels"></a>层数 Channels</h2><p><img src="/img/cnn/channel_1.png" alt=""></p><p>所有位置的点积构成一个激活层。</p><p><img src="/img/cnn/channel_2.png" alt=""></p><p>如果我们有6个卷积核，我们就会有6个激活层。</p><h2 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h2><p><img src="/img/cnn/kernel.gif" alt=""><br>上图是每次向右移动一格，一行结束向下移动一行，所以stride是1x1，如果是移动2格2行则是2x2。</p><h2 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h2><p>Padding的作用是为了获取图片上下左右边缘的特征。<br><img src="/img/cnn/pad.jpg" alt=""></p><h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>卷积层为了提取特征，但是卷积层提取完特征后特征图层依然很大。为了减少计算量，我们可以用padding的方式来减小特征图层。Pooling的方法有MaxPooling核AveragePooling。<br><img src="/img/cnn/pooling.jpg" alt=""></p><p>推荐看一下李飞飞的<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf" target="_blank" rel="noopener">这篇slide</a></p><h2 id="PyTorch-中的相关方法"><a href="#PyTorch-中的相关方法" class="headerlink" title="PyTorch 中的相关方法"></a>PyTorch 中的相关方法</h2><ul><li><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=’zeros’)</p></li><li><p>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</p><ul><li>stride 默认与kernel_size相等</li></ul></li><li><p>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</p></li><li><p>Tensor.view(*shape) -&gt; Tensor</p><ul><li>用于将卷积层展开为全连接层<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; torch.randn(4, 4)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y &#x3D; x.view(16)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z &#x3D; x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="MNIST例子"><a href="#MNIST例子" class="headerlink" title="MNIST例子"></a>MNIST例子</h2><p>MNIST 数据集的输入是 1x28x28 的数据集。在实际开发中必须要清楚每一次的输出结构。</p><ul><li>我们第一层使用 5x5的卷积核，步长为1，padding为0，28-5+1 = 24，那么输出就是 24x24。计算方法是 (input_size - kernel_size)/ stride + 1。</li><li>我们第二层使用 2x2的MaxPool，那么输出为 12x12.</li><li>第三层再使用5x5，卷积核，输出则为 12-5+1，即 8x8。</li><li>再使用 2x2 MaxPool，输出则为 4x4。</li></ul><p><img src="/img/cnn/mnist_convet.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""ConvNet -&gt; Max_Pool -&gt; RELU -&gt; ConvNet -&gt; Max_Pool -&gt; RELU -&gt; FC -&gt; RELU -&gt; FC -&gt; SOFTMAX"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>以上代码摘自 <a href="https://github.com/floydhub/mnist" target="_blank" rel="noopener">https://github.com/floydhub/mnist</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。&lt;/p&gt;
&lt;h2 id=&quot;卷积核-kernel&quot;&gt;&lt;a href=&quot;#卷积核-kernel&quot; class=&quot;headerlink&quot; title=&quot;卷积
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
</feed>
