<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>桂糊涂的博客</title>
  
  <subtitle>代码杂记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guileen.github.io/"/>
  <updated>2021-02-18T13:12:37.862Z</updated>
  <id>http://guileen.github.io/</id>
  
  <author>
    <name>桂糊涂</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>BRDF双向反射分布函数</title>
    <link href="http://guileen.github.io/2021/02/18/brdf-pbr/"/>
    <id>http://guileen.github.io/2021/02/18/brdf-pbr/</id>
    <published>2021-02-18T13:12:37.000Z</published>
    <updated>2021-02-18T13:12:37.862Z</updated>
    
    <content type="html"><![CDATA[<p>双向反射分布函数（bidirectional reflectance distribution function）$f_r(omega_i,omega_r)$是一个计算光照反射量的函数。$omega_i$表示输入光角度，$omega_r$表示反射光角度，函数返回反射光辐射率。$omega$由球面坐标系的$phi$,$theta$角度表示，因此brdf函数共有4个参数。brdf的单位是每立体角$sr^(-1)$。</p><img src="/img/brdf/spherical-coordinates.png" style="width:50%;" /><img src="/img/brdf/solid-angle-1sr.png" style="width:45%;margin-top:5%;" /><h3 id="辐射度量学-Radiometry"><a href="#辐射度量学-Radiometry" class="headerlink" title="辐射度量学(Radiometry)"></a>辐射度量学(Radiometry)</h3><table><thead><tr><th>物理量</th><th>符号</th><th>公式</th><th>国际单位制</th><th>单位符号</th><th>注释</th></tr></thead><tbody><tr><td><a href="https://zh.wikipedia.org/wiki/辐射能" target="_blank" rel="noopener">辐射能</a>（Radiant energy）</td><td>$Q_e$</td><td></td><td><a href="https://zh.wikipedia.org/wiki/焦耳" target="_blank" rel="noopener">焦耳</a></td><td>$J$</td><td>能量。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射通量" target="_blank" rel="noopener">辐射通量</a>（Radiant flux）</td><td>$Phi_e$</td><td>$Phi=(dQ)/(dt)$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a></td><td>$W$</td><td>每单位时间的辐射能量，亦作“辐射功率”。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射强度" target="_blank" rel="noopener">辐射强度</a>（Radiant intensity）</td><td>$I_e$</td><td>$I=(dPhi)/(d omega)</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每<a href="https://zh.wikipedia.org/wiki/球面度" target="_blank" rel="noopener">球面度</a></td><td>$W*sr^(-1)$</td><td>每单位<a href="https://zh.wikipedia.org/wiki/立體角" target="_blank" rel="noopener">立体角</a>的辐射通量。</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/輻照度" target="_blank" rel="noopener">辐照度</a>（Irradiance）（辉度）</td><td>$E_e$</td><td>$E=(dPhi)/(dA)=int_(Omega)  L(omega)cos theta d omega$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每平方米</td><td>$W*m^(-2)$</td><td>入射表面的辐射通量</td></tr><tr><td><a href="https://zh.wikipedia.org/wiki/辐射率" target="_blank" rel="noopener">辐射率</a>（Radiance）(光亮度）</td><td>$L_e$</td><td>$(d^2Phi)/(dAcos theta d omega)$</td><td><a href="https://zh.wikipedia.org/wiki/瓦特" target="_blank" rel="noopener">瓦特</a>每<a href="https://zh.wikipedia.org/wiki/球面度" target="_blank" rel="noopener">球面度</a>每平方米</td><td>$W*sr^(-1)*m^(-2)$</td><td>每单位<a href="https://zh.wikipedia.org/wiki/立體角" target="_blank" rel="noopener">立体角</a>每单位投射表面的<a href="https://zh.wikipedia.org/wiki/辐射通量" target="_blank" rel="noopener">辐射通量</a>。<strong>相当于辐射强度在dA上的微分</strong></td></tr><tr><td></td><td></td><td></td><td></td><td></td><td></td></tr></tbody></table><p>BRDF由Fred Nicodemus在1965年提出，函数如下：</p><p>$$f_r(omega_i,omega_r) = (dL_r(omega_r))/(dE_i(omega_i)) = (dL_r(omega_r))/(L_i(omega_i)cos theta_i d omega_i) $$</p><img src="/Users/admin/work/guileen.github.com/hexo/source/img/brdf/brdf-diagram.png" style="width:50%;" /><p>这个公式之所以定义为辐射率（radiance）和辐照度（irradiance）之比，而不是radiance和radiance之比，或irradiance和irradiance之比。是因为当考虑入射时，我们需要考虑入射光在面积上的分量，所以irradiance译为辐<strong>照</strong>度。当考虑反射时，我们需要考虑每立体角的辐射通量，并且这个辐射通量最终投影在屏幕（视网膜）面积上的辐射通量，因此我们用辐射率。如果我们用点光源，入射光的计算似乎也是可以用辐射率的，但有时我们还要考虑平行光的情况，那么对于入射光就不存在每立体角的概念了，因此对于入射光照我们用辐照度，反射我们用辐射率。</p><h3 id="基于物理的BRDF模型-PBR，Physically-based-rendering"><a href="#基于物理的BRDF模型-PBR，Physically-based-rendering" class="headerlink" title="基于物理的BRDF模型(PBR，Physically-based rendering)"></a>基于物理的BRDF模型(PBR，Physically-based rendering)</h3><h4 id="次表面散射（Subsurface-scattering）"><a href="#次表面散射（Subsurface-scattering）" class="headerlink" title="次表面散射（Subsurface scattering）"></a>次表面散射（Subsurface scattering）</h4><p>是一些半透明物质比如皮肤、玉石、大理石、塑料等。当光入射到材料表面后，一部分被反射、一部分被吸收、还有一部分经历透射，透射光在材料内部进行多次不规则的反射之后，又从不同角度反射了回来。</p><h4 id="菲涅尔反射（Fresnel-Reflectance）"><a href="#菲涅尔反射（Fresnel-Reflectance）" class="headerlink" title="菲涅尔反射（Fresnel Reflectance）"></a>菲涅尔反射（Fresnel Reflectance）</h4><p>当光从一种折射率为$n_1$的介质向另一种折射率为$n_2$的介质传播时，在两者的交界处可能会同时发生光的反射和折射。<a href="https://zh.wikipedia.org/wiki/%E8%8F%B2%E6%B6%85%E8%80%B3%E6%96%B9%E7%A8%8B" target="_blank" rel="noopener">菲涅尔方程</a>描述了光波的不同分量被折射和反射的情况，也描述了波反射时的相变。光线会随着我们的观察角度而反射不同的亮度，当我们以垂直与水面的角度观察池塘时，我们可以看到池塘的底部，但当我们以平行于水面的角度观察水面时，反射光则会很强我们无法看到池底。</p><h4 id="微表面理论（Microfacet-Theory）"><a href="#微表面理论（Microfacet-Theory）" class="headerlink" title="微表面理论（Microfacet Theory）"></a>微表面理论（Microfacet Theory）</h4><p>微表面理论假设材质的表面是由不同方向的微小细节平面（microfacet）所构成，反射光线由这些微表面的法线分布决定。我们用法线分布函数（Normal Distribution Function，NDF），D(h) 来描述表面的法线分布概率。h表示视角与入射光角度之间的半程向量。</p><p><img src="/img/brdf/microfacet.jpg" alt=""></p><p>$$f(i,o) = (F(i,h)G(i,o,h)D(h))/(4(n,i)(n,o))$$ </p><p>其中F(i,h)表示菲涅尔项，表示所有反射的比例。G(i,o,h) 表示自投影项，当光线几乎平射于微表面时，光线则将被粗糙的表面自我遮挡掉。D(h)表示法线分布。</p><p>参考:</p><p><a href="https://en.wikipedia.org/wiki/Bidirectional_reflectance_distribution_function" target="_blank" rel="noopener">Wikipedia:《Bidirectional reflectance distribution function》</a></p><p>《Real-Time Rendering, 4th edition》</p><p><a href="https://github.com/QianMo/Real-Time-Rendering-3rd-CN-Summary-Ebook" target="_blank" rel="noopener">《Real-Time Rendering 3rd》提炼总结</a></p><p><a href="https://zhuanlan.zhihu.com/p/20119162" target="_blank" rel="noopener"> Microfacet材质和多层材质——文刀秋二</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;双向反射分布函数（bidirectional reflectance distribution function）$f_r(omega_i,omega_r)$是一个计算光照反射量的函数。$omega_i$表示输入光角度，$omega_r$表示反射光角度，函数返回反射光辐射率
      
    
    </summary>
    
    
      <category term="computer-graphics" scheme="http://guileen.github.io/tags/computer-graphics/"/>
    
  </entry>
  
  <entry>
    <title>线性代数的本质</title>
    <link href="http://guileen.github.io/2021/02/18/the-essence-of-linear-algebra/"/>
    <id>http://guileen.github.io/2021/02/18/the-essence-of-linear-algebra/</id>
    <published>2021-02-18T04:03:51.000Z</published>
    <updated>2021-02-18T04:03:51.927Z</updated>
    
    <content type="html"><![CDATA[<p>当我在大学中学习线性代数的时候，我不知所云且不以为然。然后随着不断的学习，我发现不懂线性代数是没法在更深的技术领域里混的。比如机器学习、计算机图形学等等，对于其他的科研领域也都是同样的。如果学不好线性代数既不是我的问题，也不是线性代数的问题，那到底是什么问题？最近学些了<a href="https://www.bilibili.com/video/BV1X7411F744" target="_blank" rel="noopener">现代计算机图形学入门-闫令琪</a> <a href="https://www.bilibili.com/video/BV1ib411t7YR" target="_blank" rel="noopener">线性代数的本质-3blue1brown</a> 这两个系列视频使我对线性代数多了更多的感性认识，而<a href="https://www.marxists.org/chinese/maozedong/marxist.org-chinese-mao-193707.htm" target="_blank" rel="noopener">《实践论》</a> 告诉我们，<strong>理性认识是要依赖于感性认识的</strong>。传统的线性代数教材则希望构建一套完全自洽的纯粹的数学理论，它不依赖于现实世界的知识。这或许可以满足一些数学家的成就感，但这种马后炮式的“创造”是脱离历史的。线性代数就像其他的学科一样，不可能是仅凭想象产生的，虽然数学家可以伪装成不依赖历史发展而独立自洽，但这除了是一种智力游戏外，对于认识世界、传播知识并没有任何帮助。我们就在这种缺乏感性认识的数学教育中丧失了对数学理论的兴趣，岂不哀哉？</p><p>吐槽结束，进入正题。谈谈我此刻对线性代数的理解，探讨一下他的本质到底是什么。我们是否会问自己，加减乘除的本质到底是什么？我们之所以不这么问，是因为我们已经理解了四则运算的本质。我们不会问关于十进制的本质，因为日常生活中已经给我们建立了足够多的经验。但我们在学龄前的阶段，我们则可能对十进制和加减乘除充满了困惑。但我们接触线性代数太晚，我们并没有足够的练习和日常应用使我们建立起感性认识。当我们在商店里消费的时候四则运算不断强化着我们的认知，但线性代数缺少这样的机会。当我们被教授四则运算时，老师把我们当作一个普通的人类，会告诉我们3个苹果+2个苹果=5个苹果，这种现实世界的例子帮助我们更好的理解了四则运算。但当我们学习线性代数时，我们则变成了一个个抽象的理性机器，这个系统只告诉我们各种定义、运算规则，然后要求我们像计算机一样的运行，计算出结果。What are we doing？我们怎么可能不懵圈呢？线性代数就是一个增强版的加减乘除，但没有足够的案例使我们不知道我们的计算究竟代表着什么？AlphaGo就算赢了李世石，但他不知道自己在干什么。我们作为人类的尊严在哪里？我又没控制好自己的情绪，让我们回到正题。《线性代数及其应用》是一本很好的教材，他和国内教材最大的区别就在于“应用”上，这本书中列举了大量的例子来说明线性代数的应用。这本书的开头说道“<strong>线性代数是一门语言，必须用学习外语的方法每天学习这种语言</strong>”。</p><p>我们从鸡兔同笼来举个例子。鸡兔同笼是小学阶段的奥数题，也就是在小学的数学语言中，这是一道很难描述的题。到了中学阶段我们可以用未知数x表示鸡的数量，未知数y表示兔的数量，并列出方程。而对于线性代数的语言，我们用向量$$((a),(b))$$表示鸡和兔的数量，如果我们有非常多的未知数，我们不希望定义太多的未知数符号，我们直接用$$x$$表示这个n维变量。我们有一个变换矩阵$$[[1,1],[2,4]]$$ 表示鸡有1个头2只脚，兔有1个头4只脚 。如果有3只鸡5只兔则 $$[[1,1],[2,4]]*[[3],[5]]=[[3*1+5*1],[3*2+5*4]]=[[8],[26]]$$，它代表着我们将一个“鸡兔向量”映射到了“头脚向量”的空间中，共有8只头，26只脚。</p><p>我们知道函数是一种映射，$$f(x)=y$$代表将$x$到$y$的映射关系。矩阵乘法叫做线性变换，线性变换是一种函数映射，但函数映射不一定是线性变化。因此线性变换是符合函数的性质的。如果函数是可逆的，则有$$x=f^(-1)(y)$$，同样的，对于矩阵而言，$$若A是可逆的，且Ax=b，则x=A^(-1)b。设A=[[a,b],[c,d]]，则A^(-1)=1/(ad-bc)[[d,-b],[-c,a]]$$。对于鸡兔同笼问题，$$A=[[1,1],[2,4]]，则A^(-1)=1/2[[4,-1],[-2,1]]，当有8头26脚时，x=1/2[[4,-1],[-2,1]]*[[8],[26]]=1/2[[4*8-26],[-2*8+26]]=1/2[[6],[10]]=[[3],[5]]，即3只鸡5只兔。$$最重要的是，这整个计算过程，计算机可以轻松的完成，并且可以用定义标准化的操作，因为操作标准化，计算机可以被设计的更加擅长处理这类操作。这就是线性代数得到广泛应用的一个最重要原因。</p><p>我们可以从鸡兔变换到头脚，我们也可以从产量变换到成本收益（这是经济学的应用），我们也可以从速度变换到阻力（这是空气动力学的应用），我们也可以将3D空间变换到3D或2D空间（这是计算机图形学的应用），我们也可以将用户行为维度变换到兴趣标签维度（这是机器学习推荐系统的应用）。这都说明了线性代数是一门“语言”，是一个工具。并不是因为线性代数，所以这些定理存在，而是因为这些规律本身存在，才能有线性代数这门工具。人类是巧妙的“发明”了线性代数，而不是“发现”了线性代数。</p><p>线性变换可能进行多次，就像映射可以进行多次一样。因为矩阵的乘法就是一种特殊的函数，函数满足结合律$$g(f(h(x)))=((g @ f)(h(x)))$$，所以矩阵乘法也符合结合律$$A(BC)=(AB)C$$。</p><p>TODO 叉乘表示面积和垂直与平面的向量，行列式表示体积，秩表示维数，特征值与特征向量，表示空间中在线性变换中保持稳定的轴，最小二乘法</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;当我在大学中学习线性代数的时候，我不知所云且不以为然。然后随着不断的学习，我发现不懂线性代数是没法在更深的技术领域里混的。比如机器学习、计算机图形学等等，对于其他的科研领域也都是同样的。如果学不好线性代数既不是我的问题，也不是线性代数的问题，那到底是什么问题？最近学些了&lt;a
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>从余弦定理到冯氏光照模型</title>
    <link href="http://guileen.github.io/2021/02/07/law-of-cosines-and-phong-reflection-model/"/>
    <id>http://guileen.github.io/2021/02/07/law-of-cosines-and-phong-reflection-model/</id>
    <published>2021-02-06T17:01:33.000Z</published>
    <updated>2021-02-18T03:59:48.261Z</updated>
    
    <content type="html"><![CDATA[<h3 id="1-勾股定理——宇宙的密码"><a href="#1-勾股定理——宇宙的密码" class="headerlink" title="1.勾股定理——宇宙的密码"></a>1.勾股定理——宇宙的密码</h3><p>$a^2+b^2=c^2$。下图是勾股定理的一个直观证明。</p><p><img src="/img/math/ggdl.png" alt=""></p><h3 id="2-余弦定理，角与边的关系"><a href="#2-余弦定理，角与边的关系" class="headerlink" title="2. 余弦定理，角与边的关系"></a>2. 余弦定理，角与边的关系</h3><img src="/img/math/q-cosine.svg.png" width="30%"><img src="/img/math/cosine-1.png" width="30%"> <img src="/img/math/cos-sin.png" width="30%"><p>三条边可以确定一个三角形，已知三角形的三条边长，如何求出其角度呢？</p><p>由$cos,sin$定义可知<br>  $$ c = a * cos beta + b * cos alpha $$<br>  两边同乘c得：<br>$$ c^2 = ac * cos beta + bc * cos alpha $$<br>  同理可得：<br>$$ a^2 = ac cos beta + ab * cos gamma $$<br>$$ b^2 = bc cos alpha + ab cos gamma $$<br>  故：$$ a^2+b^2-c^2 = 2abcosgamma $$<br>  可得：$$ c^2 = a^2 + b^2 - 2ab cos gamma $$<br></p><h3 id="3-向量的定义（方向）"><a href="#3-向量的定义（方向）" class="headerlink" title="3. 向量的定义（方向）"></a>3. 向量的定义（方向）</h3><img src="/img/math/vector_subtraction.svg.png" width="45%"><img src="/img/math/vector_addition.svg.png" width="45%"><p>$$令 vec c = vec a - vec b$$, $$theta$$为$$vec a$$ $$vec b$$ 的夹角。余弦定理可以用向量形式写成 $$ | vec c |^2 = |vec a|^2 + |vec b|^2 -  2 |vec a| |vec b| cos theta $$ </p><h3 id="4-点积（dot-product）的代数定义"><a href="#4-点积（dot-product）的代数定义" class="headerlink" title="4. 点积（dot product）的代数定义"></a>4. 点积（dot product）的代数定义</h3><p>两个向量的点积是一个标量。向量$$vec a=[a_1, a_2, … a_n]$$与向量$$vec b=[b_1, b_2, … b_n]$$的点积定义为: $$ vec a * vec b = sum_(i=1)^n a_i b_i = a_1 b_1 + a_2 b_2 + … a_n b_n $$。</p><p>点积有以下性质（证略）：</p><ol><li>满足交换律 $$vec a * vec b = vec b * vec a$$</li><li>满足分配律 $$vec a * (vec b + vec c) = vec a * vec b + vec a * vec c$$</li><li>乘以标量时满足 $$ (c_1 vec a) * (c_2 vec b) = (c_1 c_2)(vec a * vec b)$$</li><li>不满足结合律。因为标量 $$ vec a * vec b $$ 与向量 $$ vec c $$ 的点积没有定义，所以$$(vec a * vec b) * vec c=vec a * (vec b * vec c)$$ 没有意义。</li></ol><p>点积的代数定义简单实用，易于表示，也易于使用计算机程序处理。是线性代数的基本操作之一。</p><h3 id="5-点积的几何意义"><a href="#5-点积的几何意义" class="headerlink" title="5. 点积的几何意义"></a>5. 点积的几何意义</h3><p>对于任何一个n维向量有 $|vec a|^2=a_1^2+a_2^2+…+a_n^2$。根据勾股定理，这是很显然的。换个角度<strong>说如果没有勾股定理，这一步就不存在，后面的内容也不存在了。而勾股定理不是由代数方法证明的，而是独立于代数系统之外的空间基本性质。而空间和时间是宇宙最根本的本质。这就是勾股定理最神奇的地方</strong>。</p><p>我们根据点积的定义可知：$$ vec a * vec a = a_1 * a_1 + a_2 * a_2 + … a_n * a_n = |vec a|^2$$ 即 $$ vec a * vec a == |vec a|^2$$</p><p>我们根据余弦定理的向量表示可得：$$ vec c * vec c = vec a * vec a + vec b * vec b - 2 |vec a| |vec b| cos theta . (1)$$ </p><p>根据向量的定义 $$ vec c = vec a - vec b $$ 有 $$ vec c * vec c = (vec a - vec b) * (vec a - vec b) = vec a * vec a + vec b * vec b - 2 vec a * vec b . (2)$$</p><p>结合等式$$(1)$$、$$(2)$$有 $$vec a * vec b = |vec a| |vec b| cos theta$$。一个看似简单的代数点积操作，竟然和夹角余弦相关，真是不可思议。</p><p>点积的几何意义是什么呢？关键就在这个$cos theta$，如果$$|vec b|$$为1时候，我们可以将$$vec a * vec b$$视为$$vec a $$在$$vec b$$方向上的投影长度。</p><p><img src="/img/math/dot_product_1.png" width="45%"> <img src="/img/math/dot_product_2.png" width="45%"></p><h3 id="6-点积的物理意义（从数学到宇宙）"><a href="#6-点积的物理意义（从数学到宇宙）" class="headerlink" title="6. 点积的物理意义（从数学到宇宙）"></a>6. 点积的物理意义（从数学到宇宙）</h3><p>点积的物理意义就是向量在某方向上的投影长度。这在物理上可以表达力在某方向上的投影，光在某方向的投影，速度、加速度在某方向的投影。而点积的操作，可以使我们只需要关心这些物理量的向量表示，而不需要去关心夹角，不需要去计算三角函数。而在统计学、机器学习等方面，余弦可以表示两个向量之间的相似性，比如两个词向量，两个用户的兴趣向量等，应用非常广泛。下面就以计算机图形学举例来说明点积的应用。</p><p>冯氏光照模型将一个物体的光照分解为环境光+漫反射光+镜面反射光。</p><p><img src="/img/math/Phong_components_version_4.png" alt=""></p><p>环境光比较简单就是一个常量。而漫反射光，则为光照强度在平面的法线方向的投影，与法线方向一致则光照最强。镜面反射光则为反射光方向在视角方向上的投影，与视角完全一致，则反射光最强。</p><p><img src="/img/math/diffuse_light.png" width="45%"> <img src="/img/math/specular_light.png" width="45%"></p><p>OpenGL的shader大致如下：</p><figure class="highlight glsl"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="type">vec3</span> CalcDirLight(DirLight light, <span class="type">vec3</span> normal, <span class="type">vec3</span> viewDir) &#123;</span><br><span class="line">    <span class="comment">// normalize 归一化，使法线向量的长度为1</span></span><br><span class="line">    <span class="type">vec3</span> lightDir = <span class="built_in">normalize</span>(-light.direction);</span><br><span class="line">    <span class="comment">// 漫反射光. dot 计算cos* 强度， max把负值最多降到0，表示全黑。</span></span><br><span class="line">    <span class="type">float</span> diff = <span class="built_in">max</span>(<span class="built_in">dot</span>(normal, lightDir), <span class="number">0.0</span>);</span><br><span class="line">    <span class="type">vec3</span> reflectDir = <span class="built_in">reflect</span>(-lightDir, normal);</span><br><span class="line">    <span class="comment">// dot 计算反射光在视角上的cos分量，至少为0。使用pow，模拟镜面光焦点分布集中度，shininess越高要求反射分量越接近于1</span></span><br><span class="line">    <span class="comment">// 反射分量==1 表示必须视角恰巧与反射角完全一致才能看到反射光，也就是绝对镜面</span></span><br><span class="line">    <span class="type">float</span> spec = <span class="built_in">pow</span>(<span class="built_in">max</span>(<span class="built_in">dot</span>(viewDir, reflectDir), <span class="number">0.0</span>), material.shininess);</span><br><span class="line">    <span class="comment">// 合并冯氏光照结果</span></span><br><span class="line">    <span class="type">vec3</span> ambient = light.ambient * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.diffuse, TexCoords));</span><br><span class="line">    <span class="type">vec3</span> diffuse = light.diffuse * diff * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.diffuse, TexCoords));</span><br><span class="line">    <span class="type">vec3</span> specular = light.specular * spec * <span class="type">vec3</span>(<span class="built_in">texture</span>(material.specular, TexCoords));</span><br><span class="line">    <span class="keyword">return</span> (ambient + diffuse + specular);</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h3 id=&quot;1-勾股定理——宇宙的密码&quot;&gt;&lt;a href=&quot;#1-勾股定理——宇宙的密码&quot; class=&quot;headerlink&quot; title=&quot;1.勾股定理——宇宙的密码&quot;&gt;&lt;/a&gt;1.勾股定理——宇宙的密码&lt;/h3&gt;&lt;p&gt;$a^2+b^2=c^2$。下图是勾股定理的一个直观
      
    
    </summary>
    
    
      <category term="OpenGL,math" scheme="http://guileen.github.io/tags/OpenGL-math/"/>
    
  </entry>
  
  <entry>
    <title>学习CMake</title>
    <link href="http://guileen.github.io/2021/01/11/learn-cmake/"/>
    <id>http://guileen.github.io/2021/01/11/learn-cmake/</id>
    <published>2021-01-11T12:56:32.000Z</published>
    <updated>2021-01-11T14:07:41.861Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://hsf-training.github.io/hsf-training-cmake-webpage/" target="_blank" rel="noopener">https://hsf-training.github.io/hsf-training-cmake-webpage/</a></p><h2 id="1-构建"><a href="#1-构建" class="headerlink" title="1. 构建"></a>1. 构建</h2><h3 id="构建"><a href="#构建" class="headerlink" title="构建"></a>构建</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">git clone --recursive https://github.com/CLIUtils/CLI11.git</span><br><span class="line">cd CLI11</span><br><span class="line">cmake -S . -B build</span><br><span class="line">cmake --build build</span><br><span class="line">cmake --build build --target test</span><br></pre></td></tr></table></figure><h3 id="另一种构建"><a href="#另一种构建" class="headerlink" title="另一种构建"></a>另一种构建</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">mkdir build</span><br><span class="line">cd build</span><br><span class="line">cmake ..</span><br><span class="line">make</span><br><span class="line">make test</span><br></pre></td></tr></table></figure><p><strong>永远不要</strong>在源码目录直接 <code>cmake .</code> 这样会污染源码目录。</p><h3 id="选择编译器"><a href="#选择编译器" class="headerlink" title="选择编译器"></a>选择编译器</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">CC=clang CXX=clang++ cmake -S . -B build</span><br></pre></td></tr></table></figure><h3 id="设置选项-D-列出选项-L"><a href="#设置选项-D-列出选项-L" class="headerlink" title="设置选项 -D 列出选项 -L"></a>设置选项 <code>-D</code> 列出选项 <code>-L</code></h3><ul><li><a href="https://cmake.org/cmake/help/latest/variable/CMAKE_BUILD_TYPE.html" target="_blank" rel="noopener"><code>CMAKE_BUILD_TYPE</code></a>:  <code>Release</code>, <code>RelWithDebInfo</code>, <code>Debug</code>, 或其他编译选项。</li><li><a href="https://cmake.org/cmake/help/latest/variable/CMAKE_INSTALL_PREFIX.html" target="_blank" rel="noopener"><code>CMAKE_INSTALL_PREFIX</code></a>: 安装位置，Unix上默认是 <code>/usr/local</code> , 用户安装目录常是 <code>~/.local</code> </li><li><a href="https://cmake.org/cmake/help/latest/variable/BUILD_SHARED_LIBS.html" target="_blank" rel="noopener"><code>BUILD_SHARED_LIBS</code></a>:  <code>ON</code> or <code>OFF</code> </li><li><a href="https://cmake.org/cmake/help/latest/module/CTest.html" target="_blank" rel="noopener"><code>BUILD_TESTING</code></a>: </li></ul><h3 id="调试Cmake-files-在source目录执行下面的命令："><a href="#调试Cmake-files-在source目录执行下面的命令：" class="headerlink" title="调试Cmake files 在source目录执行下面的命令："></a>调试Cmake files 在source目录执行下面的命令：</h3><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake build --trace-source="CMakeLists.txt"</span><br></pre></td></tr></table></figure><h2 id="2-编写-CMakeLists-txt"><a href="#2-编写-CMakeLists-txt" class="headerlink" title="2. 编写 CMakeLists.txt"></a>2. 编写 CMakeLists.txt</h2><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>)</span><br><span class="line"><span class="comment"># 项目名称，未设置LANGUAGES的话，则是 C, CXX 的混合项目</span></span><br><span class="line"><span class="keyword">project</span>(MyProject)</span><br><span class="line"><span class="comment"># 至少一个 add_executeable 或 add_library 来作为target。</span></span><br><span class="line"><span class="keyword">add_executable</span>(myexample simple.cpp)</span><br></pre></td></tr></table></figure><p>可以设置更多信息</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 最小。。最大版本</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>...<span class="number">3.18</span>)</span><br><span class="line"><span class="comment"># 更详细的项目信息</span></span><br><span class="line"><span class="keyword">project</span>(MyProject</span><br><span class="line">  VERSION</span><br><span class="line">    <span class="number">1.0</span></span><br><span class="line">  DESCRIPTION</span><br><span class="line">    <span class="string">"Very nice project"</span></span><br><span class="line">  LANGUAGES</span><br><span class="line">    CXX</span><br><span class="line">)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 可添加 STATIC, SHARED, or MODULE; 默认通过 BUILD_SHARED_LIBS 选择.</span></span><br><span class="line"><span class="keyword">add_library</span>(mylibrary simplelib.cpp)</span><br></pre></td></tr></table></figure><h3 id="Target可用的设置"><a href="#Target可用的设置" class="headerlink" title="Target可用的设置"></a>Target可用的设置</h3><ul><li><a href="https://cmake.org/cmake/help/latest/command/target_link_libraries.html" target="_blank" rel="noopener"><code>target_link_libraries</code></a>: Other targets; can also pass library names directly</li><li><a href="https://cmake.org/cmake/help/latest/command/target_include_directories.html" target="_blank" rel="noopener"><code>target_include_directories</code></a>: Include directories</li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_features.html" target="_blank" rel="noopener"><code>target_compile_features</code></a>: The compiler features you need activated, like <code>cxx_std_11</code></li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_definitions.html" target="_blank" rel="noopener"><code>target_compile_definitions</code></a>: Definitions</li><li><a href="https://cmake.org/cmake/help/latest/command/target_compile_options.html" target="_blank" rel="noopener"><code>target_compile_options</code></a>: More general compile flags</li><li><a href="https://cmake.org/cmake/help/latest/command/target_link_directories.html" target="_blank" rel="noopener"><code>target_link_directories</code></a>: Don’t use, give full paths instead (CMake 3.13+)</li><li><a href="https://cmake.org/cmake/help/latest/command/target_link_options.html" target="_blank" rel="noopener"><code>target_link_options</code></a>: General link flags (CMake 3.13+)</li><li><a href="https://cmake.org/cmake/help/latest/command/target_sources.html" target="_blank" rel="noopener"><code>target_sources</code></a>: Add source files</li></ul><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 不需要添加任何source，导出一个 header-only library。</span></span><br><span class="line"><span class="keyword">add_library</span>(some_header_only_lib INTERFACE)</span><br></pre></td></tr></table></figure><h4 id="什么是-INTERFACE-IMPORETED？？"><a href="#什么是-INTERFACE-IMPORETED？？" class="headerlink" title="什么是 INTERFACE IMPORETED？？"></a>什么是 INTERFACE IMPORETED？？</h4><h3 id="Script"><a href="#Script" class="headerlink" title="Script"></a>Script</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># cache.cmake</span></span><br><span class="line"><span class="comment"># 设置变量</span></span><br><span class="line"><span class="keyword">set</span>(MY_VARIABLE <span class="string">"I am a variable"</span>)</span><br><span class="line"><span class="keyword">message</span>(STATUS <span class="string">"$&#123;MY_VARIABLE&#125;"</span>)</span><br><span class="line"></span><br><span class="line"><span class="keyword">set</span>(MY_CACHE_VAR <span class="string">"I am a cached variable"</span> CACHE <span class="keyword">STRING</span> <span class="string">"Description"</span>)</span><br><span class="line"><span class="keyword">message</span>(STATUS <span class="string">"$&#123;MY_CACHE_VAR&#125;"</span>)</span><br></pre></td></tr></table></figure><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">cmake -DMY_CACHE_VAR="command line" -P cache.cmake</span><br></pre></td></tr></table></figure><p>Try setting a cached variable using <code>-DMY_VARIABLE=something</code> <em>before</em> the <code>-P</code>. Which variable is shown?</p><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">option</span>(MY_OPTION <span class="string">"On or off"</span> <span class="keyword">OFF</span>)</span><br><span class="line"><span class="comment"># $ENV&#123;name&#125;</span></span><br><span class="line"><span class="comment"># if(DEFINED ENV&#123;name&#125;)  </span></span><br><span class="line"><span class="keyword">file</span>(GLOB OUTPUT_VAR *.cxx)</span><br><span class="line"><span class="keyword">file</span>(GLOB_RECURSE  OUTPU_VAR *.cxx)</span><br></pre></td></tr></table></figure><h2 id="3-项目结构"><a href="#3-项目结构" class="headerlink" title="3. 项目结构"></a>3. 项目结构</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">code&#x2F;03-structure&#x2F;</span><br><span class="line">├── CMakeLists.txt</span><br><span class="line">├── README.md</span><br><span class="line">├── apps</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── app.cpp</span><br><span class="line">├── cmake</span><br><span class="line">│   └── FindSomeLib.cmake</span><br><span class="line">├── docs</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── mainpage.md</span><br><span class="line">├── include</span><br><span class="line">│   └── modern</span><br><span class="line">│       └── lib.hpp</span><br><span class="line">├── src</span><br><span class="line">│   ├── CMakeLists.txt</span><br><span class="line">│   └── lib.cpp</span><br><span class="line">└── tests</span><br><span class="line">    ├── CMakeLists.txt</span><br><span class="line">    └── testlib.cpp</span><br></pre></td></tr></table></figure><h3 id="CMakeLists-txt"><a href="#CMakeLists-txt" class="headerlink" title="/CMakeLists.txt"></a>/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br><span class="line">35</span><br><span class="line">36</span><br><span class="line">37</span><br><span class="line">38</span><br><span class="line">39</span><br><span class="line">40</span><br><span class="line">41</span><br><span class="line">42</span><br><span class="line">43</span><br><span class="line">44</span><br><span class="line">45</span><br><span class="line">46</span><br><span class="line">47</span><br><span class="line">48</span><br><span class="line">49</span><br><span class="line">50</span><br><span class="line">51</span><br><span class="line">52</span><br><span class="line">53</span><br><span class="line">54</span><br><span class="line">55</span><br><span class="line">56</span><br><span class="line">57</span><br><span class="line">58</span><br><span class="line">59</span><br><span class="line">60</span><br><span class="line">61</span><br><span class="line">62</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Works with 3.14 and tested through 3.18</span></span><br><span class="line"><span class="keyword">cmake_minimum_required</span>(VERSION <span class="number">3.14</span>...<span class="number">3.18</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Project name and a few useful settings. Other commands can pick up the results</span></span><br><span class="line"><span class="keyword">project</span>(</span><br><span class="line">  ModernCMakeExample</span><br><span class="line">  VERSION <span class="number">0.1</span></span><br><span class="line">  DESCRIPTION <span class="string">"An example project with CMake"</span></span><br><span class="line">  LANGUAGES CXX)</span><br><span class="line"></span><br><span class="line"><span class="comment"># 仅在主项目中运行，若是子项目中 add_subdirectory 则忽略</span></span><br><span class="line"><span class="keyword">if</span>(CMAKE_PROJECT_NAME <span class="keyword">STREQUAL</span> PROJECT_NAME)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Optionally set things like CMAKE_CXX_STANDARD,</span></span><br><span class="line">  <span class="comment"># CMAKE_POSITION_INDEPENDENT_CODE here</span></span><br><span class="line"></span><br><span class="line">  <span class="comment"># Let's ensure -std=c++xx instead of -std=g++xx</span></span><br><span class="line">  <span class="keyword">set</span>(CMAKE_CXX_EXTENSIONS <span class="keyword">OFF</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Let's nicely support folders in IDE's</span></span><br><span class="line">  <span class="keyword">set_property</span>(GLOBAL PROPERTY USE_FOLDERS <span class="keyword">ON</span>)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Testing only available if this is the main app. Note this needs to be done</span></span><br><span class="line">  <span class="comment"># in the main CMakeLists since it calls enable_testing, which must be in the</span></span><br><span class="line">  <span class="comment"># main CMakeLists.</span></span><br><span class="line">  <span class="keyword">include</span>(CTest)</span><br><span class="line"></span><br><span class="line">  <span class="comment"># Docs only available if this is the main app</span></span><br><span class="line">  <span class="keyword">find_package</span>(Doxygen)</span><br><span class="line">  <span class="keyword">if</span>(Doxygen_FOUND)</span><br><span class="line">    <span class="keyword">add_subdirectory</span>(docs)</span><br><span class="line">  <span class="keyword">else</span>()</span><br><span class="line">    <span class="keyword">message</span>(STATUS <span class="string">"Doxygen not found, not building docs"</span>)</span><br><span class="line">  <span class="keyword">endif</span>()</span><br><span class="line"><span class="keyword">endif</span>()</span><br><span class="line"></span><br><span class="line"><span class="comment"># FetchContent added in CMake 3.11, downloads during the configure step</span></span><br><span class="line"><span class="comment"># FetchContent_MakeAvailable was not added until CMake 3.14</span></span><br><span class="line"><span class="keyword">include</span>(FetchContent)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Accumulator library This is header only, so could be replaced with git</span></span><br><span class="line"><span class="comment"># submodules or FetchContent</span></span><br><span class="line"><span class="keyword">find_package</span>(Boost REQUIRED)</span><br><span class="line"><span class="comment"># Adds Boost::boost / Boost::headers (newer FindBoost / BoostConfig 3.15 name)</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Formatting library, adds fmt::fmt</span></span><br><span class="line">FetchContent_Declare(</span><br><span class="line">  fmtlib</span><br><span class="line">  GIT_REPOSITORY https://github.com/fmtlib/fmt.git</span><br><span class="line">  GIT_TAG <span class="number">7.0</span>.<span class="number">2</span>)</span><br><span class="line">FetchContent_MakeAvailable(fmtlib)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The compiled library code is here</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(src)</span><br><span class="line"></span><br><span class="line"><span class="comment"># The executable code is here</span></span><br><span class="line"><span class="keyword">add_subdirectory</span>(apps)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Testing only available if this is the main app</span></span><br><span class="line"><span class="keyword">if</span>(BUILD_TESTING)</span><br><span class="line">  <span class="keyword">add_subdirectory</span>(tests)</span><br><span class="line"><span class="keyword">endif</span>()</span><br></pre></td></tr></table></figure><h3 id="src-CMakeLists-txt"><a href="#src-CMakeLists-txt" class="headerlink" title="/src/CMakeLists.txt"></a>/src/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Note that headers are optional, and do not affect add_library, but they will</span></span><br><span class="line"><span class="comment"># not show up in IDEs unless they are listed in add_library.</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Optionally glob, but only for CMake 3.12 or later: file(GLOB HEADER_LIST</span></span><br><span class="line"><span class="comment"># CONFIGURE_DEPENDS "$&#123;ModernCMakeExample_SOURCE_DIR&#125;/include/modern/*.hpp")</span></span><br><span class="line"><span class="keyword">set</span>(HEADER_LIST <span class="string">"$&#123;ModernCMakeExample_SOURCE_DIR&#125;/include/modern/lib.hpp"</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Make an automatic library - will be static or dynamic based on user setting</span></span><br><span class="line"><span class="keyword">add_library</span>(modern_library lib.cpp <span class="variable">$&#123;HEADER_LIST&#125;</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># We need this directory, and users of our library will need it too</span></span><br><span class="line"><span class="keyword">target_include_directories</span>(modern_library PUBLIC ../<span class="keyword">include</span>)</span><br><span class="line"></span><br><span class="line"><span class="comment"># This depends on (header only) boost</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(modern_library PRIVATE Boost::boost)</span><br><span class="line"></span><br><span class="line"><span class="comment"># All users of this library will need at least C++11</span></span><br><span class="line"><span class="keyword">target_compile_features</span>(modern_library PUBLIC cxx_std_11)</span><br><span class="line"></span><br><span class="line"><span class="comment"># IDEs should put the headers in a nice place</span></span><br><span class="line"><span class="keyword">source_group</span>(</span><br><span class="line">  TREE <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/include"</span></span><br><span class="line">  PREFIX <span class="string">"Header Files"</span></span><br><span class="line">  FILES <span class="variable">$&#123;HEADER_LIST&#125;</span>)</span><br></pre></td></tr></table></figure><h3 id="apps-CMakeLists-txt"><a href="#apps-CMakeLists-txt" class="headerlink" title="/apps/CMakeLists.txt"></a>/apps/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">add_executable</span>(app app.cpp)</span><br><span class="line"><span class="keyword">target_compile_features</span>(app PRIVATE cxx_std_17)</span><br><span class="line"></span><br><span class="line"><span class="keyword">target_link_libraries</span>(app PRIVATE modern_library fmt::fmt)</span><br></pre></td></tr></table></figure><h3 id="docs-CMakeLists-txt"><a href="#docs-CMakeLists-txt" class="headerlink" title="/docs/CMakeLists.txt"></a>/docs/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line"></span><br><span class="line"><span class="keyword">set</span>(DOXYGEN_EXTRACT_ALL YES)</span><br><span class="line"><span class="keyword">set</span>(DOXYGEN_BUILTIN_STL_SUPPORT YES)</span><br><span class="line"></span><br><span class="line">doxygen_add_docs(docs modern/lib.hpp <span class="string">"$&#123;CMAKE_CURRENT_SOURCE_DIR&#125;/mainpage.md"</span></span><br><span class="line">                 WORKING_DIRECTORY <span class="string">"$&#123;PROJECT_SOURCE_DIR&#125;/include"</span>)</span><br></pre></td></tr></table></figure><h3 id="tests-CMakeLists-txt"><a href="#tests-CMakeLists-txt" class="headerlink" title="/tests/CMakeLists.txt"></a>/tests/CMakeLists.txt</h3><figure class="highlight cmake"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># Testing library</span></span><br><span class="line">FetchContent_Declare(</span><br><span class="line">  catch2</span><br><span class="line">  GIT_REPOSITORY https://github.com/catchorg/Catch2.git</span><br><span class="line">  GIT_TAG v2.<span class="number">9.1</span>)</span><br><span class="line">FetchContent_MakeAvailable(catch2)</span><br><span class="line"><span class="comment"># Adds Catch2::Catch2</span></span><br><span class="line"></span><br><span class="line"><span class="comment"># Tests need to be added as executables first</span></span><br><span class="line"><span class="keyword">add_executable</span>(testlib testlib.cpp)</span><br><span class="line"></span><br><span class="line"><span class="comment"># I'm using C++17 in the test</span></span><br><span class="line"><span class="keyword">target_compile_features</span>(testlib PRIVATE cxx_std_17)</span><br><span class="line"></span><br><span class="line"><span class="comment"># Should be linked to the main library, as well as the Catch2 testing library</span></span><br><span class="line"><span class="keyword">target_link_libraries</span>(testlib PRIVATE modern_library Catch2::Catch2)</span><br><span class="line"></span><br><span class="line"><span class="comment"># If you register a test, then ctest and make test will run it. You can also run</span></span><br><span class="line"><span class="comment"># examples and check the output, as well.</span></span><br><span class="line"><span class="keyword">add_test</span>(NAME testlibtest <span class="keyword">COMMAND</span> testlib) <span class="comment"># Command can be a target</span></span><br></pre></td></tr></table></figure><p><a href="https://hsf-training.github.io/hsf-training-cmake-webpage/" target="_blank" rel="noopener">更多内容</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://hsf-training.github.io/hsf-training-cmake-webpage/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://hsf-training.github.io/hsf-tr
      
    
    </summary>
    
    
      <category term="cpp" scheme="http://guileen.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>C++构建工具比较</title>
    <link href="http://guileen.github.io/2021/01/08/cpp-build-systems/"/>
    <id>http://guileen.github.io/2021/01/08/cpp-build-systems/</id>
    <published>2021-01-08T08:26:33.000Z</published>
    <updated>2021-01-26T12:43:02.847Z</updated>
    
    <content type="html"><![CDATA[<p>近日，又对C++产生了兴趣。</p><p>C++可以说是我内心的阴影，之所以是内心的阴影，倒不是C++的语言本身使我无法理解，或不喜欢C++的某些特性。相反我一直对C++怀有很高的敬意。之所以说C++是我的阴影，主要是因为C++的工具链实在是太长太杂了。我不介意多花点时间来学习C++的语言特性，但是我实在不想浪费时间在工具的学习上面。我屡屡在尝试用C++做点东西的时候，都被环境的配置而感到厌烦而放弃。</p><p>最近，感觉 <a href="https://github.com/ocornut/imgui" target="_blank" rel="noopener">Dear ImGui</a> 这个项目有点意思，可以用来做点有趣的事情。于是我想再次挑战一下C++的项目，顺便看看社区是否有新的构建工具出来简化我的工作。</p><p>此前的工具 Make 缺点是项目越大越复杂，需要学习很多东西。还有 autotools , scons，也有一定学习成本。</p><p>最近出现的 google的Bazel、facebook的Buck 两个build system都可以用来构建C++。看了些对比主要缺点是对原有的生态兼容性比较差，往往需要源码导入。</p><p>可以选择的有CMake和Meson。目前看来是比较合适的选择，我决定有空尝试一下meson。</p><p>找到两个template项目 <a href="https://github.com/tiernemi/meson-sample-project" target="_blank" rel="noopener">meson-sample-project</a>  、 <a href="https://github.com/kigster/cmake-project-template" target="_blank" rel="noopener">cmake-project-template</a> 。拿这个模板直接改一改就可以创建一个c++项目了，这样我内心的恐惧感减少了很多。 </p><p>— 2021-01-26 更新 —<br>最终我选择了 <a href="https://github.com/kigster/cmake-project-template" target="_blank" rel="noopener">cmake-project-template</a> 来作为当前的C++<br>项目模板。今天github的发现页给我推荐了<a href="https://github.com/lefticus/cpp_starter_project" target="_blank" rel="noopener">cpp_starter_project</a> ，还能直接使用 github 的 use this template 功能，貌似不错，下次试试。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日，又对C++产生了兴趣。&lt;/p&gt;
&lt;p&gt;C++可以说是我内心的阴影，之所以是内心的阴影，倒不是C++的语言本身使我无法理解，或不喜欢C++的某些特性。相反我一直对C++怀有很高的敬意。之所以说C++是我的阴影，主要是因为C++的工具链实在是太长太杂了。我不介意多花点时间
      
    
    </summary>
    
    
      <category term="cpp" scheme="http://guileen.github.io/tags/cpp/"/>
    
  </entry>
  
  <entry>
    <title>2020回顾：天命</title>
    <link href="http://guileen.github.io/2020/12/31/2020-review/"/>
    <id>http://guileen.github.io/2020/12/31/2020-review/</id>
    <published>2020-12-31T08:11:33.000Z</published>
    <updated>2021-01-04T03:54:32.599Z</updated>
    
    <content type="html"><![CDATA[<p>书曰：恃德者昌，恃力者亡。天命无常，惟德是辅。当世界将要发生巨变，天道自然似乎并不会静观其变，而是会用寒流瘟疫、水旱蝗震来助力一把，莫非这就是传说中的天人感应？不仅如此，还会有荧惑星降世化为儿童，作童谣警示世人。去年的标题是『新思潮的前夜』，并写道『我们正站在世界巨变的前夜，这个巨变将以我们意想不到的方式出现』。2020实在太精彩了，我今天立一个flag，2021会比2020更精彩！</p><hr><p>个人记录：<br>春节后收尾项目，解散团队。<br>知乎写了几个回答。<br>求职，不知是否因为新冠影响颇不顺利，直至4月底找到工作。<br>19年底学习强化学习，玩无限版2048，做机器人。<br>10/10/6 作息，微信『听』书。<br>文革史相关数本（戚回忆，巨人的背影尤佳），西方哲学相关，政治思想（左右派的起源）、历史类（五万年中国简史）、人类学等。十五讲系列、大师小书、王德峰bilibili演讲。《临高启明》2刷并追更。《齐天大圣传》、《丰乳肥臀》、《日光流年》。《太上章》在听。<br>精读并评注：《论语批注》《道德经》《黄帝阴符经》，《周易》在读。</p><p>娃：番茄钟积累奖励玩游戏，效果很好。C++、刷leetcode，加入大佬群，读《算法导论》，参加信息学竞赛。读《古文观止》《论语批注》。学习高中数学，数论概率、微积分等。成果：CCF入门级第二轮一等，CCF提高级第二轮三等。理解了微积分，求导、求积分很熟练了。对古文兴趣依然不大。娃的进步比我大，简直是大踏步的前进。</p><p>娃和我说学校里流传一首儿歌：</p><blockquote><p>早安，打工人！打工人，打工魂，打工都是人上人！没有完不成的任务，只有勇敢的打工人！只要我们努力，我们的老板很快就能拥有更好的明天！我们要悄悄的打工，然后让老板惊艳所有人！感觉冷吗？冷就对了！电热毯是留给有钱的人的！感觉热吗？热就对了！空调是留给有车的人的！不要擦干眼泪，因为这样骑电动车很危险！早安！打工人！</p></blockquote><p>孔颖达在[春秋正义]亦指出:</p><blockquote><p><strong>儿童不解自为文辞,而群聚集会,成此嬉游遨戏之言,其言韵而有理,似若有神凭之者……故书传时有采之者。</strong></p></blockquote><p>我和儿子解释亢龙有悔，讲解物极必反的道理。儿子问我，马云是不是亢龙有悔？孩童可能只是无心之问，我竟不能回答。不久之后马云在陆家嘴金融论坛演讲，批评中国的银行是当铺思维，批评中国的金融监管制度。抖音评论区一片叫好之声，马云颇有为民请命，指点江山之意，可谓意气风发。然后马云就被二次约谈，蚂蚁金服上市暂缓，被要求整改。国家又出手反击垄断，阿里巴巴首当其冲。网络舆论又为之一转，虽然仍然可见为马云歌功颂德者，但总体上已呈现人人痛打落水狗之势。众人皆曰，马云去年说996是福报，今年之反垄断，马云之福报也。马云总欲成为青年人的精神导师，却终被人道破其欺世盗名的骗子本质，真是世事难料。不知马云是否有亢龙有悔之感，财富的损失倒是其次，如联想之柳传志，一度被人视为英雄，如今被人视若贼寇，其精神打击已非财富可以弥补。这类人一生信奉财富即正义，自视甚高，如今跌落神坛，无异于信仰崩塌，其心情可想而知。</p><h3 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h3><p>医疗系统若要改革无疑牵扯太多利益集团，而疫情的出现简直如同神助，这是坏事，但也是好事。不仅仅是医疗系统，各个行业都有了调整或监管的动力和契机。</p><p>除医疗外，文艺、教育、科研各个领域都会开始整改，清理改革开放以来的遗毒。年末琼瑶等作家编剧联名要求郭敬明、于正两人道歉，国家出台教育惩戒、未成年人刑责年龄下调，职称评定北京推出了新的办法。明年这些领域都将进一步改革。</p><p>劳工阶层的利益将受到更多的重视，打工人的童谣是劳工阶层觉醒的信号。</p><p>RCEP的意义：不在于贸易，而在于人民币国际化的扩张。</p><p>中欧投资协议的意义：中欧投资协议的意义不在于投资和协议，而在于『人民币与欧元联手绞杀美元』。中国在中东的存在感将会增加！中东将爆发新的代理人战争，以反击美国在中东的影响，扩大人民币石油的结算份额。这种冲突甚至可能演变为冲突方的直接战争。</p><p>法德等国的价值观与英美的价值观区别其实很大的。中欧价值观将会产生碰撞融合，而昂撒民族的价值观将遭到唾弃。一切依附于昂撒价值体系之上的势力和个人都将如沉船上的耗子一样，上窜下跳但无可奈何。澳大利亚将首当其冲，乌合麒麟的事件、中国对澳大利亚的经济制裁仅仅是一个开始。最终的目标是要改变澳大利亚的政治结构，甚至不惜一战。中东、台湾、澳大利亚，三地之中必有一个地方发生军事冲突，很有可能在两个地方发生冲突。</p><p>中国劳工阶层的日子会逐渐好过点，这是中欧合作的必然结果。因为欧洲人不能容忍中国继续使用『低人权』的『优势』与欧洲的高福利竞争，那样欧洲商品很难和中国制造相竞争。而改善中国人民的生活质量也是党内毛派的诉求，而提振内需也符合那些右派经济学家们的理论，于是改善福利肯定会逐步强化。对于996之类的工作制度，可能会有一场疾风骤雨式的治理。小企业主的日子会非常难过，因为他们长期都是靠低人权优势在竞争的。小企业主若不依赖低人权，则会被大企业碾压，则大量小企业破产，造成大量失业。于是反垄断以及逼迫大企业改善福利，就势在必行了。哪怕是为了解决生育问题，也不得不提高人民福利了。步骤应是反垄断开局，民营的大企业要改组要做好福利示范，国企则要减负（之前所谓国企缺乏竞争力是因为国企承担了较多社会福利的职能）。</p><p>过去的几十年，市场换技术，可以说是彻底失败的。市场丢了，技术没换来什么，倒是培养了一大批买办阶级。这些人美其名曰企业家，实则不过是窃贼和骗子，赚的都是底层人的血汗钱。于真正的创新是无甚贡献的。私有制、市场经济仍将长期存在，但下一阶段只有真正的创新者、开拓者才会名利双收。文艺界也需要更多刘慈欣、吴京这样的人，接下来的时代更需要一些执着的『傻子』，而不需要『精明』的商人。郭敬明、于正的道歉只是清理文艺界的开始，湖南台为代表的一套玩法，都要慢慢改变。</p><p>全球治理也需要加强，主要是设法打击国际避税，这个几年之内难有大的进展。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;书曰：恃德者昌，恃力者亡。天命无常，惟德是辅。当世界将要发生巨变，天道自然似乎并不会静观其变，而是会用寒流瘟疫、水旱蝗震来助力一把，莫非这就是传说中的天人感应？不仅如此，还会有荧惑星降世化为儿童，作童谣警示世人。去年的标题是『新思潮的前夜』，并写道『我们正站在世界巨变的前夜
      
    
    </summary>
    
      <category term="随笔" scheme="http://guileen.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>iptables从入门到放弃(设置NAT)</title>
    <link href="http://guileen.github.io/2020/12/22/iptables/"/>
    <id>http://guileen.github.io/2020/12/22/iptables/</id>
    <published>2020-12-22T13:01:52.000Z</published>
    <updated>2020-12-22T13:01:52.967Z</updated>
    
    <content type="html"><![CDATA[<p>iptables 的工作流程</p><p><img src="/img/iptables/iptables-1.jpeg" alt="iptables flow"></p><p>数据包在不同层间的时序</p><p><img src="/img/iptables/netpacketflow.svg" alt="packet flow"></p><p>基本语法：iptables [-t 表] [操作命令] [链][规则匹配器][-j 目标动作]</p><h3 id="表"><a href="#表" class="headerlink" title="表"></a>表</h3><table><thead><tr><th>表名</th><th>说明</th><th>支持的链</th></tr></thead><tbody><tr><td>filter</td><td>默认。用于过滤。</td><td>INPUT、FORWARD、OUTPUT</td></tr><tr><td>nat</td><td>用于地址转换</td><td>PREROUTING，INPUT，OUTPUT，POSTROUTING</td></tr><tr><td>mangle</td><td>用于修改包</td><td>PREROUTING，INPUT，FORWARD，OUTPUT，POSTROUTING</td></tr><tr><td>raw</td><td>主要用于让iptables不再跟踪包以提高性能</td><td>PREROUTING，OUTPUT</td></tr><tr><td>security</td><td>用于MAC地址相关的控制</td><td>INPUT、FORWARD、OUTPUT</td></tr></tbody></table><table><thead><tr><th>常用操作命令</th><th>说明</th></tr></thead><tbody><tr><td>-A</td><td>在指定链尾部添加规则</td></tr><tr><td>-D</td><td>删除匹配的规则</td></tr><tr><td>-R</td><td>替换匹配的规则</td></tr><tr><td>-I</td><td>在指定位置插入规则例：iptables -I INPUT 1 –dport 80 -j ACCEPT（将规则插入到filter表INPUT链中的第一位上）</td></tr><tr><td>-L/S</td><td>列出指定链或所有链的规则</td></tr><tr><td>-F</td><td>删除指定链或所有链的规则</td></tr><tr><td>-N</td><td>创建用户自定义链例：iptables -N allowed</td></tr><tr><td>-X</td><td>删除指定的用户自定义链</td></tr><tr><td>-P</td><td>为指定链设置默认规则策略，对自定义链不起作用例：iptables -P OUTPUT DROP</td></tr><tr><td>-Z</td><td>将指定链或所有链的计数器清零</td></tr><tr><td>-E</td><td>更改自定义链的名称例：iptables -E allowed disallowed</td></tr><tr><td>-n</td><td>ip地址和端口号以数字方式显示例：iptables -Ln</td></tr></tbody></table><table><thead><tr><th>常见规则匹配器</th><th>说明</th></tr></thead><tbody><tr><td>-p tcp|udp|icmp|all</td><td>匹配协议，all会匹配所有协议</td></tr><tr><td>-s addr[/mask]</td><td>匹配源地址</td></tr><tr><td>-d addr[/mask]</td><td>匹配目标地址</td></tr><tr><td>–sport port1[:port2]</td><td>匹配源端口(可指定连续的端口）</td></tr><tr><td>–dport port1[:port2]</td><td>匹配目的端口(可指定连续的端口）</td></tr><tr><td>-o interface</td><td>匹配出口网卡，只适用FORWARD、POSTROUTING、OUTPUT。例：iptables -A FORWARD -o eth0</td></tr><tr><td>-i interface</td><td>匹配入口网卡，只使用PREROUTING、INPUT、FORWARD。</td></tr><tr><td>–icmp-type</td><td>匹配icmp类型（使用iptables -p icmp -h可查看可用的ICMP类型）</td></tr><tr><td>–tcp-flags mask comp</td><td>匹配TCP标记，mask表示检查范围，comp表示匹配mask中的哪些标记。例：iptables -A FORWARD -p tcp –tcp-flags ALL SYN，ACK -j ACCEPT（表示匹配SYN和ACK标记的数据包）</td></tr></tbody></table><table><thead><tr><th>目标动作</th><th>说明</th></tr></thead><tbody><tr><td>ACCEPT</td><td>允许数据包通过</td></tr><tr><td>DROP</td><td>丢弃数据包</td></tr><tr><td>REJECT</td><td>丢弃数据包，并且将拒绝信息发送给发送方</td></tr><tr><td>SNAT</td><td>源地址转换（在nat表上）例：iptables -t nat -A POSTROUTING -d 192.168.0.102 -j SNAT –to 192.168.0.1</td></tr><tr><td>DNAT</td><td>目标地址转换（在nat表上）例：iptables -t nat -A PREROUTING -d 202.202.202.2 -j DNAT –to-destination 192.168.0.102</td></tr><tr><td>REDIRECT</td><td>目标端口转换（在nat表上）例：iptables -t nat -D PREROUTING -p tcp –dport 8080 -i eth2.2 -j REDIRECT –to 80</td></tr><tr><td>MARK</td><td>将数据包打上标记例：iptables -t mangle -A PREROUTING -s 192.168.1.3 -j MARK –set-mark 60</td></tr></tbody></table><p>注意要点：</p><p>  1、目标地址转换一般在PREROUTING链上操作</p><p>  2、源地址转换一般在POSTROUTING链上操作</p><p><strong>保存和恢复iptables规则</strong></p><p>  使用iptables-save可以保存到特定文件中</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables-save &gt; &#x2F;etc&#x2F;sysconfig&#x2F;iptables_save</span><br></pre></td></tr></table></figure><p>  使用iptables-restore可以恢复规则</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">iptables-restore&lt; &#x2F;etc&#x2F;sysconfig&#x2F;iptables_save</span><br></pre></td></tr></table></figure><h3 id="配置VPN-NAT"><a href="#配置VPN-NAT" class="headerlink" title="配置VPN NAT"></a>配置VPN NAT</h3><p><strong>/etc/sysctl.conf</strong></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">net.ipv4.ip_forward&#x3D;1</span><br></pre></td></tr></table></figure><p>重启网络</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">$ sudo sysctl -p</span><br></pre></td></tr></table></figure><h3 id="同端口-端口转发"><a href="#同端口-端口转发" class="headerlink" title="同端口 端口转发"></a>同端口 端口转发</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -p tcp --dport [本地端口] -j DNAT --to-destination [目标IP:目标端口]</span><br><span class="line">iptables -t nat -A PREROUTING -p udp --dport [本地端口] -j DNAT --to-destination [目标IP:目标端口]</span><br><span class="line">iptables -t nat -A POSTROUTING -p tcp -d [目标IP] --dport [目标端口] -j SNAT --to-source [本地服务器主网卡绑定IP]</span><br><span class="line">iptables -t nat -A POSTROUTING -p udp -d [目标IP] --dport [目标端口] -j SNAT --to-source [本地服务器主网卡绑定IP]</span><br></pre></td></tr></table></figure><hr><p>以下示例，假设你的国外服务器（被中转服务器）是<code>1.1.1.1</code>，你的SS端口是<code>10000</code>，而你这台正在操作的VPS的主网卡绑定IP（中转服务器）是<code>2.2.2.2</code>。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -p tcp -m tcp --dport 10000 -j DNAT --to-destination 1.1.1.1:10000</span><br><span class="line">iptables -t nat -A PREROUTING -p udp -m udp --dport 10000 -j DNAT --to-destination 1.1.1.1:10000</span><br><span class="line">iptables -t nat -A POSTROUTING -d 1.1.1.1 -p tcp -m tcp --dport 10000 -j SNAT --to-source 2.2.2.2</span><br><span class="line">iptables -t nat -A POSTROUTING -d 1.1.1.1 -p udp -m udp --dport 10000 -j SNAT --to-source 2.2.2.2</span><br></pre></td></tr></table></figure><h3 id="不同端口-端口转发"><a href="#不同端口-端口转发" class="headerlink" title="不同端口 端口转发"></a><strong>不同端口</strong> 端口转发</h3><p>将<strong>本地服务器(中转服务器<code>2.2.2.2</code>)</strong>的<code>10000~20000</code>端口转发至<strong>目标IP(被中转服务器)</strong>为<code>1.1.1.1</code>的<code>30000~40000</code>端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">iptables -t nat -A PREROUTING -p tcp -m tcp --dport 10000:20000 -j DNAT --to-destination 1.1.1.1:30000-40000</span><br><span class="line">iptables -t nat -A PREROUTING -p udp -m udp --dport 10000:20000 -j DNAT --to-destination 1.1.1.1:30000-40000</span><br><span class="line">iptables -t nat -A POSTROUTING -d 1.1.1.1 -p tcp -m tcp --dport 30000:40000 -j SNAT --to-source 2.2.2.2</span><br><span class="line">iptables -t nat -A POSTROUTING -d 1.1.1.1 -p udp -m udp --dport 30000:40000 -j SNAT --to-source 2.2.2.2</span><br></pre></td></tr></table></figure><h2 id="以上尝试以失败告终，最终采用了brook"><a href="#以上尝试以失败告终，最终采用了brook" class="headerlink" title="以上尝试以失败告终，最终采用了brook"></a>以上尝试以失败告终，最终采用了brook</h2><p><a href="https://github.com/txthinking/brook" target="_blank" rel="noopener">brook</a> 非常强大，不仅能作中继，还可以在手机上安装。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">nohup brook relay -f :4500 -t 1.2.3.4:4500 &amp;</span><br><span class="line">nohup brook relay -f :500 -t 1.2.3.4:500 &amp;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;iptables 的工作流程&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/iptables/iptables-1.jpeg&quot; alt=&quot;iptables flow&quot;&gt;&lt;/p&gt;
&lt;p&gt;数据包在不同层间的时序&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/iptables/net
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>脚本控制Mac通知栏</title>
    <link href="http://guileen.github.io/2020/12/22/bitbar/"/>
    <id>http://guileen.github.io/2020/12/22/bitbar/</id>
    <published>2020-12-22T10:02:06.000Z</published>
    <updated>2020-12-22T10:13:24.151Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://github.com/matryer/bitbar" target="_blank" rel="noopener">bitbar</a> 可以让你使用任何脚本来控制Mac的菜单栏。你只需要写一个向控制台输出的脚本即可。使你在菜单栏可以轻松的切换ssh tunnel，显示BTC价格等。只需要将脚本放置在 bitbar plugin 目录中，文件名中可指定刷新频率。如：<a href="https://github.com/matryer/bitbar-plugins/blob/master/Network/ssh-tunnel.1s.sh" target="_blank" rel="noopener">ssh-tunnel.1s.sh</a></p><p>编辑 ssh config 包含 DynamicForward 的配置，将自动出现在菜单中。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">Host gz</span><br><span class="line">        HostName your_jump_ip</span><br><span class="line">        User ubuntu</span><br><span class="line"></span><br><span class="line">Host gzhk</span><br><span class="line">        HostName your_final_ip</span><br><span class="line">        User ubuntu</span><br><span class="line">        Port 22</span><br><span class="line">        Compression yes</span><br><span class="line">        DynamicForward 10088</span><br><span class="line">        ProxyJump gz</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://github.com/matryer/bitbar&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;bitbar&lt;/a&gt; 可以让你使用任何脚本来控制Mac的菜单栏。你只需要写一个向控制台输出的脚本即可。使你在菜单栏可以轻松的
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>替代图</title>
    <link href="http://guileen.github.io/2020/12/18/image-replacer/"/>
    <id>http://guileen.github.io/2020/12/18/image-replacer/</id>
    <published>2020-12-18T07:39:30.000Z</published>
    <updated>2020-12-18T07:40:06.766Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://picsum.photos/" target="_blank" rel="noopener">https://picsum.photos/</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://picsum.photos/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://picsum.photos/&lt;/a&gt;&lt;/p&gt;

      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>Ubuntu 16 安装 IKEV2 VPN SERVER</title>
    <link href="http://guileen.github.io/2020/12/18/setup-ikev2-vpn-server-on-ubuntu-16-04/"/>
    <id>http://guileen.github.io/2020/12/18/setup-ikev2-vpn-server-on-ubuntu-16-04/</id>
    <published>2020-12-18T02:50:16.000Z</published>
    <updated>2020-12-22T13:01:28.720Z</updated>
    
    <content type="html"><![CDATA[<p><a href="https://www.digitalocean.com/community/tutorials/how-to-set-up-an-ikev2-vpn-server-with-strongswan-on-ubuntu-16-04" target="_blank" rel="noopener">https://www.digitalocean.com/community/tutorials/how-to-set-up-an-ikev2-vpn-server-with-strongswan-on-ubuntu-16-04</a></p><h2 id="命令"><a href="#命令" class="headerlink" title="命令"></a>命令</h2><p>注意替换 server_name_or_ip 为服务器地址</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br><span class="line">29</span><br><span class="line">30</span><br><span class="line">31</span><br><span class="line">32</span><br><span class="line">33</span><br><span class="line">34</span><br></pre></td><td class="code"><pre><span class="line">sudo apt-get install strongswan strongswan-plugin-eap-mschapv2 moreutils iptables-persistent</span><br><span class="line"></span><br><span class="line">mkdir vpn-certs</span><br><span class="line">cd vpn-certs</span><br><span class="line"></span><br><span class="line">ipsec pki --gen --type rsa --size 4096 --outform pem &gt; server-root-key.pem</span><br><span class="line">chmod 600 server-root-key.pem</span><br><span class="line">ipsec pki --self --ca --lifetime 3650 \</span><br><span class="line">--in server-root-key.pem \</span><br><span class="line">--type rsa --dn &quot;C&#x3D;US, O&#x3D;VPN Server, CN&#x3D;VPN Server Root CA&quot; \</span><br><span class="line">--outform pem &gt; server-root-ca.pem</span><br><span class="line"></span><br><span class="line">ipsec pki --gen --type rsa --size 4096 --outform pem &gt; vpn-server-key.pem</span><br><span class="line"></span><br><span class="line">ipsec pki --pub --in vpn-server-key.pem \</span><br><span class="line">--type rsa | ipsec pki --issue --lifetime 1825 \</span><br><span class="line">--cacert server-root-ca.pem \</span><br><span class="line">--cakey server-root-key.pem \</span><br><span class="line">--dn &quot;C&#x3D;US, O&#x3D;VPN Server, CN&#x3D;server_name_or_ip&quot; \</span><br><span class="line">--san server_name_or_ip \</span><br><span class="line">--flag serverAuth --flag ikeIntermediate \</span><br><span class="line">--outform pem &gt; vpn-server-cert.pem</span><br><span class="line"></span><br><span class="line">sudo cp .&#x2F;vpn-server-cert.pem &#x2F;etc&#x2F;ipsec.d&#x2F;certs&#x2F;vpn-server-cert.pem</span><br><span class="line">sudo cp .&#x2F;vpn-server-key.pem &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;vpn-server-key.pem</span><br><span class="line"></span><br><span class="line">sudo chown root &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;vpn-server-key.pem</span><br><span class="line">sudo chgrp root &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;vpn-server-key.pem</span><br><span class="line">sudo chmod 600 &#x2F;etc&#x2F;ipsec.d&#x2F;private&#x2F;vpn-server-key.pem</span><br><span class="line"></span><br><span class="line">sudo cp &#x2F;etc&#x2F;ipsec.conf &#x2F;etc&#x2F;ipsec.conf.original</span><br><span class="line">echo &#39;&#39; | sudo tee &#x2F;etc&#x2F;ipsec.conf</span><br><span class="line"></span><br><span class="line">sudo vim &#x2F;etc&#x2F;ipsec.conf</span><br></pre></td></tr></table></figure><h3 id="etc-ipsec-conf"><a href="#etc-ipsec-conf" class="headerlink" title="/etc/ipsec.conf"></a>/etc/ipsec.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br><span class="line">26</span><br><span class="line">27</span><br><span class="line">28</span><br></pre></td><td class="code"><pre><span class="line">config setup</span><br><span class="line">    charondebug&#x3D;&quot;ike 1, knl 1, cfg 0&quot;</span><br><span class="line">    uniqueids&#x3D;no</span><br><span class="line"></span><br><span class="line">conn ikev2-vpn</span><br><span class="line">    auto&#x3D;add</span><br><span class="line">    compress&#x3D;no</span><br><span class="line">    type&#x3D;tunnel</span><br><span class="line">    keyexchange&#x3D;ikev2</span><br><span class="line">    fragmentation&#x3D;yes</span><br><span class="line">    forceencaps&#x3D;yes</span><br><span class="line">    ike&#x3D;aes256-sha1-modp1024,3des-sha1-modp1024!</span><br><span class="line">    esp&#x3D;aes256-sha1,3des-sha1!</span><br><span class="line">    dpdaction&#x3D;clear</span><br><span class="line">    dpddelay&#x3D;300s</span><br><span class="line">    rekey&#x3D;no</span><br><span class="line">    left&#x3D;%any</span><br><span class="line">    leftid&#x3D;@server_name_or_ip</span><br><span class="line">    leftcert&#x3D;&#x2F;etc&#x2F;ipsec.d&#x2F;certs&#x2F;vpn-server-cert.pem</span><br><span class="line">    leftsendcert&#x3D;always</span><br><span class="line">    leftsubnet&#x3D;0.0.0.0&#x2F;0</span><br><span class="line">    right&#x3D;%any</span><br><span class="line">    rightid&#x3D;%any</span><br><span class="line">    rightauth&#x3D;eap-mschapv2</span><br><span class="line">    rightdns&#x3D;8.8.8.8,8.8.4.4</span><br><span class="line">    rightsourceip&#x3D;10.10.10.0&#x2F;24</span><br><span class="line">    rightsendcert&#x3D;never</span><br><span class="line">    eap_identity&#x3D;%identity</span><br></pre></td></tr></table></figure><h2 id="防火墙设置"><a href="#防火墙设置" class="headerlink" title="防火墙设置"></a>防火墙设置</h2><p>可选</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">sudo ufw disable</span><br><span class="line">iptables -P INPUT ACCEPT</span><br><span class="line">iptables -P FORWARD ACCEPT</span><br><span class="line">iptables -F</span><br><span class="line">iptables -Z</span><br></pre></td></tr></table></figure><p>默认保持</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -A INPUT -m state --state ESTABLISHED,RELATED -j ACCEPT</span><br><span class="line">sudo iptables -A INPUT -p tcp --dport 22 -j ACCEPT</span><br><span class="line">sudo iptables -A INPUT -i lo -j ACCEPT</span><br></pre></td></tr></table></figure><p>VPN端口</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -A INPUT -p udp --dport  500 -j ACCEPT</span><br><span class="line">sudo iptables -A INPUT -p udp --dport 4500 -j ACCEPT</span><br><span class="line">sudo iptables -A FORWARD --match policy --pol ipsec --dir in  --proto esp -s 10.10.10.10&#x2F;24 -j ACCEPT</span><br><span class="line">sudo iptables -A FORWARD --match policy --pol ipsec --dir out --proto esp -d 10.10.10.10&#x2F;24 -j ACCEPT</span><br><span class="line">sudo iptables -t nat -A POSTROUTING -s 10.10.10.10&#x2F;24 -o eth0 -m policy --pol ipsec --dir out -j ACCEPT</span><br><span class="line">sudo iptables -t nat -A POSTROUTING -s 10.10.10.10&#x2F;24 -o eth0 -j MASQUERADE</span><br><span class="line">sudo iptables -t mangle -A FORWARD --match policy --pol ipsec --dir in -s 10.10.10.10&#x2F;24 -o eth0 -p tcp -m tcp --tcp-flags SYN,RST SYN -m tcpmss --mss 1361:1536 -j TCPMSS --set-mss 1360</span><br></pre></td></tr></table></figure><p>可选，禁用其他</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo iptables -A INPUT -j DROP</span><br><span class="line">sudo iptables -A FORWARD -j DROP</span><br></pre></td></tr></table></figure><p>重启防火墙</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">sudo netfilter-persistent save</span><br><span class="line">sudo netfilter-persistent reload</span><br></pre></td></tr></table></figure><h2 id="网络配置"><a href="#网络配置" class="headerlink" title="网络配置"></a>网络配置</h2><p>sudo vim /etc/sysctl.conf</p><h3 id="etc-sysctl-conf"><a href="#etc-sysctl-conf" class="headerlink" title="/etc/sysctl.conf"></a>/etc/sysctl.conf</h3><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line">. . .</span><br><span class="line"></span><br><span class="line"># Uncomment the next line to enable packet forwarding for IPv4</span><br><span class="line">net.ipv4.ip_forward&#x3D;1</span><br><span class="line"></span><br><span class="line">. . .</span><br><span class="line"></span><br><span class="line"># Do not accept ICMP redirects (prevent MITM attacks)</span><br><span class="line">net.ipv4.conf.all.accept_redirects &#x3D; 0</span><br><span class="line"># Do not send ICMP redirects (we are not a router)</span><br><span class="line">net.ipv4.conf.all.send_redirects &#x3D; 0</span><br><span class="line"></span><br><span class="line">. . .</span><br><span class="line"></span><br><span class="line">net.ipv4.ip_no_pmtu_disc &#x3D; 1</span><br></pre></td></tr></table></figure><h2 id="重启"><a href="#重启" class="headerlink" title="重启"></a>重启</h2><p><code>sudo reboot</code></p><h2 id="客户端测试"><a href="#客户端测试" class="headerlink" title="客户端测试"></a>客户端测试</h2><p><code>scp hk:~/vpn-certs/server-root-ca.pem ./</code></p><p>安装并信任证书，测试</p><h2 id="Relay-server"><a href="#Relay-server" class="headerlink" title="Relay server"></a>Relay server</h2><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">brook relay -f :500 -t 1.2.3.4:500 &amp;</span><br><span class="line">brook relay -f :4500 -t 1.2.3.4:4500 &amp;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;&lt;a href=&quot;https://www.digitalocean.com/community/tutorials/how-to-set-up-an-ikev2-vpn-server-with-strongswan-on-ubuntu-16-04&quot; target=&quot;_bla
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>在命令行解析HTML</title>
    <link href="http://guileen.github.io/2020/12/14/parse-html-in-command-line/"/>
    <id>http://guileen.github.io/2020/12/14/parse-html-in-command-line/</id>
    <published>2020-12-14T09:02:32.000Z</published>
    <updated>2020-12-14T09:44:16.415Z</updated>
    
    <content type="html"><![CDATA[<p>最近要做音乐app，但没有素材。 一个音乐爬虫</p><p><a href="https://github.com/BeanWei/MusicSpider" target="_blank" rel="noopener">https://github.com/BeanWei/MusicSpider</a></p><p>网页版的内容比较容易解析，为了快速了解数据结构，需要一个好工具。于是找到了pup</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br></pre></td><td class="code"><pre><span class="line">go get github.com&#x2F;ericchiang&#x2F;pup</span><br><span class="line"></span><br><span class="line">curl -s https:&#x2F;&#x2F;news.ycombinator.com&#x2F; | pup &#39;table table tr:nth-last-of-type(n+2) td.title a json&#123;&#125;&#39;</span><br></pre></td></tr></table></figure><p>以上解析可以用于命令行的分析，但若要生成更精简的数据结构，可以用下面的结构。</p><p>const { parse } = require(‘node-html-parser’)<br>const fs = require(‘fs’)</p><p>var dom = parse(fs.readFileSync(“./test_list.xml”, “utf8”))<br>var res = dom.querySelectorAll(‘li’).map(el=&gt; {<br>     return {<br>         src: el.querySelector(“img”).getAttribute(“src”),<br>         title: el.querySelector(“a.tit”).innerText,<br>         href: el.querySelector(“a.tit”).getAttribute(“href”),<br>         cnt: el.querySelector(“span.nb”).innerText,<br>     }<br> })<br>console.log(JSON.stringify(res,null,’  ‘))</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;最近要做音乐app，但没有素材。 一个音乐爬虫&lt;/p&gt;
&lt;p&gt;&lt;a href=&quot;https://github.com/BeanWei/MusicSpider&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;https://github.com/BeanWei
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>太初有道</title>
    <link href="http://guileen.github.io/2020/10/29/tao-at-the-very-begining/"/>
    <id>http://guileen.github.io/2020/10/29/tao-at-the-very-begining/</id>
    <published>2020-10-29T07:30:57.000Z</published>
    <updated>2020-10-29T07:30:57.402Z</updated>
    
    <content type="html"><![CDATA[<p>人类在数万年前进入了一个新的状态，太初之人，完满自足，而后散布天下。</p><p>庄子天下篇说，上古之人真是完备啊，可惜后世的人很不幸，他们将看到道术在天下分裂。</p><p>古希腊哲学、古印度思想、中国古代思想，有一个更统一的源头。那就是上古太初之人。</p><p>我们今天的思想是东西方思想的结合，依然不够完满，缺失了重要的部分。</p><p>美洲文化受到了破坏。非洲有一部分上古文化留存，苗瑶族也有一些上古之道留存。但都不完整。</p><p>我们的目标应该是恢复一个人类上古之道，而不是自称独立发展的，自绝于世界。</p><p>也不是盲目的学习他人，放弃自己，因为他人也不过是缺憾的存在。</p><p>人类无论从基因上，还是从思想上，都是残缺的。</p><p>我们需要追求圆满自足，这是人所遗忘的功能。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;人类在数万年前进入了一个新的状态，太初之人，完满自足，而后散布天下。&lt;/p&gt;
&lt;p&gt;庄子天下篇说，上古之人真是完备啊，可惜后世的人很不幸，他们将看到道术在天下分裂。&lt;/p&gt;
&lt;p&gt;古希腊哲学、古印度思想、中国古代思想，有一个更统一的源头。那就是上古太初之人。&lt;/p&gt;
&lt;p&gt;
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>ip转换为地理位置</title>
    <link href="http://guileen.github.io/2020/10/20/ip-to-location/"/>
    <id>http://guileen.github.io/2020/10/20/ip-to-location/</id>
    <published>2020-10-20T08:30:02.000Z</published>
    <updated>2020-10-20T08:30:02.521Z</updated>
    
    <content type="html"><![CDATA[<ol><li>安装windows xp虚拟机，安装guest addition</li><li>添加share folder</li><li>安装纯真数据库: <a href="http://www.cz88.net/fox/ipdat.shtml" target="_blank" rel="noopener">http://www.cz88.net/fox/ipdat.shtml</a></li><li>解压到share folder</li><li>mac上使用 iconv -f GBK -t UTF-8 ipdata.txt &gt; ipdata-utf8.txt</li><li>上传到github</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;ol&gt;
&lt;li&gt;安装windows xp虚拟机，安装guest addition&lt;/li&gt;
&lt;li&gt;添加share folder&lt;/li&gt;
&lt;li&gt;安装纯真数据库: &lt;a href=&quot;http://www.cz88.net/fox/ipdat.shtml&quot; target=&quot;_b
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>强化学习在长时间运行系统中的限制</title>
    <link href="http://guileen.github.io/2020/04/05/limitation-of-rl-in-long-term/"/>
    <id>http://guileen.github.io/2020/04/05/limitation-of-rl-in-long-term/</id>
    <published>2020-04-04T16:43:34.000Z</published>
    <updated>2020-08-31T07:01:49.034Z</updated>
    
    <content type="html"><![CDATA[<p>近日尝试使用强化学习解决8x8大小的2048游戏。游戏环境是一个8x8的方格，环境每回合随机生成2个方块，方块的值为2或4，AI的决策空间是上、下、左、右四个操作，所有方块都将向所指示的方向堆叠，如果两个相邻的值相同方块在堆叠方向挤压，则这两个方块合并为1个方块，值是两个方块值的和。如果任何方向都无法移动方块，则游戏结束。游戏的目标是尽可能久的玩下去，合并出最大的方块。</p><p>最初我尝试用PG来训练这个看似简单的游戏。每一步，都视为1点奖励，如果失败则给予-1000惩罚。算法很快习得了一个偷懒的方法，每一步都进行无效的移动，以此来苟延残喘。于是将无效的移动操作，视为重大的失误，也同样给予-1000的惩罚。算法很快学会了在一个局面下的有效移动操作。但这个游戏，哪怕只是随机的移动也能够取得一个普通的结果，如果要突破极限，则需要使用一些特殊的策略，我期待算法是否能在训练中学会这些策略。</p><p><img src="/img/2048/pg.gif" alt="初次训练，类似随机运动"></p><p>对于这个游戏，达到2048，需要大约500次移动，达到4096，则需要1000，达到1M，也就大约需要20多万次移动，达到4M，则需要上百万次移动。每到达一个新的难度，面临的局面都不同，之前所习得的经验就不一定继续有效了。2048游戏是一个比围棋要简单很多的游戏，围棋拥有更多的选点，2048只有4个操作选择。他们的主要区别在于围棋一般在100多手内结束，而2048的游戏时间则近乎无限长。理想的游戏结果如下图所示：</p><p><img src="/img/2048/perfect.png" alt=""></p><p>这一游戏是存在理想玩法的，经过很多局的游戏，我已总结出一些经验。但是这些经验是感性的，很难使用逻辑规则表达出来，很多时候是凭直觉的。我手段操作达到了512K的结果，虽然我依然可以挑战更高的游戏记录，但显然我不能将如此多的时间浪费在滑动手指上。这也是我要编程解决这个问题的初衷，但是强化学习算法，只能在一次次的失败中得到教训，可是这个游戏的特点是，训练的越好，游戏时间越长，获得失败经验的成本就越大。所以无论该算法在理论上是多么的正确，但在实际操作过程中已经变得不可行了。</p><p>DQN、PG等强化学习算法的基本过程是根据系统给予的奖励，努力最大化收益。但是对于一个没有明确获胜终点的系统，如果验证训练结果的有效性却是一个非常大的问题。由于强化学习本质上是通过过往的经验来调整自己的策略的，如果有明显的获胜路径，则算法可以有充分的胜利经验可供借鉴。但如果目标是永远安全的运行下去，没有获胜的路径，只有失败的惩罚，那么算法只能在有限的教训中得到学习。假设我们正在训练一个自动飞行系统，获取每一个经验教训的成本都非常巨大，强化学习在这一方面的应用，必须要搭配一些人类的理性评估作为辅助，但是将人类的意识转化为可以实施的程序逻辑又是非常复杂的事情。</p><p>如果我们训练的是一个自动驾驶系统呢？在未来无人驾驶会应用的越来越多。无人驾驶的安全性会很快超越人类，随即人们期望可以进一步提升驾驶的平均速度或其他一些智能驾驶的指标。因为无人驾驶的安全性已经超越了人类，所以无法再依赖于人类的驾驶经验给予其帮助，只能依赖于自身驾驶中的经验（尤其是事故）作为训练依据。那么这时这个系统还可能是安全的吗？</p><p>在未来的相关强化学习领域，一个好的环境模拟系统、事故全息信息的采集和共享系统，才是提升人工智能的关键，而不是算法。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;近日尝试使用强化学习解决8x8大小的2048游戏。游戏环境是一个8x8的方格，环境每回合随机生成2个方块，方块的值为2或4，AI的决策空间是上、下、左、右四个操作，所有方块都将向所指示的方向堆叠，如果两个相邻的值相同方块在堆叠方向挤压，则这两个方块合并为1个方块，值是两个方
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（六）：策略梯度实例</title>
    <link href="http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/"/>
    <id>http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/</id>
    <published>2020-01-03T06:40:51.000Z</published>
    <updated>2020-08-31T07:01:49.034Z</updated>
    
    <content type="html"><![CDATA[<p>和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网络在各个动作的Q值是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(PolicyNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.1</span>))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出层需要使用softmax</span></span><br><span class="line">        <span class="keyword">return</span> F.softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>不要忘了输出层的SoftMax。</p><h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><p>相对于DQN，我们也不需要额外的目标网络和参数复制操作，只需要一个策略网络即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">GAMMA = <span class="number">0.99</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">policy_net = PolicyNet(input_size, HIDDEN_SIZE, n_actions)</span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="选择动作"><a href="#选择动作" class="headerlink" title="选择动作"></a>选择动作</h2><p>在选择动作时，我们不再需要特地设置探索概率，因为输出结果就是各个动作的概率分布。我们使用<code>torch.distributions.categorical.Categorical</code> 来进行取样。在每次选择动作时，我们同时记录对应的概率，以便后续使用。这个概率就是 `ln pi_theta(S_t,A_t)`</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">log_probs = []</span><br><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state),<span class="number">0</span>)</span><br><span class="line">    probs = policy_net(x)</span><br><span class="line">    c = Categorical(probs)</span><br><span class="line">    action = c.sample()</span><br><span class="line">    <span class="comment"># log action probs to plt</span></span><br><span class="line">    prob = c.log_prob(action)</span><br><span class="line">    log_probs.append(prob)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h2><p>为了更新参数，我们首先需要计算`v_t`，这在后续参数迭代中需要用到。</p><ul><li>` v_t = r_(t+1) + gamma * v_(t+1) `</li></ul><p>在模拟执行的时候，我们记录了每一步的reward，我们需要计算每一步的`v_t`，其顺序与执行顺序一致。根据公式我们需要倒序的计算`v_t`，然后将计算好的结果倒序排列，就形成了`v_1,v_2…v_t`的序列。最后我们需要将数据标准化。(TODO: 这里可能存在一个序列对应的问题，其中每一个状态的累计收益，是后续状态收益之和，不包含本轮收益)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">values = []</span><br><span class="line">v = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> reward <span class="keyword">in</span> reversed(rewards):</span><br><span class="line">    v = v * GAMMA + reward</span><br><span class="line">    values.insert(<span class="number">0</span>, v)</span><br><span class="line">mean = np.mean(values)</span><br><span class="line">std = np.std(values)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">    values[i] = (values[i] - mean) / std</span><br></pre></td></tr></table></figure><p>接下来我们需要更新参数，参数更新的公式为：</p><ul><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul><p>我们将其转换为损失函数形式:</p><ul><li>` L(theta) = - v_t ln pi_theta(A_t|S_t) `</li></ul><p>这个损失函数的形式可以帮助我们更好的理解策略梯度的原理。如果一个动作价值为负值，但是其选择概率为正，则损失较大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.random.choice(size, n):</span><br><span class="line">    loss.append(- values[i] * log_probs[i])</span><br><span class="line">loss = torch.cat(loss).sum()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">    param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>训练循环需要在一局结束之后进行。并清除rewards、log_probs缓存。对于cartpole-v1环境，要注意他的每一步奖励都是1，很显然在最后一步代表着游戏失败，我们需要施加一定的惩罚，我们将最后一步的奖励设为-100。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">5000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state)</span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward, done,_ = env.step(action.item())</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt;= <span class="number">2500</span>:</span><br><span class="line">            optimize_model()</span><br><span class="line">            print(<span class="string">'EP'</span>, i_episode)</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            rewards = []</span><br><span class="line">            log_probs = []</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/files/cart-pg.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（五）：策略梯度Policy Gradient</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/</id>
    <published>2019-12-30T06:37:07.000Z</published>
    <updated>2020-08-31T07:01:49.033Z</updated>
    
    <content type="html"><![CDATA[<p>DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问题，而Q值的目的也是为了选择动作。我们可以将增加学习的问题简化为选择动作的问题。那么我们可否使用深度学习直接做出动作选择呢？显然，我们可以定义一个网络`pi_theta`，其中输入为状态`s`，输出为每个动作`a`的概率。</p><p><img src="/img/rl-5/1.png" alt="策略梯度"></p><p>因为这个网络与策略函数的定义一样，所以被称为策略网络。`pi_theta(a|s)`，表示在`s`状态下选择动作`a`的概率。只要这个网络能够收敛，我们就可以直接得到最佳策略。这个网络的奖励函数也就是最终游戏的总奖励。</p><p>`J(theta) = sum_(s in S)d^pi(s)V^pi(s) = sum_(s in S)d^pi(s)sum_(a in A)pi_theta(a|s)Q^pi(s, a)`</p><p>`d^pi(s)`指状态`s`在马尔科夫链上的稳定分布，`d^pi(s) = lim_(t-&gt;oo)P(s_t=s|s_0,pi_theta)`。</p><p>但是这个表达式看上去是不可能计算的，因为状态的分布和Q值都是随着策略的更新而不断变化的。但是我们并不需要计算`J(theta)`，在梯度下降法中我们只需要计算梯度`grad_(theta)J(theta)`即可</p><p>`grad_(theta)V^pi(s)`<br>`= grad_(theta)(sum_(a in A)pi_theta(a|s)Q^pi(s, a))`<br>根据导数乘法规则<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_thetaQ^pi(s, a))`<br>展开`Q^pi(s,a)`为各各种可能的下一状态奖励之和<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_(theta)sum_(s’,r)P(s’,r|s,a)(r+V^pi(s’)))`<br>而其中状态转移函数`P(s’,r|s,a)`、奖励`r`由环境决定，与`grad_theta`无关，所以<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’,r)P(s’,r|s,a)grad_(theta)V^pi(s’))`<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>现在我们有了一个形式非常好的递归表达式：<br>`grad_(theta)V^pi(s) = sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>设 `rho^pi(s-&gt;x, k)` 表示在策略`pi^theta`下，`k`步以后状态`s`转移到状态`x`的概率。有：</p><ul><li>`rho^pi(s-&gt;s, k=0)=1`</li><li>`rho^pi(s-&gt;s’, k=1)=sum_(a)pi_(theta)(a|s)P(s’|s,a)`</li><li>`rho^pi(s-&gt;x, k+1) = sum_(s’)rho^pi(s-&gt;s’, k)rho^pi(s’-&gt;x, 1)`</li></ul><p>为了简化计算，令 `phi(s)=sum_(a in A)grad_(theta)pi_theta(a|s)Q^pi(s,a)`</p><p>`grad_(theta)V^pi(s)`<br>`= phi(s) + sum_(a in A)pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)sum_(a in A)pi_(theta)(a|s)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)(phi(s’) + sum_(s’’)rho^pi(s’-&gt;s’’,1)grad_(theta)V^pi(s’’)) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)grad_(theta)V^pi(s’’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)phi(s’’) + sum_(s’’’)rho^pi(s-&gt;s’’’,3)grad_(theta)V^pi(s’’’) `<br>`= …`<br>`= sum_(x in S)sum_(k=0)^(oo)rho^pi(s-&gt;x, k)phi(x)`</p><p>令 `eta(s)=sum_(k=0)^(oo)rho^pi(s_0-&gt;s, k)`</p><p>`grad_(theta)J(theta)=grad_(theta)V^pi(s_0)`<br>`= sum_(s)sum_(k=0)^(oo)rho^pi(s_0-&gt;s,k)phi(s)`<br>`= sum_(s)eta(s)phi(s)`<br>`= (sum_(s)eta(s))sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `sum_(s)eta(s)` 属于常数，对于求梯度而言常数可以忽略。<br>`prop sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `eta(s)/(sum_(s)eta(s))`表示`s`的稳定分布<br>`= sum_(s)d^pi(s)sum_a grad_(theta)pi_(theta)(a|s)Q^pi(s,a)`<br>`= sum_(s)d^pi(s)sum_a pi_(theta)(a|s)Q^pi(s,a)(grad_(theta)pi_(theta)(a|s))/(pi_(theta)(a|s))`<br>因 ` (ln x)’ = 1/x `<br>`= Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)]`</p><p>所以得出策略梯度最重要的定理：</p><p>` grad_(theta)J(theta)=Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)] `</p><p>其中的`Q^pi(s,a)`也就是状态s的累计收益，可以在一次完整的动作轨迹中累计计算得出。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><p>该算法被称为 REINFORCE</p><ul><li>随机初始化`theta`</li><li>生成一个完整的策略`pi_theta`的轨迹: `S1,A1,R2,S2,A2,…,ST`。</li><li>For t=1, 2, … , T-1:<ul><li>` v_t = sum_(i=0)^(oo) gamma^i R_(t+i+1) `</li><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul></li></ul><p>参考：<br><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Lilian Weng:Policy Gradient Algorithms</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（四）：DQN实战</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/</id>
    <published>2019-12-30T06:37:03.000Z</published>
    <updated>2020-08-31T07:01:49.033Z</updated>
    
    <content type="html"><![CDATA[<p>我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用<a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank" rel="noopener">CartPole-v1</a>。我们的输入是一幅图片，动作是施加一个向左向右的力量，我们需要尽可能的保持木棍的平衡。</p><p><img src="/files/cartpole-v1.gif" alt="CartPole-v1"></p><p>对于这个环境，尝试了很多次，总是不能达到很好的效果，一度怀疑自己的代码写的有问题。后来仔细看了这个环境的奖励，是每一帧返回奖励1，哪怕是最后一帧也是返回1 的奖励。这里很明显是不合理的俄。我们需要重新定义这个奖励函数，也就是在游戏结束的时候，给一个比较大的惩罚，r=-100。很快可以达到收敛。</p><h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>, (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.mem = []</span><br><span class="line">        self.pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Save a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.mem) &lt; self.cap:</span><br><span class="line">            self.mem.append(<span class="keyword">None</span>)</span><br><span class="line">        self.mem[self.pos] = Transition(*args)</span><br><span class="line">        self.pos = (self.pos + <span class="number">1</span>) % self.cap</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.mem, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.mem)</span><br></pre></td></tr></table></figure><h2 id="Q网络"><a href="#Q网络" class="headerlink" title="Q网络"></a>Q网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="初始化参数和状态"><a href="#初始化参数和状态" class="headerlink" title="初始化参数和状态"></a>初始化参数和状态</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络</span></span><br><span class="line">policy_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络</span></span><br><span class="line">target_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络从策略网络复制参数</span></span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="探索和选择最佳动作"><a href="#探索和选择最佳动作" class="headerlink" title="探索和选择最佳动作"></a>探索和选择最佳动作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state, no_explore=False)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold <span class="keyword">or</span> no_explore:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(x).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)</span><br></pre></td></tr></table></figure><h2 id="优化模型-关键代码"><a href="#优化模型-关键代码" class="headerlink" title="优化模型(关键代码)"></a>优化模型(关键代码)</h2><p>这里主要是抽样、目标值计算、损失计算的部分。损失计算采用Huber loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span>  s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>,</span><br><span class="line">                        batch.next_state)), dtype=torch.uint8)</span><br><span class="line">    non_final_next_states = torch.FloatTensor([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>])</span><br><span class="line">    state_batch = torch.FloatTensor(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.FloatTensor(batch.reward)</span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 限制网络更新的幅度，可以大幅提升训练的效果。</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>这里主要有主循环、获取输入、记录回放、训练、复制参数等环节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state, i_episode%<span class="number">50</span>==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(i_episode%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward,done,_ = env.step(action.item())</span><br><span class="line">        <span class="comment"># reward = torch.tensor([reward])</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt; <span class="number">2500</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                optimize_model()</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br></pre></td></tr></table></figure><p><img src="/files/dqn2.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用&lt;a href=&quot;https://gym.openai.com/envs/CartPole-v1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CartPole-v1&lt;
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（三）：DQN</title>
    <link href="http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/"/>
    <id>http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/</id>
    <published>2019-12-28T03:53:37.000Z</published>
    <updated>2020-08-31T07:01:49.033Z</updated>
    
    <content type="html"><![CDATA[<p>上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学习就有了施展的空间了。</p><p>Q表存储着状态s和动作a、奖励r的信息。我们知道深度神经网络，也具有存储信息的能力。DQN算法就是将Q-table存储结构替换为神经网络来存储信息。我们定义神经网络`f(s, w) ~~ Q(s)`，输出为一个向量`[Q(s, a_1), Q(s, a_2), Q(s, a_3), …, Q(s, a_n)]`。经过这样的改造，我们就可以用Q-learing的算法思路来解决更复杂的状态空间的问题了。我们可以通过下面两张图来对比Q-learning和DQN的异同。</p><p><img src="/img/rl-3/1.png" alt="Q-learning"></p><p><img src="/img/rl-3/2.png" alt="Deep-Q-learning"></p><p>网络结构要根据具体问题来设计。在神经网络训练的过程中，损失函数是关键。我们采用MSE来计算error。</p><p>`L(w) = (ubrace(r + argmax_aQ(s’, a’))_(目标值) - ubrace(Q(s, a))_(预测值))^2`</p><h2 id="基本算法描述"><a href="#基本算法描述" class="headerlink" title="基本算法描述"></a>基本算法描述</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">使用随机参数初始化网络Q</span><br><span class="line"><span class="keyword">while</span> 未收敛:</span><br><span class="line">  action 按一定概率随机选取，其余使用argmax Q(s)选取</span><br><span class="line">  模拟执行 action 获取 状态 s_, 奖励 r, 是否完成 done</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    target = r</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    target = r + gamma * argmax Q(s_)</span><br><span class="line">  loss = MSE(target, Q(s, a))</span><br><span class="line">  使用loss更新网络Q</span><br><span class="line">  s = s_</span><br></pre></td></tr></table></figure><p>但是，通过实验我们会发现，训练过程非常的不稳定。稳定性是强化学习所面临的主要问题之一，为了达到稳定的训练我们需要运用一些优化的手段。</p><h2 id="环境的稳定性"><a href="#环境的稳定性" class="headerlink" title="环境的稳定性"></a>环境的稳定性</h2><p>Agent生活在环境之中，并根据环境的反馈进行学习，但环境是否是稳定的呢？假设agent在学习出门穿衣的技能，它需要学会在冬天多穿，夏天少穿。但是这个agent只会根据当天的反馈来修正自己的行为，也就是说这个agent是没有记忆的。那么这个agent就会在多次失败后终于在冬天学会了多穿衣，但转眼之间到了夏天他又会陷入不断的失败，最终他在夏天学会了少穿衣之后，又会在冬天陷入失败，如此循环不断，永远不会收敛。如果要能够很好的训练，这个agent至少要有一整年的记忆空间，每一批都要从过去的记忆中抽取记忆来进行训练，就可以避免遗忘过去的教训。</p><p>在DeepMind的Atari 论文中提到</p><blockquote><p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p></blockquote><p>意思是，受生物学启发，他们采用了一种叫做经验回放（experience replay）的机制，随机抽取数据来到达“移除观察序列的相关性，平滑数据分布的改变”的目的。<br><img src="/img/rl-3/4.png" alt="DQN with Atari"></p><p>我们已经理解了要有经验回放的记忆，但是为什么一定要随机抽取呢？对此论文认为这个随机抽取可以移除序列相关性、平滑分布中的改变。当如何理解呢？简单的说就是在我们不清楚合理的周期的情况下，能够保证采样的合理性。我们仍然以四季穿衣举例，假设我们不使用随机采样，我们必须在每次训练中都采用365天左右的数据，才能使我们的数据样本分布合理。可是agent并不清楚一年365天这个规律，这恰恰是我们所要学习的内容。采用随机采用，就可以自然的做到数据的分布合理，而且只需要使用记忆中的部分数据，减少单次迭代的计算量。</p><p>在这个记忆里，我们并不记录当时的网络参数（分析过程），我们只记录（状态s，动作a，新状态s’, 单步奖励r)。显然，记忆的尺寸不可能无限大。当记忆体增大到一定程度之后，我们采用滚动的方式用最新的记忆替换掉最老的记忆。就像在学习围棋的过程中，有初学者阶段的对局记忆，也有高手阶段的对局记忆，在提升棋艺的角度来看，高手阶段的记忆显然比初学者阶段的记忆更有价值。</p><p>说句题外话，其实对于一个民族而言也是一样的。我们这个民族拥有一个非常好的传统，就是记述历史，也就是等于我们这个民族拥有足够大的记忆量，这是我们胜于其他民族的。但是这个历史记录中，掺杂了历史上不同阶段的评价，这些评价是根据当时的经验得出的。而根据DQN的算法描述来看，对我们最有价值的部分其实是原始信息，而不是那些附加在之上的评价，这些评价有正确的部分，也有错误的部分，我们不用去过多关心。我们只需要在今天的认知（也就是最新的训练结果）基础上，对历史原始信息（旧状态、动作、新状态、单步奖励）进行随机的抽样分析即可。</p><h2 id="网络稳定性"><a href="#网络稳定性" class="headerlink" title="网络稳定性"></a>网络稳定性</h2><p>DQN另一个稳定性问题与目标值计算有关。因为`target = r + gamma * argmax Q(s’)`，所以目标值与网络参数本身是相关，而参数在训练中是不断变化的，所以这会造成训练中的不稳定。一个神经网络可以自动收敛，取决于存在一个稳定的目标，如果目标本身在不断的游移变动，那么想要达到稳定就比较困难。这就像站在平地上的人很容易平衡，但如果让人站在一个不断晃动的木板上，就很难达到平衡。为了解决这个问题，我们需要构建一个稳定的目标函数。</p><p>解决的方法是采用两个网络代替一个网络。一个网络用于训练调整参数，称之为策略网络，另一个专门用于计算目标，称之为目标网络。目标网络与策略网络拥有完全一样的网络结构，在训练的过程中目标网络的参数是固定的。执行一小批训练之后，将策略网络最新的参数复制到目标网络中。</p><p><img src="/img/rl-3/3.png" alt="目标网络"></p><p>经验回放和目标网络的效果见下表（引用自Nature 论文）：<br><img src="/img/rl-3/5.png" alt="优化对比"></p><h2 id="其他DQN优化"><a href="#其他DQN优化" class="headerlink" title="其他DQN优化"></a>其他DQN优化</h2><p>关于DQN的优化，这篇文章描述的比较全面 <a href="https://zhuanlan.zhihu.com/p/21547911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21547911</a>。在之后的实践中考虑是否进一步深入。主要介绍3个改进：</p><p><img src="/img/rl-3/6.png" alt="DQN优化"></p><ul><li>Double DQN：对目标值计算的优化，a’使用策略网络选择的动作来代替目标网络选择的动作。</li><li>Prioritised replay：使用优先队列（priority queue）来存储经验，避免丢弃早期的重要经验。使用error作为优先级，仿生学技巧，类似于生物对可怕往事的记忆。</li><li>Dueling Network：将Q网络分成两个通道，一个输出V，一个输出A，最后再合起来得到Q。如下图所示（引用自Dueling Network论文）。这个方法主要是idea很简单但是很难想到，然后效果一级棒，因此也成为了ICML的best paper。</li></ul><p><img src="/img/rl-3/7.png" alt="Dueling Network"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（二）：Q-learning实战</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/</id>
    <published>2019-12-27T08:02:19.000Z</published>
    <updated>2020-08-31T07:01:49.033Z</updated>
    
    <content type="html"><![CDATA[<p>我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用<code>gym</code>来作为我们对实验环境。安装方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym</span><br></pre></td></tr></table></figure><h2 id="初识环境"><a href="#初识环境" class="headerlink" title="初识环境"></a>初识环境</h2><p>我们的实验环境是一个冰湖滑行游戏，你将控制一个agent在冰面到达目标终点，前进方向并不总受你的控制，你还需要躲过冰窟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># 构造游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"><span class="comment"># 动作空间-&gt; Discrete(4)</span></span><br><span class="line">print(env.action_space)</span><br><span class="line"><span class="comment"># 状态空间-&gt; Discrete(16)</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line"><span class="comment"># 初始化游戏环境，并得到状态s</span></span><br><span class="line">s = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 渲染游戏画面</span></span><br><span class="line">    env.render()</span><br><span class="line">    <span class="comment"># 从动作空间中随机选择一个动作a</span></span><br><span class="line">    a = env.action_space.sample()</span><br><span class="line">    <span class="comment"># 执行动作a，得到新状态s，奖励r，是否完成done</span></span><br><span class="line">    s, r, done, info = env.step(a) <span class="comment"># take a random action</span></span><br><span class="line">    print(s, r, done, info)</span><br><span class="line"><span class="comment"># 关闭环境</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>游戏画面示意如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SFFF       (S: 起点，安全)</span><br><span class="line">FHFH       (F: 冰面，安全)</span><br><span class="line">FFFH       (H: 冰窟，进入则失败)</span><br><span class="line">HFFG       (G: 终点，到达则成功)</span><br></pre></td></tr></table></figure><h2 id="Agent结构"><a href="#Agent结构" class="headerlink" title="Agent结构"></a>Agent结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLAgent</span><span class="params">()</span>:</span></span><br><span class="line">    q = <span class="keyword">None</span></span><br><span class="line">    action_space = <span class="keyword">None</span></span><br><span class="line">    epsilon = <span class="number">0.1</span> <span class="comment"># 探索率</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># 衰减率</span></span><br><span class="line">    lr = <span class="number">0.1</span> <span class="comment"># 学习率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, action_space, state_count, epsilon=<span class="number">0.1</span>, lr=<span class="number">0.1</span>, gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.q = [[<span class="number">0.</span> <span class="keyword">for</span> a <span class="keyword">in</span> range(action_space.n)] <span class="keyword">for</span> s <span class="keyword">in</span> range(state_count)]</span><br><span class="line">        self.action_space = action_space</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据状态s，选择动作a</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新状态变化并学习，状态s执行了a动作，得到了奖励r，状态转移到了s_</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这是一个Agent的一般结构，主要由初始化、选择动作、更新状态变化，三个方法构成。后续的其他算法将依然采用该结构。q表数据使用一个二维数组表示，其大小为 state_count action_count，对于这个项目而言是一个 `16*4` 的大小。</p><h2 id="添加Q-table的辅助方法"><a href="#添加Q-table的辅助方法" class="headerlink" title="添加Q-table的辅助方法"></a>添加Q-table的辅助方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回状态s的最佳动作a、及其r值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    max_r = -math.inf</span><br><span class="line">    max_a = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(self.action_space.n):</span><br><span class="line">        r = self.q_get(s, a)</span><br><span class="line">        <span class="keyword">if</span> r &gt; max_r:</span><br><span class="line">            max_a = a</span><br><span class="line">            max_r = r</span><br><span class="line">    <span class="keyword">return</span> max_a, max_r</span><br><span class="line"><span class="comment"># 获得 状态s，动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_get</span><span class="params">(self, s, a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.q[s][a]</span><br><span class="line"><span class="comment"># 更新 状态s 动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_put</span><span class="params">(self, s, a, v)</span>:</span></span><br><span class="line">    self.q[s][a] = v</span><br></pre></td></tr></table></figure><h2 id="Q-learning的关键步骤"><a href="#Q-learning的关键步骤" class="headerlink" title="Q-learning的关键步骤"></a>Q-learning的关键步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="comment"># 按一定概率进行随机探索</span></span><br><span class="line">        <span class="keyword">return</span> self.action_space.sample()</span><br><span class="line">    <span class="comment"># 返回最佳动作</span></span><br><span class="line">    a, _ = self.argmax(s)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">    q = self.q_get(s, a)</span><br><span class="line">    _, r_ = self.argmax(s_)</span><br><span class="line">    <span class="comment"># Q &lt;- Q + a(Q' - Q)</span></span><br><span class="line">    <span class="comment"># &lt;=&gt; Q &lt;- (1-a)Q + a(Q')</span></span><br><span class="line">    q = (<span class="number">1</span>-self.lr) * q + self.lr * (r + self.gamma * r_)</span><br><span class="line">    self.q_put(s, a, q)</span><br></pre></td></tr></table></figure><h2 id="训练主循环"><a href="#训练主循环" class="headerlink" title="训练主循环"></a>训练主循环</h2><p>我们进行10000局游戏的训练，每局游戏执行直到完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line">agent = QLAgent(env.action_space, env.observation_space.n)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># env.render()  # 训练过程不需要渲染</span></span><br><span class="line">        a = agent.choose_action(s) <span class="comment"># 选择动作</span></span><br><span class="line">        s_, r, done, info = env.step(a) <span class="comment"># 执行动作</span></span><br><span class="line">        agent.update_transition(s, a, r, s_) <span class="comment"># 更新状态变化</span></span><br><span class="line">        s = s_</span><br><span class="line"><span class="comment"># 显示训练后的Q表</span></span><br><span class="line">print(agent.q)</span><br></pre></td></tr></table></figure><h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><p>在测试中，我们只选择最佳策略，不再探索，也不再更新Q表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获胜次数</span></span><br><span class="line">total_win = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># 选择最佳策略</span></span><br><span class="line">        a, _ = agent.argmax(s)</span><br><span class="line">        <span class="comment"># 执行动作 a</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">and</span> r == <span class="number">1</span>:</span><br><span class="line">            total_win += <span class="number">1</span></span><br><span class="line">        s = s_</span><br><span class="line">print(<span class="string">'Total win='</span>, total_win)</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>最终测试的效果是在1万局中获胜了7284次，说明达到了不错的实验效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用&lt;code&gt;gym&lt;/code&gt;来作为我们对实验环境。安装方法：&lt;/p&gt;
&lt;figure cla
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（一）：Q-learning</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/</id>
    <published>2019-12-27T03:28:16.000Z</published>
    <updated>2020-08-31T07:01:49.033Z</updated>
    
    <content type="html"><![CDATA[<p>完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。</p><p><img src="/img/rl-1/4.png" alt="强化学习图示"></p><h2 id="K摇臂赌博机"><a href="#K摇臂赌博机" class="headerlink" title="K摇臂赌博机"></a>K摇臂赌博机</h2><p>我们可以考虑一个最简单的环境：一个动作可立刻获得奖励，目标是使每一个动作的奖励最大化。对这种单步强化学习任务，可以设计一个理论模型——“K-摇臂赌博机”。这个赌博机有K个摇臂，赌徒在投入一个硬币后可一选择按下一个摇臂，每个摇臂以一定概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。最终所获得的总奖励被称为累计奖励。</p><p><img src="/img/rl-1/2.jpg" alt="K摇臂赌博机"></p><p>对于这个简单模型，若要知道每个摇臂的概率，我们只需要进行足够多的尝试即可，这是“仅探索”策略；若要奖励最大化，则需要执行奖赏概率最大的动作即可，这是“仅利用”策略。但在更复杂的环境中，我们不可能对每一个状态的每个动作都进行足够多的探索。比如围棋，我们后续的探索是需要依赖于之前的探索的。因此我们需要在探索和利用之间进行平衡。我们在学习的过程中，必须要保持一定的概率`epsilon`进行探索，其余时候则执行学习到的策略。</p><h2 id="基本概念术语"><a href="#基本概念术语" class="headerlink" title="基本概念术语"></a>基本概念术语</h2><p>为了便于分析讨论，我们定义一些术语。</p><ul><li><p>机器agent处于环境`E`中。</p></li><li><p>状态空间为`S`，每个状态`s in S`是机器感知到的环境描述。</p></li><li><p>机器能够采取的可采取的动作a的集合即动作空间`A`，`a in A`。</p></li><li><p>转移函数`P`表示：当机器执行了一个动作`a`后，环境有一定概率从状态`s`改变为新的状态`s’`。即：`s’=P(s, a)`</p></li><li><p>奖赏函数`R`则表示了执行动作可能获得的奖赏`r`。即：`r=R(s,a)`。</p></li><li><p>环境可以描述为`E=&lt;&lt;S, A, P, R&gt;&gt;`。</p></li><li><p>强化学习的任务是习得一个策略（policy）`pi`，使机器在状态`s`下选择到最佳的`a`。策略有两种描述方法：</p><ol><li>`a=pi(s)` 表示状态`s`下将执行动作`a`，是一种确定性的表示法。</li><li>`pi(s, a)` 表示状态`s`下执行动作`a`的概率。这里有 `sum_a pi(s,a)=1`</li></ol></li><li><p>累计奖励指连续的执行一串动作之后的奖励总和。</p></li><li><p>`Q^(pi)(s, a)`表示在状态`s`下，执行动作`a`，再策略`pi`的累计奖励。为方便讨论后续直接写为`Q(s,a)`。</p></li><li><p>`V^(pi)(s)` 表示在状态`s`下，使用策略`pi`的累计奖励。为方便讨论后续直接写为`V(s)`。</p></li></ul><p>强化学习往往不会立刻得到奖励，而是在很多步之后才能得到一个诸如成功/失败的奖励，这是我们的算法需要反思之前所有的动作来学习。所以强化学习可以视作一种延迟标记的监督学习。</p><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>对于我们要学习的`Q(s,a)`函数，我们可以使用一个Q-table来表示。Q-table是一个二维表，记录每个状态`s in S, a in A`的`Q`值。Q表被初始化为0的状态。在状态`s`执行了动作`a`之后，得到状态`s’`，奖励`r`。我们将潜在的`Q`函数记为`Q_(real)`，其值为当前奖励r与后续状态`s’`的最佳累计奖励之和。则有：</p><p>` Q_(real)(s, a) = r + gamma * argmax_aQ(s’, a) `<br>` err = Q_(real)(s, a) - Q(s, a) `</p><p>其中`gamma`为`Q`函数的偏差，`err`为误差，`alpha`为学习率。 可得出更新公式为：</p><p>` Q(s, a) leftarrow Q(s, a) + alpha*err `<br>即：<br>` Q(s,a) leftarrow (1-alpha)Q(s,a) + alpha(r + gamma * argmax_aQ(s’, a)) `</p><p>以上公式即为Q-learning的关键</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化Q表</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    Q(s, a) = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">  <span class="keyword">if</span> rand() &lt; epsilon:</span><br><span class="line">    a = 从 A 中随机选取一个动作</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    a = 从 A 中选取使 Q(a) 最大的a</span><br><span class="line">  <span class="comment"># 从环境中获得反馈</span></span><br><span class="line">  r = R(s, a)</span><br><span class="line">  s1 = P(s, a)</span><br><span class="line">  <span class="comment"># 更新Q表</span></span><br><span class="line">  Q(s,a) = (<span class="number">1</span>-alpha)*Q(s,a) + alpha * (r + gamma * argmax Q(s1, a) - Q(s, a))</span><br><span class="line">  s = s1</span><br></pre></td></tr></table></figure><p>下图是一个Q表内存结构，在经过一定的学习后，Q表的内容将能够不断逼近每个状态的每个动作的累计收益。<br><img src="/img/rl-1/3.png" alt="Q-table"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/rl-1/4.png&quot; alt=&quot;强化学习图示&quot;&gt;&lt;/p
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
</feed>
