<?xml version="1.0" encoding="utf-8"?>
<feed xmlns="http://www.w3.org/2005/Atom">
  <title>桂糊涂的博客</title>
  
  <subtitle>代码杂记</subtitle>
  <link href="/atom.xml" rel="self"/>
  
  <link href="http://guileen.github.io/"/>
  <updated>2020-01-03T08:09:29.661Z</updated>
  <id>http://guileen.github.io/</id>
  
  <author>
    <name>桂糊涂</name>
    
  </author>
  
  <generator uri="http://hexo.io/">Hexo</generator>
  
  <entry>
    <title>增强学习简介（六）：策略梯度实例</title>
    <link href="http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/"/>
    <id>http://guileen.github.io/2020/01/03/introduce-reinforcement-learning-6/</id>
    <published>2020-01-03T06:40:51.000Z</published>
    <updated>2020-01-03T08:09:29.661Z</updated>
    
    <content type="html"><![CDATA[<p>和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网络在各个动作的Q值是多少。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">PolicyNet</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(PolicyNet, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(F.dropout(self.fc1(x), <span class="number">0.1</span>))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="comment"># 输出层需要使用softmax</span></span><br><span class="line">        <span class="keyword">return</span> F.softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>不要忘了输出层的SoftMax。</p><h2 id="初始化参数"><a href="#初始化参数" class="headerlink" title="初始化参数"></a>初始化参数</h2><p>相对于DQN，我们也不需要额外的目标网络和参数复制操作，只需要一个策略网络即可。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">BATCH_SIZE = <span class="number">256</span></span><br><span class="line">GAMMA = <span class="number">0.99</span></span><br><span class="line">HIDDEN_SIZE = <span class="number">15</span></span><br><span class="line">LR = <span class="number">0.005</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line">policy_net = PolicyNet(input_size, HIDDEN_SIZE, n_actions)</span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line"></span><br><span class="line">steps_done = <span class="number">0</span></span><br></pre></td></tr></table></figure><h2 id="选择动作"><a href="#选择动作" class="headerlink" title="选择动作"></a>选择动作</h2><p>在选择动作时，我们不再需要特地设置探索概率，因为输出结果就是各个动作的概率分布。我们使用<code>torch.distributions.categorical.Categorical</code> 来进行取样。在每次选择动作时，我们同时记录对应的概率，以便后续使用。这个概率就是 `ln pi_theta(S_t,A_t)`</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br></pre></td><td class="code"><pre><span class="line">log_probs = []</span><br><span class="line">rewards = []</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state),<span class="number">0</span>)</span><br><span class="line">    probs = policy_net(x)</span><br><span class="line">    c = Categorical(probs)</span><br><span class="line">    action = c.sample()</span><br><span class="line">    <span class="comment"># log action probs to plt</span></span><br><span class="line">    prob = c.log_prob(action)</span><br><span class="line">    log_probs.append(prob)</span><br><span class="line">    <span class="keyword">return</span> action</span><br></pre></td></tr></table></figure><h2 id="优化模型"><a href="#优化模型" class="headerlink" title="优化模型"></a>优化模型</h2><p>为了更新参数，我们首先需要计算`v_t`，这在后续参数迭代中需要用到。</p><ul><li>` v_t = r_(t+1) + gamma * v_(t+1) `</li></ul><p>在模拟执行的时候，我们记录了每一步的reward，我们需要计算每一步的`v_t`，其顺序与执行顺序一致。根据公式我们需要倒序的计算`v_t`，然后将计算好的结果倒序排列，就形成了`v_1,v_2…v_t`的序列。最后我们需要将数据标准化。(TODO: 这里可能存在一个序列对应的问题，其中每一个状态的累计收益，是后续状态收益之和，不包含本轮收益)</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">values = []</span><br><span class="line">v = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> reward <span class="keyword">in</span> reversed(rewards):</span><br><span class="line">    v = v * GAMMA + reward</span><br><span class="line">    values.insert(<span class="number">0</span>, v)</span><br><span class="line">mean = np.mean(values)</span><br><span class="line">std = np.std(values)</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(size):</span><br><span class="line">    values[i] = (values[i] - mean) / std</span><br></pre></td></tr></table></figure><p>接下来我们需要更新参数，参数更新的公式为：</p><ul><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul><p>我们将其转换为损失函数形式:</p><ul><li>` L(theta) = - v_t ln pi_theta(A_t|S_t) `</li></ul><p>这个损失函数的形式可以帮助我们更好的理解策略梯度的原理。如果一个动作价值为负值，但是其选择概率为正，则损失较大。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">loss = []</span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> np.random.choice(size, n):</span><br><span class="line">    loss.append(- values[i] * log_probs[i])</span><br><span class="line">loss = torch.cat(loss).sum()</span><br><span class="line"></span><br><span class="line">optimizer.zero_grad()</span><br><span class="line">loss.backward()</span><br><span class="line"><span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">    param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>训练循环需要在一局结束之后进行。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">5000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state)</span><br><span class="line">        <span class="keyword">if</span> i_episode % <span class="number">2000</span> == <span class="number">0</span>:</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward, done,_ = env.step(action.item())</span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        rewards.append(reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt;= <span class="number">2500</span>:</span><br><span class="line">            optimize_model()</span><br><span class="line">            print(<span class="string">'EP'</span>, i_episode)</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            rewards = []</span><br><span class="line">            log_probs = []</span><br><span class="line">            <span class="keyword">break</span></span><br></pre></td></tr></table></figure><p><img src="/files/cart-pg.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;和第四节DQN的实例一样，我们依然使用CartPole-v1来作为训练环境。策略梯度的网络和DQN的网络结构是类似的，只是在输出层需要做Softmax处理，因为策略梯度的输出本质上是一个分类问题——将某一个状态分类到某一个动作的概率。而DQN网络则是一个回归问题——某一个网
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>增强学习简介（五）：策略梯度Policy Gradient</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-5/</id>
    <published>2019-12-30T06:37:07.000Z</published>
    <updated>2020-01-03T07:54:44.667Z</updated>
    
    <content type="html"><![CDATA[<p>DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问题，而Q值的目的也是为了选择动作。我们可以将增加学习的问题简化为选择动作的问题。那么我们可否使用深度学习直接做出动作选择呢？显然，我们可以定义一个网络`pi_theta`，其中输入为状态`s`，输出为每个动作`a`的概率。</p><p><img src="/img/rl-5/1.png" alt="策略梯度"></p><p>因为这个网络与策略函数的定义一样，所以被称为策略网络。`pi_theta(a|s)`，表示在`s`状态下选择动作`a`的概率。只要这个网络能够收敛，我们就可以直接得到最佳策略。这个网络的奖励函数也就是最终游戏的总奖励。</p><p>`J(theta) = sum_(s in S)d^pi(s)V^pi(s) = sum_(s in S)d^pi(s)sum_(a in A)pi_theta(a|s)Q^pi(s, a)`</p><p>`d^pi(s)`指状态`s`在马尔科夫链上的稳定分布，`d^pi(s) = lim_(t-&gt;oo)P(s_t=s|s_0,pi_theta)`。</p><p>但是这个表达式看上去是不可能计算的，因为状态的分布和Q值都是随着策略的更新而不断变化的。但是我们并不需要计算`J(theta)`，在梯度下降法中我们只需要计算梯度`grad_(theta)J(theta)`即可</p><p>`grad_(theta)V^pi(s)`<br>`= grad_(theta)(sum_(a in A)pi_theta(a|s)Q^pi(s, a))`<br>根据导数乘法规则<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_thetaQ^pi(s, a))`<br>展开`Q^pi(s,a)`为各各种可能的下一状态奖励之和<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)grad_(theta)sum_(s’,r)P(s’,r|s,a)(r+V^pi(s’)))`<br>而其中状态转移函数`P(s’,r|s,a)`、奖励`r`由环境决定，与`grad_theta`无关，所以<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’,r)P(s’,r|s,a)grad_(theta)V^pi(s’))`<br>`= sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>现在我们有了一个形式非常好的递归表达式：<br>`grad_(theta)V^pi(s) = sum_(a in A)(grad_(theta)pi_(theta)Q^pi(s, a)+pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’))`</p><p>设 `rho^pi(s-&gt;x, k)` 表示在策略`pi^theta`下，`k`步以后状态`s`转移到状态`x`的概率。有：</p><ul><li>`rho^pi(s-&gt;s, k=0)=1`</li><li>`rho^pi(s-&gt;s’, k=1)=sum_(a)pi_(theta)(a|s)P(s’|s,a)`</li><li>`rho^pi(s-&gt;x, k+1) = sum_(s’)rho^pi(s-&gt;s’, k)rho^pi(s’-&gt;x, 1)`</li></ul><p>为了简化计算，令 `phi(s)=sum_(a in A)grad_(theta)pi_theta(a|s)Q^pi(s,a)`</p><p>`grad_(theta)V^pi(s)`<br>`= phi(s) + sum_(a in A)pi_(theta)(a|s)sum_(s’)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)sum_(a in A)pi_(theta)(a|s)P(s’|s,a)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)grad_(theta)V^pi(s’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)(phi(s’) + sum_(s’’)rho^pi(s’-&gt;s’’,1)grad_(theta)V^pi(s’’)) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)grad_(theta)V^pi(s’’) `<br>`= phi(s) + sum_(s’)rho^pi(s-&gt;s’,1)phi(s’) + sum_(s’’)rho^pi(s-&gt;s’’,2)phi(s’’) + sum_(s’’’)rho^pi(s-&gt;s’’’,3)grad_(theta)V^pi(s’’’) `<br>`= …`<br>`= sum_(x in S)sum_(k=0)^(oo)rho^pi(s-&gt;x, k)phi(x)`</p><p>令 `eta(s)=sum_(k=0)^(oo)rho^pi(s_0-&gt;s, k)`</p><p>`grad_(theta)J(theta)=grad_(theta)V^pi(s_0)`<br>`= sum_(s)sum_(k=0)^(oo)rho^pi(s_0-&gt;s,k)phi(s)`<br>`= sum_(s)eta(s)phi(s)`<br>`= (sum_(s)eta(s))sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `sum_(s)eta(s)` 属于常数，对于求梯度而言常数可以忽略。<br>`prop sum_(s)((eta(s))/(sum_(s)eta(s)))phi(s)`<br>因 `eta(s)/(sum_(s)eta(s))`表示`s`的稳定分布<br>`= sum_(s)d^pi(s)sum_a grad_(theta)pi_(theta)(a|s)Q^pi(s,a)`<br>`= sum_(s)d^pi(s)sum_a pi_(theta)(a|s)Q^pi(s,a)(grad_(theta)pi_(theta)(a|s))/(pi_(theta)(a|s))`<br>因 ` (ln x)’ = 1/x `<br>`= Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)]`</p><p>所以得出策略梯度最重要的定理：</p><p>` grad_(theta)J(theta)=Err_pi[Q^pi(s,a)grad_theta ln pi_theta(a|s)] `</p><p>其中的`Q^pi(s,a)`也就是状态s的累计收益，可以在一次完整的动作轨迹中累计计算得出。</p><h2 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h2><ul><li>随机初始化`theta`</li><li>生成一个完整的策略`pi_theta`的轨迹: `S1,A1,R2,S2,A2,…,ST`。</li><li>For t=1, 2, … , T:<ul><li>` v_t = sum_(i=0)^(oo) gamma^i R_(t+i+1) `</li><li>` theta larr theta + alpha v_t ln pi_theta (A_t|S_t) `</li></ul></li></ul><p>参考：<br><a href="https://lilianweng.github.io/lil-log/2018/04/08/policy-gradient-algorithms.html" target="_blank" rel="noopener">Lilian Weng:Policy Gradient Algorithms</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;DQN证明了深度学习在增强学习中的可行性。深度学习可以将复杂的概念隐含在网络之中。但是DQN并没有将所有的概念都隐含在网络之中，只是把Q值的计算放在了网络之中，比如`epsilon-greedy`动作选择策略。因为如何选择动作和如何通过Q值计算出目标值，都是DQN所面临的问
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>增强学习简介（四）：DQN实战</title>
    <link href="http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/"/>
    <id>http://guileen.github.io/2019/12/30/introduce-reinforcement-learning-4/</id>
    <published>2019-12-30T06:37:03.000Z</published>
    <updated>2020-01-03T06:40:08.156Z</updated>
    
    <content type="html"><![CDATA[<p>我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用<a href="https://gym.openai.com/envs/CartPole-v1/" target="_blank" rel="noopener">CartPole-v1</a>。我们的输入是一幅图片，动作是施加一个向左向右的力量，我们需要尽可能的保持木棍的平衡。</p><p><img src="/files/cartpole-v1.gif" alt="CartPole-v1"></p><p>对于这个环境，尝试了很多次，总是不能达到很好的效果，一度怀疑自己的代码写的有问题。后来仔细看了这个环境的奖励，是每一帧返回奖励1，哪怕是最后一帧也是返回1 的奖励。这里很明显是不合理的俄。我们需要重新定义这个奖励函数，也就是在游戏结束的时候，给一个比较大的惩罚，r=-100。很快可以达到收敛。</p><h2 id="Replay-Memory"><a href="#Replay-Memory" class="headerlink" title="Replay Memory"></a>Replay Memory</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br></pre></td><td class="code"><pre><span class="line">Transition = namedtuple(<span class="string">'Transition'</span>, (<span class="string">'state'</span>, <span class="string">'action'</span>, <span class="string">'next_state'</span>, <span class="string">'reward'</span>))</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">ReplayMemory</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, capacity)</span>:</span></span><br><span class="line">        self.cap = capacity</span><br><span class="line">        self.mem = []</span><br><span class="line">        self.pos = <span class="number">0</span></span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">push</span><span class="params">(self, *args)</span>:</span></span><br><span class="line">        <span class="string">"""Save a transition."""</span></span><br><span class="line">        <span class="keyword">if</span> len(self.mem) &lt; self.cap:</span><br><span class="line">            self.mem.append(<span class="keyword">None</span>)</span><br><span class="line">        self.mem[self.pos] = Transition(*args)</span><br><span class="line">        self.pos = (self.pos + <span class="number">1</span>) % self.cap</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">sample</span><span class="params">(self, batch_size)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> random.sample(self.mem, batch_size)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__len__</span><span class="params">(self)</span>:</span></span><br><span class="line">        <span class="keyword">return</span> len(self.mem)</span><br></pre></td></tr></table></figure><h2 id="Q网络"><a href="#Q网络" class="headerlink" title="Q网络"></a>Q网络</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">DQN</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, input_size, hidden, outputs)</span>:</span></span><br><span class="line">        super(DQN, self).__init__()</span><br><span class="line">        self.fc1 = nn.Linear(input_size, hidden)</span><br><span class="line">        self.fc2 = nn.Linear(hidden, outputs)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> x</span><br></pre></td></tr></table></figure><h2 id="初始化参数和状态"><a href="#初始化参数和状态" class="headerlink" title="初始化参数和状态"></a>初始化参数和状态</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br><span class="line">25</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line">env = gym.make(<span class="string">'CartPole-v0'</span>).unwrapped</span><br><span class="line"></span><br><span class="line">BATCH_SIZE = <span class="number">128</span></span><br><span class="line">GAMMA = <span class="number">0.9</span></span><br><span class="line">EPS_START = <span class="number">0.9</span></span><br><span class="line">EPS_END = <span class="number">0.05</span></span><br><span class="line">EPS_DECAY = <span class="number">200</span></span><br><span class="line">TARGET_UPDATE = <span class="number">10</span></span><br><span class="line">LR = <span class="number">0.01</span></span><br><span class="line"></span><br><span class="line">n_actions = env.action_space.n</span><br><span class="line">input_size = env.observation_space.shape[<span class="number">0</span>]</span><br><span class="line"></span><br><span class="line"><span class="comment"># 策略网络</span></span><br><span class="line">policy_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络</span></span><br><span class="line">target_net = DQN(input_size, <span class="number">10</span>, n_actions)</span><br><span class="line"><span class="comment"># 目标网络从策略网络复制参数</span></span><br><span class="line">target_net.load_state_dict(policy_net.state_dict())</span><br><span class="line">target_net.eval()</span><br><span class="line"></span><br><span class="line">optimizer = optim.Adam(policy_net.parameters(), lr=LR)</span><br><span class="line">memory = ReplayMemory(<span class="number">10000</span>)</span><br></pre></td></tr></table></figure><h2 id="探索和选择最佳动作"><a href="#探索和选择最佳动作" class="headerlink" title="探索和选择最佳动作"></a>探索和选择最佳动作</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">steps_done = <span class="number">0</span></span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">select_action</span><span class="params">(state, no_explore=False)</span>:</span></span><br><span class="line">    x = torch.unsqueeze(torch.FloatTensor(state), <span class="number">0</span>)</span><br><span class="line">    <span class="keyword">global</span> steps_done</span><br><span class="line">    sample = random.random()</span><br><span class="line">    eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(<span class="number">-1.</span> * steps_done / EPS_DECAY)</span><br><span class="line">    steps_done += <span class="number">1</span></span><br><span class="line">    <span class="keyword">if</span> sample &gt; eps_threshold <span class="keyword">or</span> no_explore:</span><br><span class="line">        <span class="keyword">with</span> torch.no_grad():</span><br><span class="line">            <span class="comment"># t.max(1) will return largest column value of each row.</span></span><br><span class="line">            <span class="comment"># second column on max result is index of where max element was</span></span><br><span class="line">            <span class="comment"># found, so we pick action with the larger expected reward.</span></span><br><span class="line">            <span class="keyword">return</span> policy_net(x).max(<span class="number">1</span>)[<span class="number">1</span>].view(<span class="number">1</span>, <span class="number">1</span>)</span><br><span class="line">    <span class="keyword">else</span>:</span><br><span class="line">        <span class="keyword">return</span> torch.tensor([[random.randrange(n_actions)]], dtype=torch.long)</span><br></pre></td></tr></table></figure><h2 id="优化模型-关键代码"><a href="#优化模型-关键代码" class="headerlink" title="优化模型(关键代码)"></a>优化模型(关键代码)</h2><p>这里主要是抽样、目标值计算、损失计算的部分。损失计算采用Huber loss。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">optimize_model</span><span class="params">()</span>:</span></span><br><span class="line">    <span class="keyword">if</span> len(memory) &lt; BATCH_SIZE:</span><br><span class="line">        <span class="keyword">return</span></span><br><span class="line">    transitions = memory.sample(BATCH_SIZE)</span><br><span class="line">    batch = Transition(*zip(*transitions))</span><br><span class="line">    non_final_mask = torch.tensor(tuple(map(<span class="keyword">lambda</span>  s: s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>,</span><br><span class="line">                        batch.next_state)), dtype=torch.uint8)</span><br><span class="line">    non_final_next_states = torch.FloatTensor([s <span class="keyword">for</span> s <span class="keyword">in</span> batch.next_state <span class="keyword">if</span> s <span class="keyword">is</span> <span class="keyword">not</span> <span class="keyword">None</span>])</span><br><span class="line">    state_batch = torch.FloatTensor(batch.state)</span><br><span class="line">    action_batch = torch.cat(batch.action)</span><br><span class="line">    reward_batch = torch.FloatTensor(batch.reward)</span><br><span class="line">    state_action_values = policy_net(state_batch).gather(<span class="number">1</span>, action_batch)</span><br><span class="line">    next_state_values = torch.zeros(BATCH_SIZE)</span><br><span class="line">    next_state_values[non_final_mask] = target_net(non_final_next_states).max(<span class="number">1</span>)[<span class="number">0</span>].detach()</span><br><span class="line">    expected_state_action_values = (next_state_values * GAMMA) + reward_batch</span><br><span class="line"></span><br><span class="line">    loss = F.mse_loss(state_action_values, expected_state_action_values.unsqueeze(<span class="number">1</span>))</span><br><span class="line"></span><br><span class="line">    optimizer.zero_grad()</span><br><span class="line">    loss.backward()</span><br><span class="line">    <span class="comment"># 限制网络更新的幅度，可以大幅提升训练的效果。</span></span><br><span class="line">    <span class="keyword">for</span> param <span class="keyword">in</span> policy_net.parameters():</span><br><span class="line">        param.grad.data.clamp_(<span class="number">-1</span>, <span class="number">1</span>)</span><br><span class="line">    optimizer.step()</span><br></pre></td></tr></table></figure><h2 id="训练循环"><a href="#训练循环" class="headerlink" title="训练循环"></a>训练循环</h2><p>这里主要有主循环、获取输入、记录回放、训练、复制参数等环节。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line">num_episodes = <span class="number">1000</span></span><br><span class="line"><span class="keyword">for</span> i_episode <span class="keyword">in</span> range(num_episodes):</span><br><span class="line">    state = env.reset()</span><br><span class="line">    <span class="keyword">for</span> t <span class="keyword">in</span> count():</span><br><span class="line">        action = select_action(state, i_episode%<span class="number">50</span>==<span class="number">0</span>)</span><br><span class="line">        <span class="keyword">if</span>(i_episode%<span class="number">50</span>==<span class="number">0</span>):</span><br><span class="line">            env.render()</span><br><span class="line">        next_state, reward,done,_ = env.step(action.item())</span><br><span class="line">        <span class="comment"># reward = torch.tensor([reward])</span></span><br><span class="line">        <span class="keyword">if</span> done:</span><br><span class="line">            reward = <span class="number">-100</span></span><br><span class="line">        memory.push(state, action, next_state, reward)</span><br><span class="line">        state = next_state</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">or</span> t &gt; <span class="number">2500</span>:</span><br><span class="line">            <span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">                optimize_model()</span><br><span class="line">            episode_durations.append(t+<span class="number">1</span>)</span><br><span class="line">            plot_durations()</span><br><span class="line">            <span class="keyword">break</span></span><br><span class="line">    <span class="keyword">if</span> i_episode % TARGET_UPDATE == <span class="number">0</span>:</span><br><span class="line">        target_net.load_state_dict(policy_net.state_dict())</span><br></pre></td></tr></table></figure><p><img src="/files/dqn2.png" alt="Clamp"></p><p><a href="/files/demo_dqn.py">完整代码</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们在上文简述了DQN算法。此文以PyTorch来实现一个DQN的例子。我们的环境选用&lt;a href=&quot;https://gym.openai.com/envs/CartPole-v1/&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;CartPole-v1&lt;
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（三）：DQN</title>
    <link href="http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/"/>
    <id>http://guileen.github.io/2019/12/28/introduce-reinforcement-learning-3/</id>
    <published>2019-12-28T03:53:37.000Z</published>
    <updated>2019-12-28T10:55:02.725Z</updated>
    
    <content type="html"><![CDATA[<p>上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学习就有了施展的空间了。</p><p>Q表存储着状态s和动作a、奖励r的信息。我们知道深度神经网络，也具有存储信息的能力。DQN算法就是将Q-table存储结构替换为神经网络来存储信息。我们定义神经网络`f(s, w) ~~ Q(s)`，输出为一个向量`[Q(s, a_1), Q(s, a_2), Q(s, a_3), …, Q(s, a_n)]`。经过这样的改造，我们就可以用Q-learing的算法思路来解决更复杂的状态空间的问题了。我们可以通过下面两张图来对比Q-learning和DQN的异同。</p><p><img src="/img/rl-3/1.png" alt="Q-learning"></p><p><img src="/img/rl-3/2.png" alt="Deep-Q-learning"></p><p>网络结构要根据具体问题来设计。在神经网络训练的过程中，损失函数是关键。我们采用MSE来计算error。</p><p>`L(w) = (ubrace(r + argmax_aQ(s’, a’))_(目标值) - ubrace(Q(s, a))_(预测值))^2`</p><h2 id="基本算法描述"><a href="#基本算法描述" class="headerlink" title="基本算法描述"></a>基本算法描述</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">使用随机参数初始化网络Q</span><br><span class="line"><span class="keyword">while</span> 未收敛:</span><br><span class="line">  action 按一定概率随机选取，其余使用argmax Q(s)选取</span><br><span class="line">  模拟执行 action 获取 状态 s_, 奖励 r, 是否完成 done</span><br><span class="line">  <span class="keyword">if</span> done:</span><br><span class="line">    target = r</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    target = r + gamma * argmax Q(s_)</span><br><span class="line">  loss = MSE(target, Q(s, a))</span><br><span class="line">  使用loss更新网络Q</span><br><span class="line">  s = s_</span><br></pre></td></tr></table></figure><p>但是，通过实验我们会发现，训练过程非常的不稳定。稳定性是强化学习所面临的主要问题之一，为了达到稳定的训练我们需要运用一些优化的手段。</p><h2 id="环境的稳定性"><a href="#环境的稳定性" class="headerlink" title="环境的稳定性"></a>环境的稳定性</h2><p>Agent生活在环境之中，并根据环境的反馈进行学习，但环境是否是稳定的呢？假设agent在学习出门穿衣的技能，它需要学会在冬天多穿，夏天少穿。但是这个agent只会根据当天的反馈来修正自己的行为，也就是说这个agent是没有记忆的。那么这个agent就会在多次失败后终于在冬天学会了多穿衣，但转眼之间到了夏天他又会陷入不断的失败，最终他在夏天学会了少穿衣之后，又会在冬天陷入失败，如此循环不断，永远不会收敛。如果要能够很好的训练，这个agent至少要有一整年的记忆空间，每一批都要从过去的记忆中抽取记忆来进行训练，就可以避免遗忘过去的教训。</p><p>在DeepMind的Atari 论文中提到</p><blockquote><p>First, we used a biologically inspired mechanism termed experience replay that randomizes over the data, thereby removing correlations in the observation sequence and smoothing over changes in the data distribution.</p></blockquote><p>意思是，受生物学启发，他们采用了一种叫做经验回放（experience replay）的机制，随机抽取数据来到达“移除观察序列的相关性，平滑数据分布的改变”的目的。<br><img src="/img/rl-3/4.png" alt="DQN with Atari"></p><p>我们已经理解了要有经验回放的记忆，但是为什么一定要随机抽取呢？对此论文认为这个随机抽取可以移除序列相关性、平滑分布中的改变。当如何理解呢？简单的说就是在我们不清楚合理的周期的情况下，能够保证采样的合理性。我们仍然以四季穿衣举例，假设我们不使用随机采样，我们必须在每次训练中都采用365天左右的数据，才能使我们的数据样本分布合理。可是agent并不清楚一年365天这个规律，这恰恰是我们所要学习的内容。采用随机采用，就可以自然的做到数据的分布合理，而且只需要使用记忆中的部分数据，减少单次迭代的计算量。</p><p>在这个记忆里，我们并不记录当时的网络参数（分析过程），我们只记录（状态s，动作a，新状态s’, 单步奖励r)。显然，记忆的尺寸不可能无限大。当记忆体增大到一定程度之后，我们采用滚动的方式用最新的记忆替换掉最老的记忆。就像在学习围棋的过程中，有初学者阶段的对局记忆，也有高手阶段的对局记忆，在提升棋艺的角度来看，高手阶段的记忆显然比初学者阶段的记忆更有价值。</p><p>说句题外话，其实对于一个民族而言也是一样的。我们这个民族拥有一个非常好的传统，就是记述历史，也就是等于我们这个民族拥有足够大的记忆量，这是我们胜于其他民族的。但是这个历史记录中，掺杂了历史上不同阶段的评价，这些评价是根据当时的经验得出的。而根据DQN的算法描述来看，对我们最有价值的部分其实是原始信息，而不是那些附加在之上的评价，这些评价有正确的部分，也有错误的部分，我们不用去过多关心。我们只需要在今天的认知（也就是最新的训练结果）基础上，对历史原始信息（旧状态、动作、新状态、单步奖励）进行随机的抽样分析即可。</p><h2 id="网络稳定性"><a href="#网络稳定性" class="headerlink" title="网络稳定性"></a>网络稳定性</h2><p>DQN另一个稳定性问题与目标值计算有关。因为`target = r + gamma * argmax Q(s’)`，所以目标值与网络参数本身是相关，而参数在训练中是不断变化的，所以这会造成训练中的不稳定。一个神经网络可以自动收敛，取决于存在一个稳定的目标，如果目标本身在不断的游移变动，那么想要达到稳定就比较困难。这就像站在平地上的人很容易平衡，但如果让人站在一个不断晃动的木板上，就很难达到平衡。为了解决这个问题，我们需要构建一个稳定的目标函数。</p><p>解决的方法是采用两个网络代替一个网络。一个网络用于训练调整参数，称之为策略网络，另一个专门用于计算目标，称之为目标网络。目标网络与策略网络拥有完全一样的网络结构，在训练的过程中目标网络的参数是固定的。执行一小批训练之后，将策略网络最新的参数复制到目标网络中。</p><p><img src="/img/rl-3/3.png" alt="目标网络"></p><p>经验回放和目标网络的效果见下表（引用自Nature 论文）：<br><img src="/img/rl-3/5.png" alt="优化对比"></p><h2 id="其他DQN优化"><a href="#其他DQN优化" class="headerlink" title="其他DQN优化"></a>其他DQN优化</h2><p>关于DQN的优化，这篇文章描述的比较全面 <a href="https://zhuanlan.zhihu.com/p/21547911" target="_blank" rel="noopener">https://zhuanlan.zhihu.com/p/21547911</a>。在之后的实践中考虑是否进一步深入。主要介绍3个改进：</p><p><img src="/img/rl-3/6.png" alt="DQN优化"></p><ul><li>Double DQN：对目标值计算的优化，a’使用策略网络选择的动作来代替目标网络选择的动作。</li><li>Prioritised replay：使用优先队列（priority queue）来存储经验，避免丢弃早期的重要经验。使用error作为优先级，仿生学技巧，类似于生物对可怕往事的记忆。</li><li>Dueling Network：将Q网络分成两个通道，一个输出V，一个输出A，最后再合起来得到Q。如下图所示（引用自Dueling Network论文）。这个方法主要是idea很简单但是很难想到，然后效果一级棒，因此也成为了ICML的best paper。</li></ul><p><img src="/img/rl-3/7.png" alt="Dueling Network"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;上一个例子中我们使用了冰面滑行的游戏环境，这是一个有限状态空间的环境。如果我们现在面对的是一个接近无限的状态空间呢？比如围棋，比如非离散型的问题。我们无法使用一个有限的Q表来存储所有Q值，即使有足够的存储空间，也没有足够的计算资源来遍历所有的状态空间。对于这一类问题，深度学
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（二）：Q-learning实战</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-2/</id>
    <published>2019-12-27T08:02:19.000Z</published>
    <updated>2019-12-27T10:37:57.211Z</updated>
    
    <content type="html"><![CDATA[<p>我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用<code>gym</code>来作为我们对实验环境。安装方法：</p><figure class="highlight shell"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">pip install gym</span><br></pre></td></tr></table></figure><h2 id="初识环境"><a href="#初识环境" class="headerlink" title="初识环境"></a>初识环境</h2><p>我们的实验环境是一个冰湖滑行游戏，你将控制一个agent在冰面到达目标终点，前进方向并不总受你的控制，你还需要躲过冰窟。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"><span class="comment"># 构造游戏环境</span></span><br><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line"><span class="comment"># 动作空间-&gt; Discrete(4)</span></span><br><span class="line">print(env.action_space)</span><br><span class="line"><span class="comment"># 状态空间-&gt; Discrete(16)</span></span><br><span class="line">print(env.observation_space)</span><br><span class="line"><span class="comment"># 初始化游戏环境，并得到状态s</span></span><br><span class="line">s = env.reset()</span><br><span class="line"><span class="keyword">for</span> _ <span class="keyword">in</span> range(<span class="number">10</span>):</span><br><span class="line">    <span class="comment"># 渲染游戏画面</span></span><br><span class="line">    env.render()</span><br><span class="line">    <span class="comment"># 从动作空间中随机选择一个动作a</span></span><br><span class="line">    a = env.action_space.sample()</span><br><span class="line">    <span class="comment"># 执行动作a，得到新状态s，奖励r，是否完成done</span></span><br><span class="line">    s, r, done, info = env.step(a) <span class="comment"># take a random action</span></span><br><span class="line">    print(s, r, done, info)</span><br><span class="line"><span class="comment"># 关闭环境</span></span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>游戏画面示意如下：</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">SFFF       (S: 起点，安全)</span><br><span class="line">FHFH       (F: 冰面，安全)</span><br><span class="line">FFFH       (H: 冰窟，进入则失败)</span><br><span class="line">HFFG       (G: 终点，到达则成功)</span><br></pre></td></tr></table></figure><h2 id="Agent结构"><a href="#Agent结构" class="headerlink" title="Agent结构"></a>Agent结构</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> random</span><br><span class="line"><span class="keyword">import</span> math</span><br><span class="line"><span class="keyword">import</span> gym</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">QLAgent</span><span class="params">()</span>:</span></span><br><span class="line">    q = <span class="keyword">None</span></span><br><span class="line">    action_space = <span class="keyword">None</span></span><br><span class="line">    epsilon = <span class="number">0.1</span> <span class="comment"># 探索率</span></span><br><span class="line">    gamma = <span class="number">0.99</span>  <span class="comment"># 衰减率</span></span><br><span class="line">    lr = <span class="number">0.1</span> <span class="comment"># 学习率</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self, action_space, state_count, epsilon=<span class="number">0.1</span>, lr=<span class="number">0.1</span>, gamma=<span class="number">0.99</span>)</span>:</span></span><br><span class="line">        self.q = [[<span class="number">0.</span> <span class="keyword">for</span> a <span class="keyword">in</span> range(action_space.n)] <span class="keyword">for</span> s <span class="keyword">in</span> range(state_count)]</span><br><span class="line">        self.action_space = action_space</span><br><span class="line">        self.epsilon = epsilon</span><br><span class="line">        self.lr = lr</span><br><span class="line">        self.gamma = gamma</span><br><span class="line"></span><br><span class="line">    <span class="comment"># 根据状态s，选择动作a</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br><span class="line"></span><br><span class="line">    <span class="comment"># 更新状态变化并学习，状态s执行了a动作，得到了奖励r，状态转移到了s_</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">        <span class="keyword">pass</span></span><br></pre></td></tr></table></figure><p>这是一个Agent的一般结构，主要由初始化、选择动作、更新状态变化，三个方法构成。后续的其他算法将依然采用该结构。q表数据使用一个二维数组表示，其大小为 state_count action_count，对于这个项目而言是一个 `16*4` 的大小。</p><h2 id="添加Q-table的辅助方法"><a href="#添加Q-table的辅助方法" class="headerlink" title="添加Q-table的辅助方法"></a>添加Q-table的辅助方法</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 返回状态s的最佳动作a、及其r值。</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">argmax</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    max_r = -math.inf</span><br><span class="line">    max_a = <span class="keyword">None</span></span><br><span class="line">    <span class="keyword">for</span> a <span class="keyword">in</span> range(self.action_space.n):</span><br><span class="line">        r = self.q_get(s, a)</span><br><span class="line">        <span class="keyword">if</span> r &gt; max_r:</span><br><span class="line">            max_a = a</span><br><span class="line">            max_r = r</span><br><span class="line">    <span class="keyword">return</span> max_a, max_r</span><br><span class="line"><span class="comment"># 获得 状态s，动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_get</span><span class="params">(self, s, a)</span>:</span></span><br><span class="line">    <span class="keyword">return</span> self.q[s][a]</span><br><span class="line"><span class="comment"># 更新 状态s 动作a 对应的r值</span></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">q_put</span><span class="params">(self, s, a, v)</span>:</span></span><br><span class="line">    self.q[s][a] = v</span><br></pre></td></tr></table></figure><h2 id="Q-learning的关键步骤"><a href="#Q-learning的关键步骤" class="headerlink" title="Q-learning的关键步骤"></a>Q-learning的关键步骤</h2><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">choose_action</span><span class="params">(self, s)</span>:</span></span><br><span class="line">    <span class="keyword">if</span> random.random() &lt; self.epsilon:</span><br><span class="line">        <span class="comment"># 按一定概率进行随机探索</span></span><br><span class="line">        <span class="keyword">return</span> self.action_space.sample()</span><br><span class="line">    <span class="comment"># 返回最佳动作</span></span><br><span class="line">    a, _ = self.argmax(s)</span><br><span class="line">    <span class="keyword">return</span> a</span><br><span class="line"></span><br><span class="line"><span class="function"><span class="keyword">def</span> <span class="title">update_transition</span><span class="params">(self, s, a, r, s_)</span>:</span></span><br><span class="line">    q = self.q_get(s, a)</span><br><span class="line">    _, r_ = self.argmax(s_)</span><br><span class="line">    <span class="comment"># Q &lt;- Q + a(Q' - Q)</span></span><br><span class="line">    <span class="comment"># &lt;=&gt; Q &lt;- (1-a)Q + a(Q')</span></span><br><span class="line">    q = (<span class="number">1</span>-self.lr) * q + self.lr * (r + self.gamma * r_)</span><br><span class="line">    self.q_put(s, a, q)</span><br></pre></td></tr></table></figure><h2 id="训练主循环"><a href="#训练主循环" class="headerlink" title="训练主循环"></a>训练主循环</h2><p>我们进行10000局游戏的训练，每局游戏执行直到完成。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br></pre></td><td class="code"><pre><span class="line">env = gym.make(<span class="string">'FrozenLake-v0'</span>)</span><br><span class="line">agent = QLAgent(env.action_space, env.observation_space.n)</span><br><span class="line"><span class="keyword">for</span> epoch <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># env.render()  # 训练过程不需要渲染</span></span><br><span class="line">        a = agent.choose_action(s) <span class="comment"># 选择动作</span></span><br><span class="line">        s_, r, done, info = env.step(a) <span class="comment"># 执行动作</span></span><br><span class="line">        agent.update_transition(s, a, r, s_) <span class="comment"># 更新状态变化</span></span><br><span class="line">        s = s_</span><br><span class="line"><span class="comment"># 显示训练后的Q表</span></span><br><span class="line">print(agent.q)</span><br></pre></td></tr></table></figure><h2 id="测试效果"><a href="#测试效果" class="headerlink" title="测试效果"></a>测试效果</h2><p>在测试中，我们只选择最佳策略，不再探索，也不再更新Q表。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 获胜次数</span></span><br><span class="line">total_win = <span class="number">0</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(<span class="number">10000</span>):</span><br><span class="line">    s = env.reset()</span><br><span class="line">    done = <span class="keyword">False</span></span><br><span class="line">    <span class="keyword">while</span> <span class="keyword">not</span> done:</span><br><span class="line">        <span class="comment"># 选择最佳策略</span></span><br><span class="line">        a, _ = agent.argmax(s)</span><br><span class="line">        <span class="comment"># 执行动作 a</span></span><br><span class="line">        s_, r, done, info = env.step(a)</span><br><span class="line">        <span class="keyword">if</span> done <span class="keyword">and</span> r == <span class="number">1</span>:</span><br><span class="line">            total_win += <span class="number">1</span></span><br><span class="line">        s = s_</span><br><span class="line">print(<span class="string">'Total win='</span>, total_win)</span><br><span class="line">env.close()</span><br></pre></td></tr></table></figure><p>最终测试的效果是在1万局中获胜了7284次，说明达到了不错的实验效果。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;我们现在通过一个例子来演练Q-learning算法。在练习强化学习之前，我们需要拥有一个模拟器环境。我们主要的目的是学习编写机器人算法，所以对模拟器环境部分不推荐自己写。直接使用&lt;code&gt;gym&lt;/code&gt;来作为我们对实验环境。安装方法：&lt;/p&gt;
&lt;figure cla
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>强化学习简介（一）：Q-learning</title>
    <link href="http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/"/>
    <id>http://guileen.github.io/2019/12/27/introduce-reinforcement-learning-1/</id>
    <published>2019-12-27T03:28:16.000Z</published>
    <updated>2019-12-27T09:57:48.329Z</updated>
    
    <content type="html"><![CDATA[<p>完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。</p><p><img src="/img/rl-1/1.png" alt="强化学习图示"></p><h2 id="K摇臂赌博机"><a href="#K摇臂赌博机" class="headerlink" title="K摇臂赌博机"></a>K摇臂赌博机</h2><p>我们可以考虑一个最简单的环境：一个动作可立刻获得奖励，目标是使每一个动作的奖励最大化。对这种单步强化学习任务，可以设计一个理论模型——“K-摇臂赌博机”。这个赌博机有K个摇臂，赌徒在投入一个硬币后可一选择按下一个摇臂，每个摇臂以一定概率吐出硬币，但这个概率赌徒并不知道。赌徒的目标是通过一定的策略最大化自己的奖赏，即获得最多的硬币。最终所获得的总奖励被称为累计奖励。</p><p><img src="/img/rl-1/2.jpg" alt="K摇臂赌博机"></p><p>对于这个简单模型，若要知道每个摇臂的概率，我们只需要进行足够多的尝试即可，这是“仅探索”策略；若要奖励最大化，则需要执行奖赏概率最大的动作即可，这是“仅利用”策略。但在更复杂的环境中，我们不可能对每一个状态的每个动作都进行足够多的探索。比如围棋，我们后续的探索是需要依赖于之前的探索的。因此我们需要在探索和利用之间进行平衡。我们在学习的过程中，必须要保持一定的概率`epsilon`进行探索，其余时候则执行学习到的策略。</p><h2 id="基本概念术语"><a href="#基本概念术语" class="headerlink" title="基本概念术语"></a>基本概念术语</h2><p>为了便于分析讨论，我们定义一些术语。</p><ul><li><p>机器agent处于环境`E`中。</p></li><li><p>状态空间为`S`，每个状态`s in S`是机器感知到的环境描述。</p></li><li><p>机器能够采取的可采取的动作a的集合即动作空间`A`，`a in A`。</p></li><li><p>转移函数`P`表示：当机器执行了一个动作`a`后，环境有一定概率从状态`s`改变为新的状态`s’`。即：`s’=P(s, a)`</p></li><li><p>奖赏函数`R`则表示了执行动作可能获得的奖赏`r`。即：`r=R(s,a)`。</p></li><li><p>环境可以描述为`E=&lt;&lt;S, A, P, R&gt;&gt;`。</p></li><li><p>强化学习的任务是习得一个策略（policy）`pi`，使机器在状态`s`下选择到最佳的`a`。策略有两种描述方法：</p><ol><li>`a=pi(s)` 表示状态`s`下将执行动作`a`，是一种确定性的表示法。</li><li>`pi(s, a)` 表示状态`s`下执行动作`a`的概率。这里有 `sum_a pi(s,a)=1`</li></ol></li><li><p>累计奖励指连续的执行一串动作之后的奖励总和。</p></li><li><p>`Q^(pi)(s, a)`表示在状态`s`下，执行动作`a`，再策略`pi`的累计奖励。为方便讨论后续直接写为`Q(s,a)`。</p></li><li><p>`V^(pi)(s)` 表示在状态`s`下，使用策略`pi`的累计奖励。为方便讨论后续直接写为`V(s)`。</p></li></ul><p>强化学习往往不会立刻得到奖励，而是在很多步之后才能得到一个诸如成功/失败的奖励，这是我们的算法需要反思之前所有的动作来学习。所以强化学习可以视作一种延迟标记的监督学习。</p><h2 id="Q-learning"><a href="#Q-learning" class="headerlink" title="Q-learning"></a>Q-learning</h2><p>对于我们要学习的`Q(s,a)`函数，我们可以使用一个Q-table来表示。Q-table是一个二维表，记录每个状态`s in S, a in A`的`Q`值。Q表被初始化为0的状态。在状态`s`执行了动作`a`之后，得到状态`s’`，奖励`r`。我们将潜在的`Q`函数记为`Q_(real)`，其值为当前奖励r与后续状态`s’`的最佳累计奖励之和。则有：</p><p>` Q_(real)(s, a) = r + gamma * argmax_aQ(s’, a) `<br>` err = Q_(real)(s, a) - Q(s, a) `</p><p>其中`gamma`为`Q`函数的偏差，`err`为误差，`alpha`为学习率。 可得出更新公式为：</p><p>` Q(s, a) leftarrow Q(s, a) + alpha*err `<br>即：<br>` Q(s,a) leftarrow (1-alpha)Q(s,a) + alpha(r + gamma * argmax_aQ(s’, a)) `</p><p>以上公式即为Q-learning的关键</p><h3 id="算法描述"><a href="#算法描述" class="headerlink" title="算法描述"></a>算法描述</h3><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># 初始化Q表</span></span><br><span class="line"><span class="keyword">for</span> s <span class="keyword">in</span> S:</span><br><span class="line">  <span class="keyword">for</span> a <span class="keyword">in</span> A:</span><br><span class="line">    Q(s, a) = <span class="number">0</span></span><br><span class="line"><span class="comment"># 训练过程</span></span><br><span class="line"><span class="keyword">for</span> i <span class="keyword">in</span> range(N):</span><br><span class="line">  <span class="keyword">if</span> rand() &lt; epsilon:</span><br><span class="line">    a = 从 A 中随机选取一个动作</span><br><span class="line">  <span class="keyword">else</span>:</span><br><span class="line">    a = 从 A 中选取使 Q(a) 最大的a</span><br><span class="line">  <span class="comment"># 从环境中获得反馈</span></span><br><span class="line">  r = R(s, a)</span><br><span class="line">  s1 = P(s, a)</span><br><span class="line">  <span class="comment"># 更新Q表</span></span><br><span class="line">  Q(s,a) = (<span class="number">1</span>-alpha)*Q(s,a) + alpha * (r + gamma * argmax Q(s1, a) - Q(s, a))</span><br><span class="line">  s = s1</span><br></pre></td></tr></table></figure><p>下图是一个Q表内存结构，在经过一定的学习后，Q表的内容将能够不断逼近每个状态的每个动作的累计收益。<br><img src="/img/rl-1/3.png" alt="Q-table"></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;完成图像分类之类的监督学习任务，已经没有特别大的难度。我们希望AI能够帮助我们处理一些更加复杂的任务，比如：下棋、玩游戏、自动驾驶、行走等。这一类的学习任务被称为强化学习。&lt;/p&gt;
&lt;p&gt;&lt;img src=&quot;/img/rl-1/1.png&quot; alt=&quot;强化学习图示&quot;&gt;&lt;/p
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>2019年度回顾：新思潮的前夜</title>
    <link href="http://guileen.github.io/2019/12/25/2019-new-thinking/"/>
    <id>http://guileen.github.io/2019/12/25/2019-new-thinking/</id>
    <published>2019-12-25T02:38:44.000Z</published>
    <updated>2019-12-26T10:05:58.289Z</updated>
    
    <content type="html"><![CDATA[<p>过去一年，事业上没多少进步，思想上有贯通之感。这些“无用之学”是一年的无所事事换来，所以需要记录一下。</p><p>今年不再听得到，改听微信读书。办了张图书馆的借书证。主要知识来源：书、知乎、B站。排序与阅读顺序无关</p><p>与同学论战，论战过程中不断提高自己的理论水平，发现有很多东西虽然有概念，如果缺乏实证没有说服力。世界银行提供了很多经济数据，很有价值，如GDP、PPP、Gini系数等。</p><p>读冯友兰的《中国哲学史》，思考中国哲学所具有的普世意义。温习《庄子》《墨子》的部分内容。尝试论证人生的意义，一个可以被自己接受的人生意义。</p><p>B站看秦晖讲的《中国哲学史》，其以群己界限、乡举里选、儒法斗争为叙述线索，似有可取之处，但其对民族性论述不以为然。新文化运动以来诸如胡适、鲁迅，皆有可取之处，却其对民族性的贬低又有精日、西奴之嫌。</p><p>读了《夏禹神话研究》。了解相关知识，对五帝、夏禹历史尝试进行分析，梳理一个叙事结构。录了几个关于五帝抖音视频，暂停了。</p><p>进一步了解了地理环境决定论，读《枪炮病菌与钢铁》。</p><p>了解加州学派的主要观点，读了《白银资本》《火枪与账簿》全球史视角下的东西方对比（明清时期）。</p><p>了解了明史，尤其是晚明史，读了《万历十五年》，《剑桥中国明代史》，B站看张西平、商传等学者的视频，重新认识了明朝。</p><p>了解东林党的历史和评价，褒贬不一。猜想：东林岳麓两家书院，传承中华文明，他们的学生从明末到民国，风格迥异。常凯申是江浙人，属于东林一脉，毛主席是湖南人，属于岳麓一脉。</p><p>了解了中学西传，启蒙运动与中国哲学的关系。西方近代哲学主要发端于启蒙运动，那么西方哲学到底是继承自中学西传，还是继承自基督教，还是继承自古希腊？这个问题引人深思。</p><p>读了白云先生的文章，虽有偏激之辞，但格局极大，有打通任督二脉之感。尤其是关于韩愈-陈抟-朱熹-宋明理学-王夫之-曾国藩、杨昌济、毛泽东的中华道统传承的论述。</p><p>了解王夫之生平，对此先贤之前无甚了解。他的《读通鉴论》看了一节。钦佩。读《曾国藩传》。</p><p>读《天朝的崩溃》，思考满清的农奴式统治的影响，新文化运动批评的民族性，其实是满遗余孽。</p><p>在知乎上回答了李约瑟难题，基本认为李约瑟难题是个伪命题。</p><p>读了《毛泽东自传》《毛泽东传》，一个越了解越觉得伟大的人，千古无二，真神，远胜佛祖、摩西之流。相信毛主义会再次回归神坛。</p><p>读了温铁军的《八次危机》，认识到前三十年的巨大成就和困难，以及后三十年的巨大经济与社会问题。</p><p>在读《临高启明》，一群工业党写的。了解现代国家形成的过程，工业化的巨大困难，以及工业化完成后将带来财富激增。</p><p>看了《无悔追踪》《一年又一年》《天道》这三部连续剧可以串起中国几十年民众的生活状态。</p><p>读了《战后日本经济史》，德国、苏联、日本 的崛起都有社会主义/国家资本主义的原因，与自由市场经济无关。而那些鼓吹自由市场理论的，都是在完成工业化后，才开始采用自由市场，工业化前都有压榨式的原始积累过程。上学时没学好马哲，现在要补习。</p><p>读了《大国悲剧：苏联解体的前因后果》，看了HBO电视剧《切尔诺贝利》。</p><p>了解中国在冷战中的地位与策略。战后历史就是中美苏的斗争历史，其他国家忙着搭便车。</p><p>看了《切腹》《寄生虫》《美国工厂》这几部电影，无产阶级无处不在啊。</p><h2 id="今年"><a href="#今年" class="headerlink" title="今年"></a>今年</h2><p>华为被视为民族英雄，联想被视作美帝买办。</p><p>非洲猪瘟，猪肉价格上涨。</p><p>香港动乱。</p><p>王健林内贷外投，转移财产。</p><p>国庆大阅兵，民心振奋。</p><p>华为胡玲发帖、251事件。</p><p>抖音上忽然走红 沈巍（流浪的大师）、李子柒（世外桃源）、牧马人（文革晚期、改革初期的淳朴生活）。</p><p>中美贸易战接近收官。</p><p>世界各地区社会运动不断。</p><h2 id="未来"><a href="#未来" class="headerlink" title="未来"></a>未来</h2><p>我们正站在世界巨变的前夜，这个巨变将以我们意向不到的方式出现。</p><p>美国会倒下，但我们不知道美国将会如何倒下。中美最根本的战争在金融领域，人民币国际化就是在争夺全球铸币权份额。美元可能会崩溃，但中国拥有的大量美元资产需要安全置换。如果美元霸权能够缓缓的衰落，对中国的影响最小，如果美元霸权骤然终结，也将对中国带来冲击。</p><p>在现在的国际环境下，依靠出口拉动经济的方式是不可持续的，最重要的是提振内需，但目前老百姓手里没钱，内需也拉不动。所以现在必须要平衡贫富差距。缩小贫富差距就要有人掏钱，因此有钱的人都想着抽逃资金。严打不仅稳定社会治安，也会收回大量不法收入。</p><p>房价不会大涨也不会大跌，上涨会让还没买房的人不满意，下跌会让已经买了房的人不满意，所以就锁住，不涨不跌。房子已经进入了计划时代，就像火车票必须凭身份证购买来抑制黄牛是一样的。信息化使计划经济具有了更大的可行性。即使通货膨胀了，房价也不会涨，因为已经不再是市场化环境了。所以本质上，房价在缓缓下跌。</p><p>需要优化财富分配，医疗养老之类的福利会加强，教育科研投入会加大，贫困人口持续扶贫。</p><p>大量高科技依然掌握在欧美手中。中国的普及教育很好，但是高等教育与欧美差距较大，教育改革也是困难重重。仅从本人专业来看，编程语言、操作系统、深度学习框架，都是美国人的。</p><p>生产过剩、人工智能，会影响就业率。新增人口下滑，老龄化加速。</p><p>创业会越来越难，基本上属于解决就业的公益事业。体制内、大平台的职工相当于新的铁饭碗，同时临时工也会越来越多。</p><p>创业的最佳阶段是GDP高速增长的阶段，只要入局基本都有得赚。如果宏观上的财富没有增加，那么创业相当于是在存量市场与其他人竞争。除非有压倒性的武器，找到可以战胜的敌人，否则不要创业。海外市场机会较大。</p><p>创新是有闲阶级特权，有闲不一定是非常富有，但需要有一份保证基本生活的、时间投入少的收入，即“睡后收入”。如果创新可立刻获得回报，则可以进入增强回路循环。</p><p>随着机器人取代越来越多的低端就业，UBI即无条件基本收入的概念会被越来越多的人接受。今年参加美国总统竞选的杨安泽使用了UBI的口号，别人问他钱从哪来，他说向富人收税。桑德斯再次参加大选，美国民主社会主义近年快速崛起。</p><p>传统发达国家因为后发国家的追赶，导致本国产业受到影响，就业率下滑。从而产生越来越多的社会运动，希望获得更高的社会福利，有的地方甚至为了几毛钱的地铁涨价就闹了起来。但这些国家由于债务压力已经很大，经济环境恶化，无法给到更多的社会福利。富人们则利用国际避税手段，来躲避社会责任。一场全球性的萧条和左派革命正在酝酿，那些小政府的发达地区问题最严重（比如香港、韩国是小政府、新加坡是大政府）。</p><p>中美的竞争不会转化为热战，因为中国不想打，美国打不赢。中美握手后，将联手打击国际避税，共治天下。</p><p>资本因其可以转移，在国际共运中，虽然一部分资产被没收，但更多的则逃到了避风港中。苏联解体、改革开放也使大量的公有资产再次被私有化。美国衰落将使资本最大的避风港消失，所有的资本都将处于监管之下。</p><p>中国国内思想分歧增大。近几十年的高速增长，右派认为前人有罪，自己有功，而问题则留给后人处理，大力宣传，所以右派思想居主流。近十年文革一代领导人上台，左派思想逐渐兴起，加之贫富差距、环保、贪腐等问题的存在，助推了左派思潮的壮大。我也是因为近几年的反腐、扶贫、强军才开始重新认识这个国家。</p><p>如果说这一代领导人的思想是在文革中形成的话，那么下一届领导人的思想又是在何时形成的呢？文革给中国续了命，修正主义则给苏联送了命。戈尔巴乔夫说“我们是苏共二十大的孩子，苏联六十年代的历史对我们影响很大”，六十年代的苏联就相当于八十年代的中国，赫鲁晓夫教出了戈尔巴乔夫，戈尔巴乔夫一手送走了苏联。下一届领导人是左还是右？改革开放中形成的利益集团是否已经可以左右政治格局，形成类似日韩的财阀统治？这是未来最大的不确定因素。</p><p>正因为这是未来最大的不确定因素，所以伟人在晚年才要发动文革。不断受人非议的文革，完成了一代人的政治教育，奠定了一代人的思想基础，顶住了世界范围的社会主义颠覆潮。伟人看得太远了，我们无法企及。但是我们虽然顶住了颠覆，却也经历了改革，大量国有资产被私有化，形成了利益集团。几十年过去了，人心不古。既得利益者一定会想方设法垄断政治权力，不加控制结局与明朝无异。解决方案伟人都说过了，而我们首先要做的就是正确评价文革。反对文革的人常拿文革中的极端案例来举例，犯罪案件每个时代都有，我们要注意到文革时期的犯罪率是远低于改革之后的，即便把群众运动中极端案件算上，也比改革之后的犯罪率低。就连当时“受迫害”的人都没有反对文革（比如现任领导人），那些不了解文革的人又凭什么反对呢？妖魔化文革的本质就是妖魔化群众运动，从而顺理成章的剥夺了民众的政治权力。</p><p>扯远了，看来可以写的主题有很多。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;过去一年，事业上没多少进步，思想上有贯通之感。这些“无用之学”是一年的无所事事换来，所以需要记录一下。&lt;/p&gt;
&lt;p&gt;今年不再听得到，改听微信读书。办了张图书馆的借书证。主要知识来源：书、知乎、B站。排序与阅读顺序无关&lt;/p&gt;
&lt;p&gt;与同学论战，论战过程中不断提高自己的理论
      
    
    </summary>
    
      <category term="随笔" scheme="http://guileen.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Torch的损失函数和优化器</title>
    <link href="http://guileen.github.io/2019/12/24/torch-output-loss-optimizer/"/>
    <id>http://guileen.github.io/2019/12/24/torch-output-loss-optimizer/</id>
    <published>2019-12-24T14:05:59.000Z</published>
    <updated>2019-12-25T03:43:30.327Z</updated>
    
    <content type="html"><![CDATA[<p>深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。</p><h1 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h1><p>pytorch 损失函数的基本用法</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = LossCriterion(参数)</span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure><p>Mean Absolute Error<br>torch.nn.L1Loss<br>Measures the mean absolute error.</p><h2 id="Mean-Absolute-Error-L1Loss"><a href="#Mean-Absolute-Error-L1Loss" class="headerlink" title="Mean Absolute Error/ L1Loss"></a>Mean Absolute Error/ L1Loss</h2><p>nn.L1Loss<br><img src="/img/loss/l1loss.png" alt=""><br>很少使用</p><h2 id="Mean-Square-Error-Loss"><a href="#Mean-Square-Error-Loss" class="headerlink" title="Mean Square Error Loss"></a>Mean Square Error Loss</h2><p>nn.MSELoss<br><img src="/img/loss/mseloss.png" alt=""><br>针对数值不大的回归问题。</p><h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><p>nn.SmoothL1Loss<br><img src="/img/loss/smoothl1loss.png" alt=""><br>它在绝对差值大于1时不求平方，可以避免梯度爆炸。大部分回归问题都可以适用，尤其是数值比较大的时候。</p><h2 id="Negative-Log-Likelihood-Loss"><a href="#Negative-Log-Likelihood-Loss" class="headerlink" title="Negative Log-Likelihood Loss"></a>Negative Log-Likelihood Loss</h2><p>torch.nn.NLLLoss，一般与 LogSoftmax 成对使用。使用时 <code>loss(softmaxTarget, target)</code>。用于处理多分类问题。<br><img src="/img/loss/nllloss.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line"><span class="comment"># input is of size N x C = 3 x 5， C为分类数</span></span><br><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># each element in target has to have 0 &lt;= value &lt; C</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>nn.CrossEntropyLoss 将 LogSoftmax 和 NLLLoss 绑定到了一起。所以无需再对结果使用Softmax</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)</span><br><span class="line">target &#x3D; torch.empty(3, dtype&#x3D;torch.long).random_(5)</span><br><span class="line">output &#x3D; loss(input, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h2><p>二分类问题的CrossEntropyLoss。输入、目标结构是一样的。</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line">input = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure><h2 id="Margin-Ranking-Loss"><a href="#Margin-Ranking-Loss" class="headerlink" title="Margin Ranking Loss"></a>Margin Ranking Loss</h2><p><img src="/img/loss/marginrankingloss.png" alt=""></p><p>常用户增强学习、对抗生成网络、排序任务。给定输入x1，x2，y的值是1或-1，如果y==1表示x1应该比x2的排名更高，y==-1则相反。如果y值与x1、x2顺序一致，那么loss为0，否则错误为 y*(x1-x2)</p><h2 id="Hinge-Embedding-Loss"><a href="#Hinge-Embedding-Loss" class="headerlink" title="Hinge Embedding Loss"></a>Hinge Embedding Loss</h2><p>y的值是1或-1，用于衡量两个输入是否相似或不相似。</p><h2 id="Cosine-Embedding-Loss"><a href="#Cosine-Embedding-Loss" class="headerlink" title="Cosine Embedding Loss"></a>Cosine Embedding Loss</h2><p>给定两个输入x1，x2，y的值是1或-1，用于衡量x1和x2是否相似。<br><img src="/img/loss/cosineembeddingloss.png" alt=""><br>其中cos(x1, x2)表示相似度<br><img src="/img/loss/cossim.png" alt=""></p><h1 id="各种优化器"><a href="#各种优化器" class="headerlink" title="各种优化器"></a>各种优化器</h1><p>大多数情况Adam能够取得比较好的效果。SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 所以说并不是越先进的优化器, 结果越佳.</p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD 就是随机梯度下降</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line"><span class="comment"># momentum 动量加速,在SGD函数里指定momentum的值即可</span></span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># RMSprop 指定参数alpha</span></span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># Adam 参数betas=(0.9, 0.99)</span></span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。&lt;/p&gt;
&lt;h1 id=&quot;常用损失函数&quot;&gt;&lt;a href=&quot;#常用损失函数&quot; class=&quot;headerlink&quot; title=&quot;常用损失函数&quot;&gt;&lt;
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>理解CNN参数及PyTorch实例</title>
    <link href="http://guileen.github.io/2019/12/24/understanding-cnn/"/>
    <id>http://guileen.github.io/2019/12/24/understanding-cnn/</id>
    <published>2019-12-24T07:56:21.000Z</published>
    <updated>2019-12-25T03:43:17.534Z</updated>
    
    <content type="html"><![CDATA[<p>本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。</p><h2 id="卷积核-kernel"><a href="#卷积核-kernel" class="headerlink" title="卷积核 kernel"></a>卷积核 kernel</h2><p>Kernel，卷积核，有时也称为filter。在迭代过程中，学习的结果就保存在kernel里面。深度学习，学习的就是一个权重。kernel的尺寸越小，计算量越小，一般选择3x3，更小就没有意义了。<br><img src="/img/cnn/kernel_2.png" alt=""></p><p>结果是对卷积核与一小块输入数据的点积。</p><h2 id="层数-Channels"><a href="#层数-Channels" class="headerlink" title="层数 Channels"></a>层数 Channels</h2><p><img src="/img/cnn/channel_1.png" alt=""></p><p>所有位置的点积构成一个激活层。</p><p><img src="/img/cnn/channel_2.png" alt=""></p><p>如果我们有6个卷积核，我们就会有6个激活层。</p><h2 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h2><p><img src="/img/cnn/kernel.gif" alt=""><br>上图是每次向右移动一格，一行结束向下移动一行，所以stride是1x1，如果是移动2格2行则是2x2。</p><h2 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h2><p>Padding的作用是为了获取图片上下左右边缘的特征。<br><img src="/img/cnn/pad.jpg" alt=""></p><h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>卷积层为了提取特征，但是卷积层提取完特征后特征图层依然很大。为了减少计算量，我们可以用padding的方式来减小特征图层。Pooling的方法有MaxPooling核AveragePooling。<br><img src="/img/cnn/pooling.jpg" alt=""></p><p>推荐看一下李飞飞的<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf" target="_blank" rel="noopener">这篇slide</a></p><h2 id="PyTorch-中的相关方法"><a href="#PyTorch-中的相关方法" class="headerlink" title="PyTorch 中的相关方法"></a>PyTorch 中的相关方法</h2><ul><li><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=’zeros’)</p></li><li><p>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</p><ul><li>stride 默认与kernel_size相等</li></ul></li><li><p>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</p></li><li><p>Tensor.view(*shape) -&gt; Tensor</p><ul><li>用于将卷积层展开为全连接层<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; torch.randn(4, 4)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y &#x3D; x.view(16)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z &#x3D; x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br></pre></td></tr></table></figure></li></ul></li></ul><h2 id="MNIST例子"><a href="#MNIST例子" class="headerlink" title="MNIST例子"></a>MNIST例子</h2><p>MNIST 数据集的输入是 1x28x28 的数据集。在实际开发中必须要清楚每一次的输出结构。</p><ul><li>我们第一层使用 5x5的卷积核，步长为1，padding为0，28-5+1 = 24，那么输出就是 24x24。计算方法是 (input_size - kernel_size)/ stride + 1。</li><li>我们第二层使用 2x2的MaxPool，那么输出为 12x12.</li><li>第三层再使用5x5，卷积核，输出则为 12-5+1，即 8x8。</li><li>再使用 2x2 MaxPool，输出则为 4x4。</li></ul><p><img src="/img/cnn/mnist_convet.png" alt=""></p><figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""ConvNet -&gt; Max_Pool -&gt; RELU -&gt; ConvNet -&gt; Max_Pool -&gt; RELU -&gt; FC -&gt; RELU -&gt; FC -&gt; SOFTMAX"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure><p>以上代码摘自 <a href="https://github.com/floydhub/mnist" target="_blank" rel="noopener">https://github.com/floydhub/mnist</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。&lt;/p&gt;
&lt;h2 id=&quot;卷积核-kernel&quot;&gt;&lt;a href=&quot;#卷积核-kernel&quot; class=&quot;headerlink&quot; title=&quot;卷积
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>AlphaGo Zero 工作原理</title>
    <link href="http://guileen.github.io/2019/11/01/alpha-go-zero/"/>
    <id>http://guileen.github.io/2019/11/01/alpha-go-zero/</id>
    <published>2019-10-31T17:00:00.000Z</published>
    <updated>2019-12-25T03:43:42.853Z</updated>
    
    <content type="html"><![CDATA[<p>本文写于2017年12月，获<a href="https://zhuanlan.zhihu.com/p/32952677" target="_blank" rel="noopener">Udacity专栏转载</a>。今将其搬运至我的博客。</p><p>2016年3月，Alpha Go Master击败最强的人类围棋选手之一李世石。击败李的版本，在训练过程中使用了大量人类棋手的棋谱。2017年10月19日，DeepMind公司在《自然》杂志发布了一篇新的论文，AlphaGo Zero——它完全不依赖人类棋手的经验，经过3天的训练，Alpha Go Zero击败了Master版本。AlphaGo Zero最重要的价值在于，它不仅仅可以解决围棋问题，它可以在不需要知识预设的情况下，解决一切棋类问题，经过几个小时的训练，已击败最强国际象棋冠军程序Stockfish。其应用场景非常广泛。</p><p>AlphaGo Zero 采用了蒙特卡洛树搜索＋深度学习算法，本文将尽可能用简单易懂的语言解释其工作原理。</p><h2 id="树搜索"><a href="#树搜索" class="headerlink" title="树搜索"></a>树搜索</h2><p><img src="http://upload-images.jianshu.io/upload_images/31319-b9de3b3bde6ac1c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="treesearch"></p><p>从一个棋盘的初始状态，开始思考下一步如何走。我们可以回顾一下我们思考的过程，我们会思考自己可以有哪几种走法，如果我走了这里，对手可能会走哪里，那么我还可以在哪里走。我和对手都会选择最有利的走法，最终价值最大的那一手，就是我要选择的下法。很明显这个思维过程是一颗树，为了寻找最佳的行棋点的过程，就是树搜索。</p><p>围棋第一手有361种下法，第二手有360种，第三手有359，依次类推，即一共有 361! 种下法，考虑到存在大量不合规则的棋子分布，合理的棋局约占这个数字的1.2%(<a href="https://link.zhihu.com/?target=https%3A//tromp.github.io/go/legal.html">Counting Legal Positions in Go</a>). 约为2.081681994 * 10^170。这个一个天文数字，比目前可观测宇宙的所有原子数还要多。要进行完全树搜索，是不可能的。因此我们必须进行剪枝，并限制思考的深度。所谓剪枝，就是指没必要考虑每种下法，我们只需考虑最有价值的几手下法。所谓限制思考的深度，就是我们最多只思考5步，10步，20步。常见的算法是Alpha-beta剪枝算法。但是，剪枝算法也有它的缺陷，它很有可能过早的剪掉了后期价值很大走法。</p><h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p>简而言之，蒙特卡洛方法(Monte Carlo method)，是一种“统计模拟方法”。20世纪40年代，为建造核武器，冯.诺伊曼 等人发明了该算法。因赌城蒙特卡洛而得名，暗示其以概率作为算法的基础。</p><p>假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则N+1，如果这个点在不规则图形内则W+1。落入不规则图形的概率即为 W/N。当掷出足够多的点之后，我们可以认为：不规则图形面积＝矩形面积＊W/N。</p><p>要应用蒙特卡洛算法的问题，首先要将问题转化为概率问题，然后通过统计方法将其问题的解估计出来。</p><h2 id="蒙特卡洛树搜索（MCTS）"><a href="#蒙特卡洛树搜索（MCTS）" class="headerlink" title="蒙特卡洛树搜索（MCTS）"></a>蒙特卡洛树搜索（MCTS）</h2><p>1987年Bruce Abramson在他的博士论文中提出了基于蒙特卡洛方法的树搜索这一想法。这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率。如果描述的再具体一些，通过不断的模拟每一种走法，直至终局，该走法的模拟总次数N，与胜局次数W，即可推算出该走法的胜率为 W/N。</p><p>该算法的每个循环包含4个步骤：选择、扩展、仿真、反向传播。一图胜千言。</p><p><img src="http://upload-images.jianshu.io/upload_images/31319-08a2e9e9174b591f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MCTS"></p><p>图中N表示总模拟次数，W表示胜局次数。每次都选择胜率最大的节点进行模拟。但是这样会导致新节点无法被探索到。为了在最大胜率和新节点探索上保持平衡，UCT（Upper Confidence Bound，上限置信区间算法）被引入。所谓置信区间，就是概率计算结果的可信度。打个比方，如果掷了3次硬币，都是正面朝上，我们就认为掷硬币正面朝上概率是100%，那肯定是错误的，因为我们的样本太少了。所以UCT就是用来修正这个样本太少的问题。具体公式如下：</p><p><img src="http://upload-images.jianshu.io/upload_images/31319-dbbfb7db809a4111.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="UCT公式"></p><p>其中wi 是i节点的胜利次数，ni是i节点的模拟次数，Ni是所有模拟次数，c是探索常数，理论值为 √2，可根据经验调整。公式的后半部分，探索次数越少，值会越大，所以，那些被探索比较少的点，会获得更多的探索机会。</p><p>蒙特卡洛树搜索算法因为是直接模拟到游戏终局，所以这种算法更加的准确，而且并不需要一个明确的“估值函数”，你只需要实现游戏机制就足够了。而且，蒙特卡洛算法，可以随时终止，根据其训练的时间给予近似的最优结果。</p><p>但是对于围棋这种游戏而言，它的选择点依然太多，这棵树会非常的大。可能有一个分支早已被丢弃，那么它将不会被统计，这可能是李世石能够在第四局击败AlphaGo的主要原因。对于这类情况，我们依然需要依赖一个好的估值函数来辅助。</p><h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>近年来，深度卷积神经网络在视觉领域取得很大的成功，如图片分类，人脸识别等。深度学习的网络结构在此不赘述，简而言之，深度学习是一个最优化算法。</p><p>我们可以将深度神经网络理解为一个黑盒，这个黑盒接收一批输入，得到一个输出，并根据输出计算出损失（误差），这个误差会反馈给黑盒，当给了足够多的数据之后，这个黑盒将具备一个特性，就是使误差最小化。</p><p>如果这么说还是难以理解的话，可以打个比方：深度神经网络是一种生物，它喜欢吃糖，有学习的能力，你给它看一张图片，它告诉你是猫还是狗，如果它猜对了，你就给它一颗糖，猜错了，就不给糖，久而久之，它就有了分辨猫狗的能力。作为创造者，你甚至不知道它是如何分辨猫狗的，但是它做到了，看得越多，识别的就越准。</p><p>这里至关重要的是——输入是什么？输出是什么？什么时候给糖的动作，也就是损失函数如何设计？在实际的操作过程中，网络结构的设计也很重要，这里不再细述。</p><p>对于围棋来说，深度网络可以用来评估下一步的主要选点（降低树的宽度），以及评估当前局面的值。</p><h2 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h2><p>在AlphaGo Lee版本，有两个神经网络，一个是策略网络，是一个有监督学习，它利用了大量的人类高手的对弈棋局来评估下一步的可能性，另一个是价值网络，用来评价当前局面的评分。而在AlphaGo Zero版本，除了围棋规则外，没有任何背景知识，并且只使用一个神经网络。</p><p>这个神经网络以19x19棋盘为输入，以下一步各下法的概率以及胜率为输出，这个网络有多个batch normalization卷积层以及全连接层。</p><p>AlphaGo Zero的核心思想是：<em>MCTS算法生成的对弈可以作为神经网络的训练数据。</em> 还记得我们前面说过的深度学习最重要的部分吗？输入、输出、损失！随着MCTS的不断执行，下法概率及胜率会趋于稳定，而深度神经网络的输出也是下法概率和胜率，而两者之差即为损失。随着训练的不断进行，网络对于胜率的下法概率的估算将越来越准确。这意味着什么呢？这意味着，即便某个下法AGZ没有模拟过，但是通过神经网络依然可以达到蒙特卡洛的模拟效果！也就是说，我虽然没下过这手棋，但凭借我在神经网络中训练出的“棋感”，我可以估算出这么走的胜率是多少！</p><p>AlphaGo Zero的对弈过程只需应用深度网络计算出的下法概率、胜率、MCTS的置信区间等数据即可进行选点。</p><h2 id="AlphaGo-Zero-论文节选"><a href="#AlphaGo-Zero-论文节选" class="headerlink" title="AlphaGo Zero 论文节选"></a>AlphaGo Zero 论文节选</h2><p><img src="http://upload-images.jianshu.io/upload_images/31319-caf7b3f0dffdabac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="AlphaGo Zero增强学习过程"></p><p>a:自我对弈过程s1，…，sT。 在每个状态st, 使用最近一次的网络fθ，执行一次MCTS αθ （见图2）。 下法根据MCTS计算的搜索概率而选择，at ~ πt. 评价终止状态sT，根据游戏规则来计算胜利者z。<br>b: AlphaGo Zero的神经网络训练。网络使用原始的棋盘状态st作为输入，通过数个卷积层，使用参数θ，输出有向量 pt, 表示下法的分布概率，以及一个标量vt，表示当前玩家在st的胜率。网络参数θ将自动更新，以最大化策略向量pt和搜索概率πt的相似性，并最小化预测赢家vt与实际赢家z的误差。新参数将应用于下一次自我对弈a的迭代。</p><p><img src="http://upload-images.jianshu.io/upload_images/31319-540aea408a78ee1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="AlphaGo Zero 蒙特卡洛树搜索过程"></p><p>a: 每次模拟选择的分支，有最大Q+U, 其中Q是动作价值，U是上限置信，U依赖于一个存储在分支上的优先概率P和该分支的访问次数N（每访问一次N+1）。<br>b: 扩展叶节点，神经网络（P(s, .), V(s)) = fθ(s)评估s; 将向量P的值被存储在s的扩展边上。<br>c: 根据V更新动作价值（action-value)Q，反映所有该动作的子树的平均值。<br>d: 一旦搜索结束，搜索概率π被返回，与 Ν^(1/τ) 成正比，N是每个分支的访问次数，而τ是一个参数控制着温度（temperature）。</p><h2 id="AlphaGo-Zero的应用"><a href="#AlphaGo-Zero的应用" class="headerlink" title="AlphaGo Zero的应用"></a>AlphaGo Zero的应用</h2><p>AGZ算法本质上是一个最优化搜索算法，对于所有开放信息的离散的最优化问题，只要我们可以写出完美的模拟器，就可以应用AGZ算法。所谓开放信息，就像围棋象棋，斗地主不是开放信息，德扑虽然不是开放信息，但本身主要是概率问题，也可以应用。所谓离散问题，下法是一步一步的，变量是一格一格，可以有限枚举的，比如围棋361个点是可以枚举的，而股票、无人驾驶、星际争霸，则不是这类问题。Deepmind要攻克的下一个目标是星际争霸，因为它是不完全信息，连续性操作，没有完美模拟器（随机性），目前在这方面AI还是被人类完虐</p><p>所以看到AG打败人类，AGZ打败AG，就认为人工智能要打败人类了，这种观点在未来可能成立，但目前还有点危言耸听。距离真正打败人类，AGZ还差得很远。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;本文写于2017年12月，获&lt;a href=&quot;https://zhuanlan.zhihu.com/p/32952677&quot; target=&quot;_blank&quot; rel=&quot;noopener&quot;&gt;Udacity专栏转载&lt;/a&gt;。今将其搬运至我的博客。&lt;/p&gt;
&lt;p&gt;2016年3月，A
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
    
  </entry>
  
  <entry>
    <title>Promise模式在防止缓存雪崩中的应用</title>
    <link href="http://guileen.github.io/2019/11/01/promise-in-cache-crash/"/>
    <id>http://guileen.github.io/2019/11/01/promise-in-cache-crash/</id>
    <published>2019-10-31T16:00:00.000Z</published>
    <updated>2019-12-26T07:30:56.779Z</updated>
    
    <content type="html"><![CDATA[<p>对大多数高并发架构而言，缓存是不可或缺的。在数据持久化层，其核心是保证数据一致性，而吞吐能力往往较弱。而在缓存层，因其逻辑简单，则具备较高的吞吐能力，但为了保证数据的时效性，则必须设置缓存的过期时间。在缓存过期后，程序会从持久化层读取数据，填充缓存。我们通常称这种缓存加载方式为懒加载（lazy load）。</p><p>在缓存失效的瞬间，如果突然爆发大量缓存请求，则会导致所有请求穿透至持久化层，给持久化层带来巨大压力，这种现象叫做缓存雪崩。</p><p><img src="/img/promise/1.png" alt="缓存雪崩"></p><h2 id="解决缓存雪崩的几种方案"><a href="#解决缓存雪崩的几种方案" class="headerlink" title="解决缓存雪崩的几种方案"></a>解决缓存雪崩的几种方案</h2><ol><li>在预加载时设置锁状态。后至的缓存请求，将获得锁状态，在一段时间后重试加载缓存。但这一方法不能保证第一时间返回数据。</li></ol><p><img src="/img/promise/2.png" alt="穿透锁"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">def lazyload(key):</span><br><span class="line">    value &#x3D; cache.get(key)</span><br><span class="line">    if(!value):</span><br><span class="line">        cache.set(key, &#39;__lock__&#39;)</span><br><span class="line">        value &#x3D; db.get(key)</span><br><span class="line">        cache.set(key, value)</span><br><span class="line">    if(value &#x3D;&#x3D; &#39;__lock__&#39;):</span><br><span class="line">        sleep(100)</span><br><span class="line">        return lazyload(key)</span><br><span class="line">    return value</span><br></pre></td></tr></table></figure><ol start="2"><li>这里重点介绍的Promise解决缓存穿透的思路，这种方法将使同一进程内对同一缓存的访问进行汇总，不仅减少对持久层的缓存穿透，而且也可以降低对缓存层的请求量。拥有极强的汇聚效果。</li></ol><p><img src="/img/promise/3.png" alt="Promise解决缓存雪崩"></p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br></pre></td><td class="code"><pre><span class="line">def _lazyload(key):</span><br><span class="line">    value &#x3D; cache.get(key)</span><br><span class="line">    if(!value):</span><br><span class="line">        value &#x3D; db.get(key)</span><br><span class="line">        cache.set(key, value)</span><br><span class="line"></span><br><span class="line">promiseMap &#x3D; &#123;&#125;</span><br><span class="line">def lazyload(key):</span><br><span class="line">    def clearPromiseMap:</span><br><span class="line">        delete promiseMap[key]</span><br><span class="line">    promise &#x3D; promiseMap[key]</span><br><span class="line">    if(!promise):</span><br><span class="line">        promise &#x3D; Promise(_lazyload, key)</span><br><span class="line">        promise.then(clearPromiseMap)</span><br><span class="line">        promiseMap[key] &#x3D; promise</span><br><span class="line">    return promise.resolve()</span><br></pre></td></tr></table></figure><p>本文所有代码为伪代码！</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;对大多数高并发架构而言，缓存是不可或缺的。在数据持久化层，其核心是保证数据一致性，而吞吐能力往往较弱。而在缓存层，因其逻辑简单，则具备较高的吞吐能力，但为了保证数据的时效性，则必须设置缓存的过期时间。在缓存过期后，程序会从持久化层读取数据，填充缓存。我们通常称这种缓存加载方
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>使用docker-compose设置Gogs</title>
    <link href="http://guileen.github.io/2019/03/29/setup-gogs-with-docker-compose/"/>
    <id>http://guileen.github.io/2019/03/29/setup-gogs-with-docker-compose/</id>
    <published>2019-03-29T08:58:02.000Z</published>
    <updated>2019-12-25T02:37:31.672Z</updated>
    
    <content type="html"><![CDATA[<p>创建 ~/gogs/docker-compose.yml</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br></pre></td><td class="code"><pre><span class="line">version: &#39;2&#39;</span><br><span class="line">services:</span><br><span class="line">  gogs:</span><br><span class="line">    container_name: gogs</span><br><span class="line">    image: gogs&#x2F;gogs</span><br><span class="line">    volumes:</span><br><span class="line">      - &#x2F;data&#x2F;gogs&#x2F;:&#x2F;data</span><br><span class="line">    ports:</span><br><span class="line">      - &quot;3080:3000&quot;</span><br><span class="line">      - &quot;3022:22&quot;</span><br><span class="line">    restart: always</span><br></pre></td></tr></table></figure><p>执行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">sudo docker-compose up -d</span><br></pre></td></tr></table></figure><p>Nginx配置</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 80;</span><br><span class="line">  server_name git.example.com;</span><br><span class="line"></span><br><span class="line">  location &#x2F; &#123;</span><br><span class="line">        proxy_set_header Host $http_host;</span><br><span class="line">        proxy_pass http:&#x2F;&#x2F;127.0.0.1:3080;</span><br><span class="line">        proxy_redirect off;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure><p>配置注意事项：</p><ul><li><em>SSH Port:</em> Use the exposed port from Docker container. For example, your SSH server listens on 22 inside Docker, but you expose it by 10022:22, then use 10022 for this value. Builtin SSH server is not recommended inside Docker Container</li><li><em>HTTP Port:</em> Use port you want Gogs to listen on inside Docker container. For example, your Gogs listens on 3000 inside Docker, and you expose it by 10080:3000, but you still use 3000 for this value.</li><li><em>Application URL:</em> Use combination of Domain and exposed HTTP Port values (e.g. <a href="http://192.168.99.100:10080/" target="_blank" rel="noopener">http://192.168.99.100:10080/</a>).</li></ul>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;创建 ~/gogs/docker-compose.yml&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td class=&quot;gutter&quot;&gt;&lt;pre&gt;&lt;span class=&quot;line&quot;&gt;1&lt;/span&gt;&lt;br&gt;&lt;span 
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>How ssl works and Letscrypt</title>
    <link href="http://guileen.github.io/2019/01/30/How-ssl-works-and-letscrypt/"/>
    <id>http://guileen.github.io/2019/01/30/How-ssl-works-and-letscrypt/</id>
    <published>2019-01-30T12:49:39.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="SSL-TLS-解决了什么问题？"><a href="#SSL-TLS-解决了什么问题？" class="headerlink" title="SSL/TLS 解决了什么问题？"></a>SSL/TLS 解决了什么问题？</h2><p>假设A给B发信息，直接明文发送，那么所有的中间传输节点，都可以截获明文，这种通信是不安全的，想象一下你的密码全部在网上明文传输，是不是很危险。</p><p>现在A将信息加密后传输给B，B解密信息，加密密钥和解密密钥是相同，这种加密算法叫做对称加密算法。题是，A如何把密钥告诉B？如果依然通过同一个中间人告诉B，一旦中间人知道这个密钥，那么传输过程就依然是不安全的了。</p><p>这时非对称加密算法出现了，非对称加密的加密解密需要两把钥匙，我们称之为公钥和私钥，所谓公钥就是可以公开的钥匙，可以安全的分享出去。使用公钥加密的数据必须用私钥解密，使用私钥加密的必须用公钥解密。如果A和B之间进行通信，那么AB双方首先交换各自的公钥，保留各自的私钥，这个过程是安全的。A要给B发信息则使用B的公钥加密，因为只有B自己拥有私钥，所以只有B可以解密信息，反之依然。这是非对称加密的第一个用途，防止中间人破译信息。非对称加密还有另一个用途————身份验证，如果A要向B表明身份证明自己的确是A，只需要按照B的要求加密一段随机字符串S即可，A使用自己的私钥加密S，将加密结果发送给B，B使用A提供的公钥进行解密，若结果为S，则证明对方的身份的确是A，这样就完成了认证过程，我们常用的ssh公钥登录，就是这个原理。</p><p>一切看起来很完美，但是还有一个问题没有解决。假设A、B之间有一个中间人C，AB之间的所有消息都经过C传递。这是C在这个传输过程做了手脚，当AB交换公钥时，A把公钥发给C，希望C把A的公钥转交给B，可是这是C没有把A的公钥交给B，而是把C的公钥交给了B，B误以为C的公钥就是A的公钥。在B把公钥发给A的过程中，C做了同样的手脚。这时A、B手上都是C的公钥，而C手上也有A、B的公钥。这时A给B发消息时，会使用C的公钥加密，C则先用C的私钥解密得出原文完成信息窃取，再用B的公钥加密信息发给B。AB都认为自己进行了安全的传输，一切天衣无缝。更可怕的是C还可以直接篡改信息。谁有能力做这件事？你的网络提供商，你连接的免费wifi，网络上的交换节点，都有能力实施这个中间人攻击。那么问题来了，既然交换公钥这种非对称加密手段都无法奏效，还搞毛线呢？这时就需要CA上场了。</p><pre><code>A  ---- A的公钥 ---&gt;  C  ---- C的公钥 ---&gt; BA  &lt;--- C的公钥 ----  C  &lt;--- B的公钥 ---- BA  --C公钥加密信息--&gt;  C  -- B公钥加密信息--&gt; BA  &lt;-C公钥加密信息---  C  &lt;- A公钥加密信息--- B</code></pre><p>CA的全称是Certificate Authority，即证书颁发机构。A为了保证自己的公钥不被中间人篡改，会先将自己的公钥交给CA，CA用自己的私钥教秘A的公钥，B使用CA的公钥解密A的公钥，只要CA的签名的公钥没有问题，则A的公钥也必然没有问题。那么又有一个新的问题来了，如果中间人C伪造CA的公钥怎么办？这个问题的解决方案比较粗暴，CA的公钥是直接写在浏览器里的。如果CA的公钥被篡改，浏览器会直接提示不安全的网络连接。因此我们也需要警惕一些山寨浏览器，如果没有道德底线的约束，他们完全可以篡改CA证书，为网络监听大开方便之门。</p><p>因为CA需要各大浏览器厂商的共同认可，因此是个壁垒很高的生意。如果一个网站需要提供安全的网络连接，则需要将自己的网站公钥通过CA生成一个认证公钥，这个认证的公钥也就是证书，这个证书不便宜。想象一下，每一年你都需要付出两千块钱，就是为了证明你是你，这钱花的是不是挺冤枉。你说我的网站就不提供安全传输不就完了吗，反正我这信息也都是公开的，也没什么值得监听的。不行，因为很多业务场景，第三方接口，必须要求你提供安全的网络连接。这就增加了开设一个网络服务的成本，尤其是增加了玩票的成本，小微创新就会受影响。</p><p><img src="/img/https_flowchart.jpg" alt=""></p><p>现在就该今天的主角上场了，<a href="https://letsencrypt.org/" target="_blank" rel="noopener">Letsencrypt</a>，他是一个免费、自动化的证书颁发机构。今天就向大家安利一下Letsencrypt。 Letsencryt在web server上运行了一个证书管理代理（Certificate management agent）的程序。假设我们希望设置<code>https://example.com</code>的证书。那么一共有两个步骤，第一步，证明你拥有exmaple.com，第二步，代理程序可以请求（request）、更新（renew）以及废除（revoke）证书，</p><p>Letsencrypt根据agent公钥来验证账号，agent第一次与Letsencrypt交互时，Letsencrypt会要求agent证明自己拥有某个域名。agent会询问需要自己做什么来证明自己拥有这个域名，这时Letsencrypt会下发一组任务，比如添加某个DNS记录，在网站下提供某个制定的资源。这和传统的CA证书机构类似。当agent完成操作后，CA就认为该agent已经拥有这个域名了，之后的域名更新都可以通过agent操作。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">$ sudo add-apt-repository ppa:certbot&#x2F;certbot</span><br><span class="line">$ sudo apt-get update</span><br><span class="line">$ sudo apt-get install certbot</span><br><span class="line">$ sudo certbot certonly -d ipub.io -d *.ipub.io --manual --preferred-challenges dns --server https:&#x2F;&#x2F;acme-v02.api.letsencrypt.org&#x2F;directory</span><br></pre></td></tr></table></figure><p>跟着提示填写，其中一步要求在DNS记录中添加一个TXT，修改后继续，准确无误的话，证书会在/etc/letsencrypt/live/ipub.io/ 目录下, 修改nginx配置文件。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br></pre></td><td class="code"><pre><span class="line">server &#123;</span><br><span class="line">  listen 443 ssl;</span><br><span class="line">  listen [::]:443 ssl;</span><br><span class="line">  keepalive_timeout 70;</span><br><span class="line"></span><br><span class="line">  root &#x2F;var&#x2F;www&#x2F;leen.ipub.io;</span><br><span class="line">  index index.html index.htm;</span><br><span class="line">  server_name leen.ipub.io;</span><br><span class="line">  ssl_certificate  &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;ipub.io&#x2F;fullchain.pem;</span><br><span class="line">  ssl_certificate_key  &#x2F;etc&#x2F;letsencrypt&#x2F;live&#x2F;ipub.io&#x2F;privkey.pem;</span><br><span class="line">  location &#x2F; &#123;</span><br><span class="line">    try_files $uri $uri&#x2F; &#x2F;index.html &#x3D;404;</span><br><span class="line">  &#125;</span><br><span class="line">&#125;</span><br></pre></td></tr></table></figure>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;SSL-TLS-解决了什么问题？&quot;&gt;&lt;a href=&quot;#SSL-TLS-解决了什么问题？&quot; class=&quot;headerlink&quot; title=&quot;SSL/TLS 解决了什么问题？&quot;&gt;&lt;/a&gt;SSL/TLS 解决了什么问题？&lt;/h2&gt;&lt;p&gt;假设A给B发信息，直接明文发
      
    
    </summary>
    
    
  </entry>
  
  <entry>
    <title>设置邮箱服务器</title>
    <link href="http://guileen.github.io/2019/01/29/set-up-a-mail-server/"/>
    <id>http://guileen.github.io/2019/01/29/set-up-a-mail-server/</id>
    <published>2019-01-29T14:40:54.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<p>发现自己已经有足足一年多没有更新任何博客了。过去的一年中，自己越来越多的在做管理型的工作，远离了一线开发，开发笔记类的东西也就少了。</p><p>最近这段时间一直在整理自己的思路，希望能够做些能够触达用户的事情，比如搞一个公众号之类的。而各种自媒体号都需要邮箱，申请免费邮箱又是很麻烦的事，搞不好密码忘了，也很麻烦。既然自己有域名，为什么不自己搞一个企业邮箱呢。常用的是腾讯的免费企业邮箱，但是只能绑定一个域名，而我的账号下已经绑定了一个域名。于是想着自己动手搭建一个邮箱服务器。</p><p>这个邮箱服务器的主要目标是接收各种注册邮件、验证码，并不要求完善的账号管理系统。备选方案有：</p><ol><li><a href="https://postal.atech.media/" target="_blank" rel="noopener">Postal</a>, write with ruby, MySQL</li><li><a href="https://modoboa.org/en/" target="_blank" rel="noopener">Modoboa</a>, write with python.</li><li>postfix</li><li>dovecot</li><li><a href="https://github.com/mailhog/MailHog" target="_blank" rel="noopener">MailHog</a></li><li><a href="https://github.com/toorop/tmail" target="_blank" rel="noopener">tmail</a></li><li><a href="https://mailinabox.email/" target="_blank" rel="noopener">Mail in a box</a></li><li><a href="https://github.com/djfarrelly/MailDev" target="_blank" rel="noopener">MailDev</a></li><li><a href="https://www.inbucket.org/" target="_blank" rel="noopener">Inbucket</a></li></ol><p>最终选择了maildev，安装简单方便</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">npm i -g maildev</span><br><span class="line">maildev --web-user xx --web-pass xx</span><br></pre></td></tr></table></figure><p>使用supervisor，后台运行</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"># supervisor&#x2F;conf.d&#x2F;maildev.conf</span><br><span class="line">[program:maildev]</span><br><span class="line">command&#x3D;&#x2F;opt&#x2F;nodejs&#x2F;bin&#x2F;maildev --web-user&#x3D;xx --web-pass&#x3D;xx -s 25 -w 1080</span><br><span class="line">redirect_stderr&#x3D;true</span><br><span class="line">stdout_logfile&#x3D;&#x2F;var&#x2F;log&#x2F;maildev.log</span><br><span class="line">stdout_logfile_maxbytes&#x3D;10MB</span><br><span class="line">stdout_logfile_backups&#x3D;10</span><br><span class="line">stdout_capture_maxbytes&#x3D;100MB</span><br></pre></td></tr></table></figure><p>修改DNS，添加MX记录为服务器IP。发邮件到<a href="mailto:test@example.com">test@example.com</a> 测试，打开 <code>http://example.com:1080/</code> ，可以看到自己刚发的邮件。说明已经可以有任意多的邮箱了。</p><p>把他和brook科学上网服务放在同一台服务器上，充分利用资源。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;发现自己已经有足足一年多没有更新任何博客了。过去的一年中，自己越来越多的在做管理型的工作，远离了一线开发，开发笔记类的东西也就少了。&lt;/p&gt;
&lt;p&gt;最近这段时间一直在整理自己的思路，希望能够做些能够触达用户的事情，比如搞一个公众号之类的。而各种自媒体号都需要邮箱，申请免费邮
      
    
    </summary>
    
    
      <category term="ops" scheme="http://guileen.github.io/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>机器的意识</title>
    <link href="http://guileen.github.io/2018/06/01/conscious-of-machine/"/>
    <id>http://guileen.github.io/2018/06/01/conscious-of-machine/</id>
    <published>2018-06-01T03:49:41.000Z</published>
    <updated>2019-12-25T05:52:27.265Z</updated>
    
    <content type="html"><![CDATA[<p>机器学习当下是非常火热的课题，大量的模型被设计出来令人眼花缭乱。诸如SVM，随机森林，贝叶斯网络，神经网络只不过是这个领域的基础知识，在这些基础方法之上的各种优化分支方法更是数不胜数。大量的所谓机器学习的研究者，在某个问题上，通过对现有方法的小修小补，达到了一些微不足道的提升，或仅仅是对几种模型／优化方法／特征分析／问题领域进行一些常规的比较分析，便可以一本正经的发布一篇学术论文。为了这些方法应用到实际的工程中时，通常需要消耗大量的人力用于前期的数据分析和处理，这一过程被称为“特征工程”。对于此类人员的专业度要求非常高，需求量也非常大，因此这类人才的溢价也非常高，以致于一个刚刚毕业的研究生的薪酬可以轻松超越一个有多年经验的软件工程师。然而这类人力工作通常并没有太多的创新性可言。这是在当前的科技背景下产生的独特的现象——一大批资质平平的智力工作者受到了疯狂的追捧。这一现象必然在机器学习真正取得突破性进展后终结，本文将讨论突破性进展可能的方向。</p><p>科技发展有两种驱动力，其一是资本对于更高生产力的追求，其二是纯粹的为了智力上的满足感或是对于某种超自然信仰而展开的对于真理本质的追求。前者更长于将科学成果转化为实际应用，而后者才是实现技术爆炸的真正内核。机器学习领域也是如此，目前正处于科学成果转化的高峰期，是基于前些年的一些基础研究进行的转化。但也是瓶颈期，因为我们感受到了现有方法的局限。目前人们对于人工智能的态度行业内外的看法截然相反：业外的人大部分担心人工智能将超越、统治甚至毁灭人类，业内大部分则恰恰相反。因为专业人士更加清楚，现有方法大多是非常愚蠢的，所谓智能不过是一种魔术。你一定体验过魔术被揭秘的感觉，这就是我兴致勃勃的了解了许多机器学习方法后的感受。可以很负责任的说，我们距离人工智能的本质还差的很远很远。大部分的人醉心于如何更好的表演魔术，只有很少的人去追求真正的本质，而这些追求本质的方法如果在魔术表演上不够精彩，是很容易被忽视的。</p><p>人工智能概念最早由约翰·麦卡锡（John McCarthy，1927年9月4日－2011年10月24日）在1956年由他自己召集的达特矛斯会议上提出的：“人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样”。这是人工智能的早期定义，人们后来意识到这个定义忽略了强人工智能的可能性。所谓强人工智能是指机器拥有真正的智能而不仅仅是“看起来像”，与之相对的则被称为弱人工智能，而目前实现的都是弱人工智能。这引发了一系列的哲学问题——什么是智能？强人工智能是否有可能实现？强人工智能是不是人类的灾难？即便我们出于伦理和人类安全的考虑，不追求实现强人工智能，而只是追求弱人工智能的“看起来像”的目的，也不得不借鉴强人工智能的思路，也就不可避免的不断的朝着强人工智能方向前进。本文不讨论人工智能是否对人类构成威胁的问题，仅从方向和方法上讨论强人工智能的可行性。</p><p>人工智能本质上属于仿生学范畴的。智能本身就是很复杂的，而人类仅能认识到自己的智能，不可能理解龙虾或者外星人的智能。所以对人工智能本质的追求，应该是对人类自身深入认识的过程。并且这种认识不能仅仅停留于哲学解构的层面，必须通过工程手段加以重新建构，我们用哲学上和工程上都能理解的语言进行表达，而这种语言我们称之为模型。</p><p>人类智能可划分为一些层次，无意识的精神、意识、自我、逻辑、知识、表达、知觉、经验。这些划分并不存在清晰的界限。通过真正的智能与现有弱人工智能的比较，我们可以更好的理解这些概念。</p><p>当AlphaGo轻松打败人类棋手时，人类棋手感到压力、恐惧、伤心、绝望。我们为AlphaGo的棋艺感到惊叹，但事实上二者在智能上相比，人类棋手的情绪反应才是真正令人惊叹的。AlphaGo不会为胜利而感到骄傲，也不会为失败而羞耻，他不会为存在而感到快乐，也不会为消失感到痛苦。他甚至根本没有想要胜利的欲望，更不会有人类各种复杂的情感。这些就是无意识的精神，精神在不自觉的驱动着我们的意识，或纵情声色，或知耻而勇，或趋利避害，或舍生忘死。虽然我们看起来是由理智所控制的动物，但精神不受理智的控制。这点在恋爱中的人身上尤为明显，即便他们的理智告诉他们不要对另一方过于依恋，但他们却无法控制自己所思所想。对于音乐的感受也是类似的，不过是不同频率的声波的组合，竟能使人不由自主的感到快乐或悲伤，多么的神奇。生物学家认为，当多巴胺分泌增加时，人就会感到快乐。多巴胺是一种神经传导物质，这就像是给神经系统的奖励，以刺激我们更加努力的工作。但这并不能解释快乐的本质，我们可以想象我们给车添加了燃油和润滑油，这相当于是给了车奖励，但是车并不会感到快乐。我们在训练AlphaGo时，每次他获胜，我们也会给他一个反馈，告诉他这次做的结果不错，他就会更加靠近这个结果，但他并不会感到快乐。那么为什么多巴胺分泌增加人就会感到快乐呢？显然多巴胺不是快乐的本质，只是快乐的使者而已。</p><p>人类诸多行为有可能是后天习得的，但有些则是在被创造时就存在的。食欲、快乐、痛觉、饥饿感、自我、对母亲的依恋，这些都是与生俱来的。所以我们可以认为某种无意识的精神在母体中被创造时就存在了。但是从那一刻这种精神开始存在呢？我们很难想象单细胞状态的受精卵拥有这种精神。当基因代码一层层展开，从单细胞到多细胞，脊椎动物，鱼、两栖、哺乳类、灵长目、人科、智人属的基因片段开始逐步表达，胚胎的结构从简单到复杂，直到降生时我们依然没有发育完全，经历长时间的幼态持续，青春期，我们更多的情绪和欲望被展现出来。当我们回忆我们成长阶段的各种性格变化，这一切究竟是在哪一刻如何产生的呢？我们可以肯定这种精神不是来自于单一的细胞，而是来自大量脑细胞的集体创造。在我们大脑中有一个部分，是在基因中硬编码的部分，做出一些基本的身体机能感受判断、审美判断、情感判断。这部分不受理智控制的部分，我们称为精神。如果没有这种本能的精神，我们的智力也就没有了用武之地。我们可以设想任何的人工智能程序，必须拥有某个目标，这就是他的欲望，在欲望被满足时会得到某种激励。但人类的欲望是如此的不同，在欲望被满足时得到的奖励感受也是如此不同。他不是一个简单弱人工智能所能解释的，以梯度下降法举例：当错误率很高时，函数会修正自己的参数朝着错误率更低的方向前进一小步，直到错误率不再有明显下降。错误率最小是他的目标，但是这个函数是如此机械，我们不可能他是有精神的。</p><p><em>精神是目标，但目标不一定是精神，精神本身就是复杂的，是在长期进化中形成的一种复杂的多细胞集体创造。</em></p><p><em>人类的意识是一系列智能子系统的集合</em>。佛教所谓六根即：眼耳鼻舌身意，意识是起到总控的作用。意识可以连接我们的视觉、听觉、触觉、味觉、嗅觉，可以控制我们的肌肉，意识是用来做出决策的。精神是意识的内核，是引擎，为意识指出明确的方向。精神是底层的，意识是上层的。当我们在梦境时，我们的意识模糊，但是精神却是清醒的，梦境混乱无序，我们对身体的控制减弱，但我们对于快乐悲伤的感受依然真实。意识不仅仅是各个单元的连接器，其自身也是复杂的，可能是某种层垒的结构，最低级的意识就是对于感觉的条件反射，更高级的意识活动则包含知识、经验、逻辑。意识不等于记忆、知识、逻辑、推理的能力。草原上的野兽，尤其是那些捕猎者，它们都拥有很强的意识，但不等于他们拥有人类一样的高级智力活动。</p><p>感觉系统大体是类似的，目前的人工智能在某种程度上已经能够做到一些感知能力，尤其是视觉、听觉方面。对于视觉而言，如果在你的周围空间随便选取一个点，那么通过这个点的只有一些杂乱的电磁波而已，我们称之为光线，但是当我们的眼睛置于这个点，我们能够从这堆杂乱的电磁波中提取出周围物体的信息，投射在我们的视网膜上，视网膜将这些刺激信号传递到我们的视觉处理系统，我们能够分辨“颜色”，“形状”，“材质”，“种类”，“运动”。听觉也是同样神奇，我们的双耳中的空气分子不过是在无序的震动，我们却能从中提取出音色、音高，如果有多种声音同时存在，我们也能很好的将这些声音分离。在一个混乱的环境下，如果我们想要看清或听清一些东西，我们必须付出更多的注意力，也就是我们需要投入更多的意识。这说明我们的感觉系统不仅仅是一个物理系统，更加是一个信息处理的系统。我们通过信息的时间变化，组合，特征分离和重新组合，得出结论。因此对于同一事物的感受，不同的个体是不一样的，这也就是我们说的主观感受。比如我们对于颜色的感受，在意识中的投射是不一样的，一个很明显的证据就是色盲。所以我们虽然都认同某个物体是红色的，红色在我的意识中与在你的意识中可能是完全不同的。在跨越物种之时，感受的区别必然更加明显。有些物种则拥有完全不同的感觉器官，比如可以看到地球磁场的知更鸟（引用）。感觉系统的模拟，首先需要有相应的仪器设备采集信号，信号必须经过无意识的处理，转化为抽象的特征，将抽象特征交给意识体来处理。</p><p>假设我们已经能够创造出具有精神、意识、感觉的系统，我们可以开始讨论“自我”这个问题。这个问题看起来非常诡异，一个具有精神、意识、感觉的系统，难道会没有自我的概念吗？如果一个意识体就是整个世界的全部，除了自身之外再无其他，他还会有自我的概念存在吗？自我相对于非我而存在的，没有非我，也就没有自我。如果一个意识体置于一个容器之内，他对容器外的世界一无所知，在他看来他就是全部。当我们沉浸于某个目标时，我们也会忘记了自己的存在。当我们进入竞争环境，自我概念就会强化。自我与存在是同一个问题，保持自我存在是一个本能的精神元素之一。   。。。必须区分内外，头发是不是我，指甲是不是手臂是不是我？</p><p>未完待续。。。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;机器学习当下是非常火热的课题，大量的模型被设计出来令人眼花缭乱。诸如SVM，随机森林，贝叶斯网络，神经网络只不过是这个领域的基础知识，在这些基础方法之上的各种优化分支方法更是数不胜数。大量的所谓机器学习的研究者，在某个问题上，通过对现有方法的小修小补，达到了一些微不足道的提
      
    
    </summary>
    
      <category term="AI" scheme="http://guileen.github.io/categories/AI/"/>
    
      <category term="随笔" scheme="http://guileen.github.io/categories/AI/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>人设——下一个现象级产品会是谁？</title>
    <link href="http://guileen.github.io/2016/12/19/public-avatar/"/>
    <id>http://guileen.github.io/2016/12/19/public-avatar/</id>
    <published>2016-12-19T04:15:00.000Z</published>
    <updated>2019-12-25T04:18:10.451Z</updated>
    
    <content type="html"><![CDATA[<p>Update 2019-12：此文预见了新社交软件抖音的崛起。红人霸场的现象使社交平台被红人绑架，新人也被压制。抖音能够成功，首先是4G移动网络的进一步普及，提供了短视频社交的需求，但这一赛道的玩家众多，并不能保证抖音胜出。真正保证抖音胜出的是其“捧红新人“的能力，完全基于推荐算法的产品设计，将流量分发的控制权从红人手中收回到平台，可以根据算法计划性的将流量分配给新人进行实验。至于其滤镜、音乐之类的辅助运营手段，并非其制霸全球的根本原因。</p><p>另在清理微博时，看见了自己在2012年对游戏收费模式的看法。王者荣耀的成功从另一个角度印证了我的预见性。<br><img src="/img/screen_shot/wb2012.jpeg" alt=""></p><h2 id="原文："><a href="#原文：" class="headerlink" title="原文："></a>原文：</h2><p>什么是SNS的本质？当我们看待一个SNS的时候，我们看到的是什么？</p><p>信息、状态、转发、私信、粉丝、评论，我们看到的是这些产品功能吗？还是这些数据？他的价值在哪里？</p><p>一个SNS，我们看到的是，我们自己的公共形象，或者是某些人的公共形象。SNS为这些红人提供了舞台。</p><p>而且，每个SNS所提供的舞台是不一样的，往来的红人也是不一样的。</p><p>SNS就像一个主题酒吧，来往形形色色的人，但他们身上或多或少会有一些共同点。在同一个SNS上，他们有同样的期许。</p><p>粉丝是一个两难的设计。没有粉丝，红人们没有动力在SNS上奋斗，而有了粉丝之后，任何的SNS最终会走向固化，<strong>红人霸占了主要的SNS资源</strong>。</p><p>这也是各种SNS会不断推陈出新的原因，因为有一批有潜质的红人，无论他／她是什么类别的红人，他需要一个舞台。但是有些场子，已经被人霸占，于是他们不得不寻找新的舞台。而总有一些SNS平台会抓住一些机会，提供某个类别的小舞台，这就是垂直SNS。</p><p>是否有可能干翻霸主级的社交网络？Facebook，twitter，国内的腾讯、微博。<strong>答案是能！</strong>本质上腾讯就被自己干翻了，微信取代QQ，本质上就是对霸主的挑战，虽然这一切发生在腾讯内部。但是微信取代QQ，这样的现象是难以复制的，为什么呢？因为微信取代QQ是发生在<strong>移动互联网与传统互联网更替的阶段的，是大事因缘</strong>，没有微信取代QQ，也会有其他应用取代QQ，但机会只有一次。</p><p>陌陌，用陌生人社交的方式占领了一定的市场，但是陌陌本身的名称定位已经把自己钉死了，就是陌生人社交。微信早期用摇一摇也达到了同样的爆发式增长，但用户规模上去之后，微信就可以弱化了这一个功能。</p><p>SNS下一个风口在哪里？SNS不是风口，但机会永远存在。挑战霸主级的社交网络，需要的是天时地利人和。产品与用户，双方共同成就了一个霸主级社交产品。得红人者得天下，微博的发展正是这样的模式。</p><p>但红人是本身就存在的，还是通过平台而红的，这个问题很关键。<strong>一个优秀的SNS平台应该有制造红人的能力，或者是让红人可以成长起来的能力</strong>。当你知道的红人越来越多的来自微信公众号，而不是来自微博的时候，那么说明微博的影响力降低了。</p><p>因此霸主级的似乎不可撼动的庞然大物其实是可以被打败的。</p><p>如何再造一个现象级的社交网络，我们不应该从产品模式上来思考这个问题，而是需要从人性的角度上来思考这个问题。人性是社交网络之所以成立的本质。更具体的说，<strong>人的社会性，情绪化，是社交网络的根本驱动力</strong>。人渴望被关注，渴望交流，渴望存在感，或只是渴望来自他人的声音。找到人在精神上的未被充分满足的需求点，强化这种概念，就有机会在社交产品中找到一席之地。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;Update 2019-12：此文预见了新社交软件抖音的崛起。红人霸场的现象使社交平台被红人绑架，新人也被压制。抖音能够成功，首先是4G移动网络的进一步普及，提供了短视频社交的需求，但这一赛道的玩家众多，并不能保证抖音胜出。真正保证抖音胜出的是其“捧红新人“的能力，完全基于
      
    
    </summary>
    
      <category term="随笔" scheme="http://guileen.github.io/categories/%E9%9A%8F%E7%AC%94/"/>
    
    
  </entry>
  
  <entry>
    <title>Python数据分析笔记</title>
    <link href="http://guileen.github.io/2016/12/15/python-data-analysis-note/"/>
    <id>http://guileen.github.io/2016/12/15/python-data-analysis-note/</id>
    <published>2016-12-15T13:51:36.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<h1 id="可选"><a href="#可选" class="headerlink" title="[可选]"></a>[可选]</h1><p>install python</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">pip install -U pip</span><br><span class="line"></span><br><span class="line">pip install &lt;something&gt;</span><br><span class="line">pip uninstall &lt;something&gt;</span><br></pre></td></tr></table></figure><p>pip support virtualenv</p><h1 id="Anaconda"><a href="#Anaconda" class="headerlink" title="[Anaconda]"></a>[Anaconda]</h1><p><a href="https://www.continuum.io/downloads" target="_blank" rel="noopener">Install anaconda</a></p><p>国内可从清华镜像下载，并设置镜像</p><p><a href="https://mirror.tuna.tsinghua.edu.cn/help/anaconda/" target="_blank" rel="noopener">清华镜像源</a></p><p>MacOS:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">wget https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;archive&#x2F;Anaconda2-x.x.x-MacOSX-x86_64.sh&#96;</span><br><span class="line">bash Anaconda2-x.x.x-MacOS-x86_64.sh</span><br></pre></td></tr></table></figure><p>会安装到~/anaconda2下，默认会将PATH设置在 bash_profile中，根据你自己的shell设置，加入</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br></pre></td><td class="code"><pre><span class="line">export PATH&#x3D;&quot;&#x2F;home&#x2F;xx&#x2F;anaconda2&#x2F;bin:$PATH&quot;</span><br></pre></td></tr></table></figure><p>重新打开你的shell，执行<code>conda</code>命令测试。</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda config --add channels https:&#x2F;&#x2F;mirrors.tuna.tsinghua.edu.cn&#x2F;anaconda&#x2F;pkgs&#x2F;free&#x2F;</span><br><span class="line">conda config --set show_channel_urls yes</span><br></pre></td></tr></table></figure><p>Python环境管理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda create --name py27 python&#x3D;2.7</span><br><span class="line">activate py27</span><br></pre></td></tr></table></figure><p>包管理:</p><figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">conda list</span><br><span class="line">conda install numpy</span><br></pre></td></tr></table></figure><p>(conda install 会安装或更新依赖库，pip install则不会)</p><h1 id="安装工具包"><a href="#安装工具包" class="headerlink" title="[安装工具包]"></a>[安装工具包]</h1><p>conda install numpy scipy pandas matplotlib</p><h1 id="IDE"><a href="#IDE" class="headerlink" title="[IDE]"></a>[IDE]</h1><p>spyder, pycharm, sublime text, vim</p><h1 id="Jupyter"><a href="#Jupyter" class="headerlink" title="[Jupyter]"></a>[Jupyter]</h1><p><code>ipython</code> 一个更好的python交互命令</p><p><code>jupyter notebook</code> Web记事本，可以将交互过程记录并分享。</p><p>后续使用 jupyter 记录。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h1 id=&quot;可选&quot;&gt;&lt;a href=&quot;#可选&quot; class=&quot;headerlink&quot; title=&quot;[可选]&quot;&gt;&lt;/a&gt;[可选]&lt;/h1&gt;&lt;p&gt;install python&lt;/p&gt;
&lt;figure class=&quot;highlight plain&quot;&gt;&lt;table&gt;&lt;tr&gt;&lt;td 
      
    
    </summary>
    
    
      <category term="machine-learning" scheme="http://guileen.github.io/tags/machine-learning/"/>
    
      <category term="python" scheme="http://guileen.github.io/tags/python/"/>
    
      <category term="data" scheme="http://guileen.github.io/tags/data/"/>
    
  </entry>
  
  <entry>
    <title>问题汇总</title>
    <link href="http://guileen.github.io/2016/11/24/architect-problems/"/>
    <id>http://guileen.github.io/2016/11/24/architect-problems/</id>
    <published>2016-11-24T06:55:49.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<p>负责某公司技术以来，遇到了一些问题，汇总一下：</p><ol><li>接口设计问题，错误的接口设计将导致难以避免的问题</li><li>DNS劫持问题，用户网络问题定位困难</li><li>苹果IAP漏单问题</li><li>缓存时间、状态不一致、压力太大、崩溃重启、启动慢问题</li><li>NoSQL选型问题</li><li>数据库负载问题，主从同步问题</li><li>日志监控问题</li><li>服务自动扩容，全球部署问题</li><li>垃圾信息、僵尸账号问题</li><li>IM丢消息，收不到消息问题。</li><li>IM、群聊问题定位、测试困难</li><li>聊天信息存储问题</li></ol>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;负责某公司技术以来，遇到了一些问题，汇总一下：&lt;/p&gt;
&lt;ol&gt;
&lt;li&gt;接口设计问题，错误的接口设计将导致难以避免的问题&lt;/li&gt;
&lt;li&gt;DNS劫持问题，用户网络问题定位困难&lt;/li&gt;
&lt;li&gt;苹果IAP漏单问题&lt;/li&gt;
&lt;li&gt;缓存时间、状态不一致、压力太大、崩溃重
      
    
    </summary>
    
    
      <category term="ops" scheme="http://guileen.github.io/tags/ops/"/>
    
  </entry>
  
  <entry>
    <title>笔墨之殇</title>
    <link href="http://guileen.github.io/2016/11/24/bimozhishang/"/>
    <id>http://guileen.github.io/2016/11/24/bimozhishang/</id>
    <published>2016-11-23T16:16:01.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<p>说点什么好呢？这两天我开始，翻阅以前我读过的书，我发现，有很多的书，只是草草看过，还有一些书，根本没有看过。原来自己有做读书笔记的习惯，可是当我把阅读的时间放到地铁上之后，这个习惯，也随之消失了。以至于我根本不知道这本书，有没有读过。纸质书，可以有记录笔记的空间，而电子书，阅读起来比较方便，我想这就是纸质书和电子书的区别吧！</p><p>以前我总想写点文字。可是我发现自己，写作的速度越来越慢了，也找不到合适的输入法。拼音输入法，当你想打一些，书面语，或者是文言文的时候，就很难输入。而，五笔输入法或者其他的字型输入法，都有一个问题，就是，学起来太麻烦了，我根本记不住那些字码表，而且，当我，去回忆那些字码的时候，我的思路已经被打断了。</p><p>我还是喜欢拿着笔在纸上写字的感觉。在这种状态下，我的思路是最流畅的。</p><p>一直以来，我都想写一两本小说，可是构思了很久，却迟迟没有动笔。自己内心真正想表达的东西，和整个故事的结构，往往存在冲突。我不愿意写一些，没有人想看的东西，但我也不想，完全是为了迎合别人的口味，而写作。</p><p>其实，这并不是什么文章，我这是在测试一种新的输入法，一种，新的写作方式，那就是，直接把我想写的，念出来。你所看到的一切，都是我通过语音输入的。好吧，今天就说到这里，我的脚也泡好了。</p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;p&gt;说点什么好呢？这两天我开始，翻阅以前我读过的书，我发现，有很多的书，只是草草看过，还有一些书，根本没有看过。原来自己有做读书笔记的习惯，可是当我把阅读的时间放到地铁上之后，这个习惯，也随之消失了。以至于我根本不知道这本书，有没有读过。纸质书，可以有记录笔记的空间，而电子书，
      
    
    </summary>
    
    
      <category term="gossip" scheme="http://guileen.github.io/tags/gossip/"/>
    
  </entry>
  
  <entry>
    <title>如何学习编程</title>
    <link href="http://guileen.github.io/2016/11/21/how-to-learn-computer-sciense/"/>
    <id>http://guileen.github.io/2016/11/21/how-to-learn-computer-sciense/</id>
    <published>2016-11-21T06:35:31.000Z</published>
    <updated>2019-12-25T02:37:31.671Z</updated>
    
    <content type="html"><![CDATA[<h2 id="编程入门"><a href="#编程入门" class="headerlink" title="编程入门"></a>编程入门</h2><p>大部分人都是通过某一个原因，喜欢上了编程。同样，你也需要一个理由。也许只是走在路上，忽然有人对你说：“年轻人，我看你天赋异禀，骨骼惊奇，我这里有一套编程秘笈，你想不想学啊”</p><p>入门编程语言，有很多选择，你可以选择python、JS，也可以选择C。比如我自己，是通过Basic语言入门的，也是因为它喜欢上了编程。</p><p>…</p><p>在这一阶段，最容易出现的问题是：Compiler Error，Syntax Error。你还不习惯和计算机进行沟通，你们之间的语言不够顺畅。它听不懂你说什么，当它说Error，你也听不懂它在说什么。你总是很抓狂的问，What’s the Error，一定是计算机出了什么问题，而不是我的代码有任何问题。直到你意识到，计算机没有任何问题，有问题的一定是我的代码，你能够检查你的代码，修复语法错误，恭喜你，你已经入门了。</p><p>这一阶段，你对编程有了感性的认识。你为自己写出来的东西感到骄傲，完全不会注意到其实那些代码其实只是piece of shit。你也会遇到很多的问题，即使你已经查阅到了所需要的知识点，还是无法实现你要的功能。就好像已经把所需要的材料全部交给了你，你却无法用这些材料造出你想造的东西。</p><p>这一段时间，你的灵感泉涌，有很多的想法想要去做，但却又感觉力不从心。你需要开始补充一些基础知识了，难度也要开始增加。</p><h2 id="计算机科学基础"><a href="#计算机科学基础" class="headerlink" title="计算机科学基础"></a>计算机科学基础</h2><p>这是一个非常重要的阶段，这一阶段的学习效果，直接决定了你的技术实力。有些知识，并不会立刻用得上，但是，这些知识，已经潜移默化的影响了你的思维方式。你的任督二脉将在这一阶段打开。</p><h3 id="算法"><a href="#算法" class="headerlink" title="算法"></a>算法</h3><p>算法和数据结构 这是一切程序的基础，你需要恰当的使用这些数据结构，你无法绕开它们。有一些算法，你需要知道它们的原理，这有助于你理解你的代码最终在计算机上是如何运行的。你可能并不需要掌握算法设计的机巧，也不需要去参加一个算法比赛。但你需要理解这些经典算法，记住它们的名字，在你遇到问题时，第一时间想到它们。最终，你可能不需要自己去实现它们，但你需要恰当的使用现有的算法代码。</p><h3 id="设计模式"><a href="#设计模式" class="headerlink" title="设计模式"></a>设计模式</h3><p>函数式编程、面向对象编程，设计模式。这些方面的知识，你也需要掌握，你需要了解如何将自己的代码组织在一起。这就像你拥有了一些积木，你需要了解玩积木的常用技巧。阅读一些书籍或者阅读他人的代码，都是非常好的途径。学习这些知识，就像是学习作画。这些技巧你一看就会明白，但却不能熟练的使用。这里是考验你艺术天分的地方。</p><p>在拥有了以上这些技能后，你已经可以算是一个不错的程序员了。但这些知识可以让你写出很棒的代码，却无法使你的程序有任何的功能。你必须要让你的程序和外界进行交互，它才有存在的价值。</p><h3 id="运行环境"><a href="#运行环境" class="headerlink" title="运行环境"></a>运行环境</h3><p>你的程序如何和外界进行交互？你的程序是运行在一个封闭的环境之中的。想象一下，一只猴子被关进了一个房间中，房间中有一个操作面板，当它按下某个按钮，外界就会发生某件事情，比如说在你的电脑上打开一个新网页。你的程序就是这只猴子，你必须熟悉你的操作面板，有些时候面对一些奇怪的面板故障，你甚至需要了解它的工作原理，以避免发生那些故障。</p><p>网络基础, TCP/IP 协议，HTTP协议，如果你要开发网络应用，这些都是非常重要的基础知识。</p><p>操作系统原理，无论你开发桌面应用、移动应用、嵌入式应用、网络应用、服务器端应用，你都应该对你的程序所运行的环境有足够了解。</p><p>如果你开发的是Web应用，也许你不需要了解操作系统，但你需要了解你的浏览器，它是你的运行环境。</p><p>计算机架构，这个你可以不必知道。但如果你要开发一个操作系统，那么你必须对计算机架构有所了解，还是那句话，你需要了解你的程序所运行的环境。而操作系统所运行的环境就是计算机硬件的体系架构。</p><h2 id="交流"><a href="#交流" class="headerlink" title="交流"></a>交流</h2><p>你需要一个目标，你定下的这个目标可能就是下一个facebook。带着目标学习，这是我所推崇的方式。对于还处于学习阶段的你来说，这是一段幸福的时光。你不必为了生计而学习，你可以纯粹的为了爱好而学习。</p><p>你需要加入开发者交流的社区，加入论坛、QQ群、讨论组、邮件列表。与别人分享你的收获与挫折，社区的氛围也是你继续学习的动力之一。当你遇到难题，可以在社区里提问。但是你应该学习一些提问的智慧，不要做伸手党，这对你的学习不会有任何帮助。你应该至少已经阅读了相关的书籍、资料，并借助搜索引擎（不要使用baidu，中国可以用bing）来寻求答案。记住一点：社区不能给你想要的东西，但社区可以解答你的疑惑。社区是用来交流的，你也可以通过回答别人的问题来提高自己的知识。</p><p>开源社区，也是你获取知识技能的主要来源之一。当你需要某个功能，有人可能已经实现了他，并将他开源在了github（目前最大的开源社区）之类的地方。这些开源项目可以帮助你解决某些细节问题，使你可以更专注在你的主要目标上。对于优秀的开源项目，你可以阅读他们的代码，学习他们的机巧。</p><h2 id="再论运行环境"><a href="#再论运行环境" class="headerlink" title="再论运行环境"></a>再论运行环境</h2><p>如果你仔细体会的话，你会发现，编写代码只占到你学习编程中的很少的一部分时间，而大部分时间，你是在查阅资料。你需要花大量的时间在学习程序的运行环境上，而不是学习编程语言上。运行环境会提供给你很多的编程接口，一般被称之为API。</p><p>我这里所说的运行环境，并不是仅仅指操作系统运行环境，它也可以是浏览器，Java运行环境，Sevlet容器，node.js，unity3D运行环境，flash运行环境，directX，OpenGL，cocos2d游戏开发框架。</p><p>你可能注意到了一点，我将开发框架，视作了一种运行环境。为什么这么说呢？因为框架是对运行环境的再次封装，在框架之上，你将看到更加易用，更加人性化的接口。有一些框架，还额外提供了很多辅助的库，甚至插件机制，让你可以直接使用整个社区贡献的插件。你只需要面对框架编程，借来几个插件，再搭配几个辅助库，就可以完成一个作品。</p><p>你明白了吗？编程就是这么简单！这也是为什么有那么多平庸的程序员的主要原因————他们只懂得在框架之上编程，使用别人写好的现成的代码。<em>可是一旦他所赖以生存的框架或插件或库，出现了任何问题，他们的平庸就会显现</em>。</p><p>如果你是初次接触编程，还不知道什么是开发框架（framework）什么是库（library）的话，没关系，你只需要记住一点：Library决定了你能做什么，framework决定了你不能做什么。如果有一个新的框架，让你眼前一亮，蠢蠢欲动，请保持冷静，先想一想，如果你用了这个框架，你将失去哪些能力，是否是可以接受的？我看过太多的项目因为框架的限制，而不得不使用一些旁门左道来突破框架的限制，从而失去了代码的美感。抑或是自己动手，将框架改的面目全非，完全失去了框架的意义。</p><p>对于初学者，我强烈建议远离框架。框架是一种捷径，但对于一个以学习阶段的人来说，捷径是并不是什么好事。如果你已经有了足够的经验，对你的运行环境足够了解了之后，你应该在你的运行环境之上，寻找一个优秀的框架，学习它的设计思想。更进一步的，你可以搭建一个你自己的框架。我并不反对使用框架，但我反对不求甚解的使用框架。</p><p>修炼是一个长期的过程，即使你已经成为了一名优秀的程序员，你依然需要不断的修炼。记住一点：<em>修炼的捷径就是不走捷径</em>。</p><h2 id="Hack"><a href="#Hack" class="headerlink" title="Hack"></a>Hack</h2><p>你可能想要做一些看起来不可能的事情，这通常是从某个夸张的想法而引起的。Hook、反射，这些略有些高级的技巧。</p><h2 id="软件工程"><a href="#软件工程" class="headerlink" title="软件工程"></a>软件工程</h2><p>你已经找到了你的第一份工作，你加入了一个团队，经过一段时间，你又被提升为了项目组长。你需要开始考虑一些团队管理问题、系统架构问题</p><h2 id="专业方向"><a href="#专业方向" class="headerlink" title="专业方向"></a>专业方向</h2><h2 id="一些资源："><a href="#一些资源：" class="headerlink" title="一些资源："></a>一些资源：</h2><p><a href="https://pdos.csail.mit.edu/6.828/2014/" target="_blank" rel="noopener">MIT 6.828 操作系统工程</a><br><a href="github.com">github.com</a><br><a href="stackshare.io">stackshare.io</a><br><a href="stackoverflow.com">stackoverflow.com</a></p>]]></content>
    
    <summary type="html">
    
      
      
        &lt;h2 id=&quot;编程入门&quot;&gt;&lt;a href=&quot;#编程入门&quot; class=&quot;headerlink&quot; title=&quot;编程入门&quot;&gt;&lt;/a&gt;编程入门&lt;/h2&gt;&lt;p&gt;大部分人都是通过某一个原因，喜欢上了编程。同样，你也需要一个理由。也许只是走在路上，忽然有人对你说：“年轻人，我看你天赋异禀
      
    
    </summary>
    
    
      <category term="education" scheme="http://guileen.github.io/tags/education/"/>
    
  </entry>
  
</feed>
