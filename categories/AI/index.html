<!DOCTYPE html>
<html>
<head>
  <meta charset="utf-8">
  
  <meta name="renderer" content="webkit">
  <meta http-equiv="X-UA-Compatible" content="IE=edge" >
  <link rel="dns-prefetch" href="http://guileen.github.io">
  <title>Category: AI | 桂糊涂的博客</title>
  <meta name="viewport" content="width=device-width, initial-scale=1, maximum-scale=1">
  <meta name="description" content="Description">
<meta property="og:type" content="website">
<meta property="og:title" content="桂糊涂的博客">
<meta property="og:url" content="http://guileen.github.io/categories/AI/index.html">
<meta property="og:site_name" content="桂糊涂的博客">
<meta property="og:description" content="Description">
<meta property="article:author" content="桂糊涂">
<meta name="twitter:card" content="summary">
  
    <link rel="alternative" href="/atom.xml" title="桂糊涂的博客" type="application/atom+xml">
  
  
    <link rel="icon" href="/img/avatar.jpg">
  
  
<link rel="stylesheet" href="/main.css">

  

<meta name="generator" content="Hexo 4.1.1"></head>

<body>
  <div id="container">
    <div class="left-col">
      <div class="overlay"></div>
<div class="intrude-less">
	<header id="header" class="inner">
		<a href="/" class="profilepic">
			
			<img src="/img/avatar.jpg" class="js-avatar">
			
		</a>

		<hgroup>
		  <h1 class="header-author"><a href="/">桂糊涂</a></h1>
		</hgroup>

		
		<p class="header-subtitle">代码杂记</p>
		

		<nav class="header-menu">
			<ul>
			
				<li><a href="/">主页</a></li>
	        
				<li><a href="/categories/AI">AI</a></li>
	        
				<li><a href="/categories/%E9%9A%8F%E7%AC%94">随笔</a></li>
	        
			</ul>
		</nav>
		<nav class="header-smart-menu">
	        
    		
    			
    			<a class="js-smart-menu" data-idx="0" href="javascript:void(0)">所有文章</a>
    			
    			
            
    			
    			<a class="js-smart-menu" data-idx="1" href="javascript:void(0)">标签</a>
    			
    			
            
    			
            
    			
    			<a class="js-smart-menu" data-idx="2" href="javascript:void(0)">关于我</a>
    			
    			
            
		</nav>
		<nav class="header-nav">
			<div class="social">
				
					<a class="github" target="_blank" href="https://github.com/guileen" title="github">github</a>
		        
					<a class="weibo" target="_blank" href="https://weibo.com/guileen" title="weibo">weibo</a>
		        
					<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
		        
			</div>
		</nav>
	</header>		
</div>

    </div>
    <div class="mid-col">
      <nav id="mobile-nav">
  	<div class="overlay">
  		<div class="slider-trigger"><i class="icon-list"></i></div>
  		<h1 class="header-author js-mobile-header hide">桂糊涂</h1>
  	</div>
	<div class="intrude-less">
		<header id="header" class="inner">
			<div class="profilepic">
				
					<img src="/img/avatar.jpg" class="js-avatar">
				
			</div>
			<hgroup>
			  <h1 class="header-author">桂糊涂</h1>
			</hgroup>
			
			<p class="header-subtitle">代码杂记</p>
			
			<nav class="header-menu">
				<ul>
				
					<li><a href="/">主页</a></li>
		        
					<li><a href="/categories/AI">AI</a></li>
		        
					<li><a href="/categories/%E9%9A%8F%E7%AC%94">随笔</a></li>
		        
				</ul>
			</nav>
			<nav class="header-nav">
				<div class="social">
					
						<a class="github" target="_blank" href="https://github.com/guileen" title="github">github</a>
			        
						<a class="weibo" target="_blank" href="https://weibo.com/guileen" title="weibo">weibo</a>
			        
						<a class="rss" target="_blank" href="/atom.xml" title="rss">rss</a>
			        
				</div>
			</nav>
		</header>				
	</div>
</nav>

      <div class="body-wrap">
        
  
    <article id="post-torch-output-loss-optimizer" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/24/torch-output-loss-optimizer/">Torch的损失函数和优化器</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>深度神经网络输出的结果与标注结果进行对比，计算出损失，根据损失进行优化。那么输出结果、损失函数、优化方法就需要进行正确的选择。</p>
<h1 id="常用损失函数"><a href="#常用损失函数" class="headerlink" title="常用损失函数"></a>常用损失函数</h1><p>pytorch 损失函数的基本用法</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br></pre></td><td class="code"><pre><span class="line">criterion = LossCriterion(参数)</span><br><span class="line">loss = criterion(x, y)</span><br></pre></td></tr></table></figure>
<p>Mean Absolute Error<br>torch.nn.L1Loss<br>Measures the mean absolute error.</p>
<h2 id="Mean-Absolute-Error-L1Loss"><a href="#Mean-Absolute-Error-L1Loss" class="headerlink" title="Mean Absolute Error/ L1Loss"></a>Mean Absolute Error/ L1Loss</h2><p>nn.L1Loss<br><img src="/img/loss/l1loss.png" alt=""><br>很少使用</p>
<h2 id="Mean-Square-Error-Loss"><a href="#Mean-Square-Error-Loss" class="headerlink" title="Mean Square Error Loss"></a>Mean Square Error Loss</h2><p>nn.MSELoss<br><img src="/img/loss/mseloss.png" alt=""><br>针对数值不大的回归问题。</p>
<h2 id="Smooth-L1-Loss"><a href="#Smooth-L1-Loss" class="headerlink" title="Smooth L1 Loss"></a>Smooth L1 Loss</h2><p>nn.SmoothL1Loss<br><img src="/img/loss/smoothl1loss.png" alt=""><br>它在绝对差值大于1时不求平方，可以避免梯度爆炸。大部分回归问题都可以适用，尤其是数值比较大的时候。</p>
<h2 id="Negative-Log-Likelihood-Loss"><a href="#Negative-Log-Likelihood-Loss" class="headerlink" title="Negative Log-Likelihood Loss"></a>Negative Log-Likelihood Loss</h2><p>torch.nn.NLLLoss，一般与 LogSoftmax 成对使用。使用时 <code>loss(softmaxTarget, target)</code>。用于处理多分类问题。<br><img src="/img/loss/nllloss.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line">m = nn.LogSoftmax(dim=<span class="number">1</span>)</span><br><span class="line">loss = nn.NLLLoss()</span><br><span class="line"><span class="comment"># input is of size N x C = 3 x 5， C为分类数</span></span><br><span class="line">input = torch.randn(<span class="number">3</span>, <span class="number">5</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line"><span class="comment"># each element in target has to have 0 &lt;= value &lt; C</span></span><br><span class="line">target = torch.tensor([<span class="number">1</span>, <span class="number">0</span>, <span class="number">4</span>])</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="Cross-Entropy-Loss"><a href="#Cross-Entropy-Loss" class="headerlink" title="Cross Entropy Loss"></a>Cross Entropy Loss</h2><p>nn.CrossEntropyLoss 将 LogSoftmax 和 NLLLoss 绑定到了一起。所以无需再对结果使用Softmax</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">loss &#x3D; nn.CrossEntropyLoss()</span><br><span class="line">input &#x3D; torch.randn(3, 5, requires_grad&#x3D;True)</span><br><span class="line">target &#x3D; torch.empty(3, dtype&#x3D;torch.long).random_(5)</span><br><span class="line">output &#x3D; loss(input, target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="BCELoss"><a href="#BCELoss" class="headerlink" title="BCELoss"></a>BCELoss</h2><p>二分类问题的CrossEntropyLoss。输入、目标结构是一样的。</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br></pre></td><td class="code"><pre><span class="line">m = nn.Sigmoid()</span><br><span class="line">loss = nn.BCELoss()</span><br><span class="line">input = torch.randn(<span class="number">3</span>, requires_grad=<span class="keyword">True</span>)</span><br><span class="line">target = torch.empty(<span class="number">3</span>).random_(<span class="number">2</span>)</span><br><span class="line">output = loss(m(input), target)</span><br><span class="line">output.backward()</span><br></pre></td></tr></table></figure>

<h2 id="Margin-Ranking-Loss"><a href="#Margin-Ranking-Loss" class="headerlink" title="Margin Ranking Loss"></a>Margin Ranking Loss</h2><p><img src="/img/loss/marginrankingloss.png" alt=""></p>
<p>常用户增强学习、对抗生成网络、排序任务。给定输入x1，x2，y的值是1或-1，如果y==1表示x1应该比x2的排名更高，y==-1则相反。如果y值与x1、x2顺序一致，那么loss为0，否则错误为 y*(x1-x2)</p>
<h2 id="Hinge-Embedding-Loss"><a href="#Hinge-Embedding-Loss" class="headerlink" title="Hinge Embedding Loss"></a>Hinge Embedding Loss</h2><p>y的值是1或-1，用于衡量两个输入是否相似或不相似。</p>
<h2 id="Cosine-Embedding-Loss"><a href="#Cosine-Embedding-Loss" class="headerlink" title="Cosine Embedding Loss"></a>Cosine Embedding Loss</h2><p>给定两个输入x1，x2，y的值是1或-1，用于衡量x1和x2是否相似。<br><img src="/img/loss/cosineembeddingloss.png" alt=""><br>其中cos(x1, x2)表示相似度<br><img src="/img/loss/cossim.png" alt=""></p>
<h1 id="各种优化器"><a href="#各种优化器" class="headerlink" title="各种优化器"></a>各种优化器</h1><p>大多数情况Adam能够取得比较好的效果。SGD 是最普通的优化器, 也可以说没有加速效果, 而 Momentum 是 SGD 的改良版, 它加入了动量原则. 后面的 RMSprop 又是 Momentum 的升级版. 而 Adam 又是 RMSprop 的升级版. 不过从这个结果中我们看到, Adam 的效果似乎比 RMSprop 要差一点. 所以说并不是越先进的优化器, 结果越佳.</p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br></pre></td><td class="code"><pre><span class="line"><span class="comment"># SGD 就是随机梯度下降</span></span><br><span class="line">opt_SGD         = torch.optim.SGD(net_SGD.parameters(), lr=LR)</span><br><span class="line"><span class="comment"># momentum 动量加速,在SGD函数里指定momentum的值即可</span></span><br><span class="line">opt_Momentum    = torch.optim.SGD(net_Momentum.parameters(), lr=LR, momentum=<span class="number">0.8</span>)</span><br><span class="line"><span class="comment"># RMSprop 指定参数alpha</span></span><br><span class="line">opt_RMSprop     = torch.optim.RMSprop(net_RMSprop.parameters(), lr=LR, alpha=<span class="number">0.9</span>)</span><br><span class="line"><span class="comment"># Adam 参数betas=(0.9, 0.99)</span></span><br><span class="line">opt_Adam        = torch.optim.Adam(net_Adam.parameters(), lr=LR, betas=(<span class="number">0.9</span>, <span class="number">0.99</span>))</span><br></pre></td></tr></table></figure>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/24/torch-output-loss-optimizer/" class="archive-article-date">
  	<time datetime="2019-12-24T14:05:59.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-24</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-understanding-cnn" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/12/24/understanding-cnn/">理解CNN参数及PyTorch实例</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文假设读者已经了解了CNN的基本原理。在实际的项目中，会发现CNN有多个参数需要调整，本文主要目的在于理清各个参数的作用。</p>
<h2 id="卷积核-kernel"><a href="#卷积核-kernel" class="headerlink" title="卷积核 kernel"></a>卷积核 kernel</h2><p>Kernel，卷积核，有时也称为filter。在迭代过程中，学习的结果就保存在kernel里面。深度学习，学习的就是一个权重。kernel的尺寸越小，计算量越小，一般选择3x3，更小就没有意义了。<br><img src="/img/cnn/kernel_2.png" alt=""></p>
<p>结果是对卷积核与一小块输入数据的点积。</p>
<h2 id="层数-Channels"><a href="#层数-Channels" class="headerlink" title="层数 Channels"></a>层数 Channels</h2><p><img src="/img/cnn/channel_1.png" alt=""></p>
<p>所有位置的点积构成一个激活层。</p>
<p><img src="/img/cnn/channel_2.png" alt=""></p>
<p>如果我们有6个卷积核，我们就会有6个激活层。</p>
<h2 id="步长-Stride"><a href="#步长-Stride" class="headerlink" title="步长 Stride"></a>步长 Stride</h2><p><img src="/img/cnn/kernel.gif" alt=""><br>上图是每次向右移动一格，一行结束向下移动一行，所以stride是1x1，如果是移动2格2行则是2x2。</p>
<h2 id="填充-Padding"><a href="#填充-Padding" class="headerlink" title="填充 Padding"></a>填充 Padding</h2><p>Padding的作用是为了获取图片上下左右边缘的特征。<br><img src="/img/cnn/pad.jpg" alt=""></p>
<h2 id="池化-Pooling"><a href="#池化-Pooling" class="headerlink" title="池化 Pooling"></a>池化 Pooling</h2><p>卷积层为了提取特征，但是卷积层提取完特征后特征图层依然很大。为了减少计算量，我们可以用padding的方式来减小特征图层。Pooling的方法有MaxPooling核AveragePooling。<br><img src="/img/cnn/pooling.jpg" alt=""></p>
<p>推荐看一下李飞飞的<a href="http://cs231n.stanford.edu/slides/2017/cs231n_2017_lecture5.pdf" target="_blank" rel="noopener">这篇slide</a></p>
<h2 id="PyTorch-中的相关方法"><a href="#PyTorch-中的相关方法" class="headerlink" title="PyTorch 中的相关方法"></a>PyTorch 中的相关方法</h2><ul>
<li><p>torch.nn.Conv2d(in_channels, out_channels, kernel_size, stride=1, padding=0, dilation=1, groups=1, bias=True, padding_mode=’zeros’)</p>
</li>
<li><p>torch.nn.MaxPool2d(kernel_size, stride=None, padding=0, dilation=1, return_indices=False, ceil_mode=False)</p>
<ul>
<li>stride 默认与kernel_size相等</li>
</ul>
</li>
<li><p>torch.nn.AvgPool2d(kernel_size, stride=None, padding=0, ceil_mode=False, count_include_pad=True, divisor_override=None)</p>
</li>
<li><p>Tensor.view(*shape) -&gt; Tensor</p>
<ul>
<li>用于将卷积层展开为全连接层<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br></pre></td><td class="code"><pre><span class="line">&gt;&gt;&gt; x &#x3D; torch.randn(4, 4)</span><br><span class="line">&gt;&gt;&gt; x.size()</span><br><span class="line">torch.Size([4, 4])</span><br><span class="line">&gt;&gt;&gt; y &#x3D; x.view(16)</span><br><span class="line">&gt;&gt;&gt; y.size()</span><br><span class="line">torch.Size([16])</span><br><span class="line">&gt;&gt;&gt; z &#x3D; x.view(-1, 8)  # the size -1 is inferred from other dimensions</span><br><span class="line">&gt;&gt;&gt; z.size()</span><br><span class="line">torch.Size([2, 8])</span><br></pre></td></tr></table></figure>

</li>
</ul>
</li>
</ul>
<h2 id="MNIST例子"><a href="#MNIST例子" class="headerlink" title="MNIST例子"></a>MNIST例子</h2><p>MNIST 数据集的输入是 1x28x28 的数据集。在实际开发中必须要清楚每一次的输出结构。</p>
<ul>
<li>我们第一层使用 5x5的卷积核，步长为1，padding为0，28-5+1 = 24，那么输出就是 24x24。计算方法是 (input_size - kernel_size)/ stride + 1。</li>
<li>我们第二层使用 2x2的MaxPool，那么输出为 12x12.</li>
<li>第三层再使用5x5，卷积核，输出则为 12-5+1，即 8x8。</li>
<li>再使用 2x2 MaxPool，输出则为 4x4。</li>
</ul>
<p><img src="/img/cnn/mnist_convet.png" alt=""></p>
<figure class="highlight python"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br></pre></td><td class="code"><pre><span class="line"><span class="keyword">import</span> torch.nn <span class="keyword">as</span> nn</span><br><span class="line"><span class="keyword">import</span> torch.nn.functional <span class="keyword">as</span> F</span><br><span class="line"></span><br><span class="line"><span class="class"><span class="keyword">class</span> <span class="title">Net</span><span class="params">(nn.Module)</span>:</span></span><br><span class="line">    <span class="string">"""ConvNet -&gt; Max_Pool -&gt; RELU -&gt; ConvNet -&gt; Max_Pool -&gt; RELU -&gt; FC -&gt; RELU -&gt; FC -&gt; SOFTMAX"""</span></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">__init__</span><span class="params">(self)</span>:</span></span><br><span class="line">        super(Net, self).__init__()</span><br><span class="line">        self.conv1 = nn.Conv2d(<span class="number">1</span>, <span class="number">10</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.conv2 = nn.Conv2d(<span class="number">10</span>, <span class="number">20</span>, <span class="number">5</span>, <span class="number">1</span>)</span><br><span class="line">        self.fc1 = nn.Linear(<span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>, <span class="number">50</span>)</span><br><span class="line">        self.fc2 = nn.Linear(<span class="number">50</span>, <span class="number">10</span>)</span><br><span class="line"></span><br><span class="line">    <span class="function"><span class="keyword">def</span> <span class="title">forward</span><span class="params">(self, x)</span>:</span></span><br><span class="line">        x = F.relu(self.conv1(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = F.relu(self.conv2(x))</span><br><span class="line">        x = F.max_pool2d(x, <span class="number">2</span>, <span class="number">2</span>)</span><br><span class="line">        x = x.view(<span class="number">-1</span>, <span class="number">4</span>*<span class="number">4</span>*<span class="number">20</span>)</span><br><span class="line">        x = F.relu(self.fc1(x))</span><br><span class="line">        x = self.fc2(x)</span><br><span class="line">        <span class="keyword">return</span> F.log_softmax(x, dim=<span class="number">1</span>)</span><br></pre></td></tr></table></figure>

<p>以上代码摘自 <a href="https://github.com/floydhub/mnist" target="_blank" rel="noopener">https://github.com/floydhub/mnist</a></p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/12/24/understanding-cnn/" class="archive-article-date">
  	<time datetime="2019-12-24T07:56:21.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-12-24</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-alpha-go-zero" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2019/11/01/alpha-go-zero/">AlphaGo Zero 工作原理</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>本文写于2017年12月，获<a href="https://zhuanlan.zhihu.com/p/32952677" target="_blank" rel="noopener">Udacity专栏转载</a>。今将其搬运至我的博客。</p>
<p>2016年3月，Alpha Go Master击败最强的人类围棋选手之一李世石。击败李的版本，在训练过程中使用了大量人类棋手的棋谱。2017年10月19日，DeepMind公司在《自然》杂志发布了一篇新的论文，AlphaGo Zero——它完全不依赖人类棋手的经验，经过3天的训练，Alpha Go Zero击败了Master版本。AlphaGo Zero最重要的价值在于，它不仅仅可以解决围棋问题，它可以在不需要知识预设的情况下，解决一切棋类问题，经过几个小时的训练，已击败最强国际象棋冠军程序Stockfish。其应用场景非常广泛。</p>
<p>AlphaGo Zero 采用了蒙特卡洛树搜索＋深度学习算法，本文将尽可能用简单易懂的语言解释其工作原理。</p>
<h2 id="树搜索"><a href="#树搜索" class="headerlink" title="树搜索"></a>树搜索</h2><p><img src="http://upload-images.jianshu.io/upload_images/31319-b9de3b3bde6ac1c2.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="treesearch"></p>
<p>从一个棋盘的初始状态，开始思考下一步如何走。我们可以回顾一下我们思考的过程，我们会思考自己可以有哪几种走法，如果我走了这里，对手可能会走哪里，那么我还可以在哪里走。我和对手都会选择最有利的走法，最终价值最大的那一手，就是我要选择的下法。很明显这个思维过程是一颗树，为了寻找最佳的行棋点的过程，就是树搜索。</p>
<p>围棋第一手有361种下法，第二手有360种，第三手有359，依次类推，即一共有 361! 种下法，考虑到存在大量不合规则的棋子分布，合理的棋局约占这个数字的1.2%(<a href="https://link.zhihu.com/?target=https%3A//tromp.github.io/go/legal.html">Counting Legal Positions in Go</a>). 约为2.081681994 * 10^170。这个一个天文数字，比目前可观测宇宙的所有原子数还要多。要进行完全树搜索，是不可能的。因此我们必须进行剪枝，并限制思考的深度。所谓剪枝，就是指没必要考虑每种下法，我们只需考虑最有价值的几手下法。所谓限制思考的深度，就是我们最多只思考5步，10步，20步。常见的算法是Alpha-beta剪枝算法。但是，剪枝算法也有它的缺陷，它很有可能过早的剪掉了后期价值很大走法。</p>
<h2 id="蒙特卡洛方法"><a href="#蒙特卡洛方法" class="headerlink" title="蒙特卡洛方法"></a>蒙特卡洛方法</h2><p>简而言之，蒙特卡洛方法(Monte Carlo method)，是一种“统计模拟方法”。20世纪40年代，为建造核武器，冯.诺伊曼 等人发明了该算法。因赌城蒙特卡洛而得名，暗示其以概率作为算法的基础。</p>
<p>假设我们要计算一个不规则形状的面积，我们只需在包含这个不规则形状的矩形内，随机的掷出一个点，每掷出一个点，则N+1，如果这个点在不规则图形内则W+1。落入不规则图形的概率即为 W/N。当掷出足够多的点之后，我们可以认为：不规则图形面积＝矩形面积＊W/N。</p>
<p>要应用蒙特卡洛算法的问题，首先要将问题转化为概率问题，然后通过统计方法将其问题的解估计出来。</p>
<h2 id="蒙特卡洛树搜索（MCTS）"><a href="#蒙特卡洛树搜索（MCTS）" class="headerlink" title="蒙特卡洛树搜索（MCTS）"></a>蒙特卡洛树搜索（MCTS）</h2><p>1987年Bruce Abramson在他的博士论文中提出了基于蒙特卡洛方法的树搜索这一想法。这种算法简而言之是用蒙特卡洛方法估算每一种走法的胜率。如果描述的再具体一些，通过不断的模拟每一种走法，直至终局，该走法的模拟总次数N，与胜局次数W，即可推算出该走法的胜率为 W/N。</p>
<p>该算法的每个循环包含4个步骤：选择、扩展、仿真、反向传播。一图胜千言。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/31319-08a2e9e9174b591f.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="MCTS"></p>
<p>图中N表示总模拟次数，W表示胜局次数。每次都选择胜率最大的节点进行模拟。但是这样会导致新节点无法被探索到。为了在最大胜率和新节点探索上保持平衡，UCT（Upper Confidence Bound，上限置信区间算法）被引入。所谓置信区间，就是概率计算结果的可信度。打个比方，如果掷了3次硬币，都是正面朝上，我们就认为掷硬币正面朝上概率是100%，那肯定是错误的，因为我们的样本太少了。所以UCT就是用来修正这个样本太少的问题。具体公式如下：</p>
<p><img src="http://upload-images.jianshu.io/upload_images/31319-dbbfb7db809a4111.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="UCT公式"></p>
<p>其中wi 是i节点的胜利次数，ni是i节点的模拟次数，Ni是所有模拟次数，c是探索常数，理论值为 √2，可根据经验调整。公式的后半部分，探索次数越少，值会越大，所以，那些被探索比较少的点，会获得更多的探索机会。</p>
<p>蒙特卡洛树搜索算法因为是直接模拟到游戏终局，所以这种算法更加的准确，而且并不需要一个明确的“估值函数”，你只需要实现游戏机制就足够了。而且，蒙特卡洛算法，可以随时终止，根据其训练的时间给予近似的最优结果。</p>
<p>但是对于围棋这种游戏而言，它的选择点依然太多，这棵树会非常的大。可能有一个分支早已被丢弃，那么它将不会被统计，这可能是李世石能够在第四局击败AlphaGo的主要原因。对于这类情况，我们依然需要依赖一个好的估值函数来辅助。</p>
<h2 id="深度学习"><a href="#深度学习" class="headerlink" title="深度学习"></a>深度学习</h2><p>近年来，深度卷积神经网络在视觉领域取得很大的成功，如图片分类，人脸识别等。深度学习的网络结构在此不赘述，简而言之，深度学习是一个最优化算法。</p>
<p>我们可以将深度神经网络理解为一个黑盒，这个黑盒接收一批输入，得到一个输出，并根据输出计算出损失（误差），这个误差会反馈给黑盒，当给了足够多的数据之后，这个黑盒将具备一个特性，就是使误差最小化。</p>
<p>如果这么说还是难以理解的话，可以打个比方：深度神经网络是一种生物，它喜欢吃糖，有学习的能力，你给它看一张图片，它告诉你是猫还是狗，如果它猜对了，你就给它一颗糖，猜错了，就不给糖，久而久之，它就有了分辨猫狗的能力。作为创造者，你甚至不知道它是如何分辨猫狗的，但是它做到了，看得越多，识别的就越准。</p>
<p>这里至关重要的是——输入是什么？输出是什么？什么时候给糖的动作，也就是损失函数如何设计？在实际的操作过程中，网络结构的设计也很重要，这里不再细述。</p>
<p>对于围棋来说，深度网络可以用来评估下一步的主要选点（降低树的宽度），以及评估当前局面的值。</p>
<h2 id="AlphaGo-Zero"><a href="#AlphaGo-Zero" class="headerlink" title="AlphaGo Zero"></a>AlphaGo Zero</h2><p>在AlphaGo Lee版本，有两个神经网络，一个是策略网络，是一个有监督学习，它利用了大量的人类高手的对弈棋局来评估下一步的可能性，另一个是价值网络，用来评价当前局面的评分。而在AlphaGo Zero版本，除了围棋规则外，没有任何背景知识，并且只使用一个神经网络。</p>
<p>这个神经网络以19x19棋盘为输入，以下一步各下法的概率以及胜率为输出，这个网络有多个batch normalization卷积层以及全连接层。</p>
<p>AlphaGo Zero的核心思想是：<em>MCTS算法生成的对弈可以作为神经网络的训练数据。</em> 还记得我们前面说过的深度学习最重要的部分吗？输入、输出、损失！随着MCTS的不断执行，下法概率及胜率会趋于稳定，而深度神经网络的输出也是下法概率和胜率，而两者之差即为损失。随着训练的不断进行，网络对于胜率的下法概率的估算将越来越准确。这意味着什么呢？这意味着，即便某个下法AGZ没有模拟过，但是通过神经网络依然可以达到蒙特卡洛的模拟效果！也就是说，我虽然没下过这手棋，但凭借我在神经网络中训练出的“棋感”，我可以估算出这么走的胜率是多少！</p>
<p>AlphaGo Zero的对弈过程只需应用深度网络计算出的下法概率、胜率、MCTS的置信区间等数据即可进行选点。</p>
<h2 id="AlphaGo-Zero-论文节选"><a href="#AlphaGo-Zero-论文节选" class="headerlink" title="AlphaGo Zero 论文节选"></a>AlphaGo Zero 论文节选</h2><p><img src="http://upload-images.jianshu.io/upload_images/31319-caf7b3f0dffdabac.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="AlphaGo Zero增强学习过程"></p>
<p>a:自我对弈过程s1，…，sT。 在每个状态st, 使用最近一次的网络fθ，执行一次MCTS αθ （见图2）。 下法根据MCTS计算的搜索概率而选择，at ~ πt. 评价终止状态sT，根据游戏规则来计算胜利者z。<br>b: AlphaGo Zero的神经网络训练。网络使用原始的棋盘状态st作为输入，通过数个卷积层，使用参数θ，输出有向量 pt, 表示下法的分布概率，以及一个标量vt，表示当前玩家在st的胜率。网络参数θ将自动更新，以最大化策略向量pt和搜索概率πt的相似性，并最小化预测赢家vt与实际赢家z的误差。新参数将应用于下一次自我对弈a的迭代。</p>
<p><img src="http://upload-images.jianshu.io/upload_images/31319-540aea408a78ee1c.png?imageMogr2/auto-orient/strip%7CimageView2/2/w/1240" alt="AlphaGo Zero 蒙特卡洛树搜索过程"></p>
<p>a: 每次模拟选择的分支，有最大Q+U, 其中Q是动作价值，U是上限置信，U依赖于一个存储在分支上的优先概率P和该分支的访问次数N（每访问一次N+1）。<br>b: 扩展叶节点，神经网络（P(s, .), V(s)) = fθ(s)评估s; 将向量P的值被存储在s的扩展边上。<br>c: 根据V更新动作价值（action-value)Q，反映所有该动作的子树的平均值。<br>d: 一旦搜索结束，搜索概率π被返回，与 Ν^(1/τ) 成正比，N是每个分支的访问次数，而τ是一个参数控制着温度（temperature）。</p>
<h2 id="AlphaGo-Zero的应用"><a href="#AlphaGo-Zero的应用" class="headerlink" title="AlphaGo Zero的应用"></a>AlphaGo Zero的应用</h2><p>AGZ算法本质上是一个最优化搜索算法，对于所有开放信息的离散的最优化问题，只要我们可以写出完美的模拟器，就可以应用AGZ算法。所谓开放信息，就像围棋象棋，斗地主不是开放信息，德扑虽然不是开放信息，但本身主要是概率问题，也可以应用。所谓离散问题，下法是一步一步的，变量是一格一格，可以有限枚举的，比如围棋361个点是可以枚举的，而股票、无人驾驶、星际争霸，则不是这类问题。Deepmind要攻克的下一个目标是星际争霸，因为它是不完全信息，连续性操作，没有完美模拟器（随机性），目前在这方面AI还是被人类完虐</p>
<p>所以看到AG打败人类，AGZ打败AG，就认为人工智能要打败人类了，这种观点在未来可能成立，但目前还有点危言耸听。距离真正打败人类，AGZ还差得很远。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2019/11/01/alpha-go-zero/" class="archive-article-date">
  	<time datetime="2019-10-31T17:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2019-11-01</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-conscious-of-machine" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2018/06/01/conscious-of-machine/">机器的意识</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>机器学习当下是非常火热的课题，大量的模型被设计出来令人眼花缭乱。诸如SVM，随机森林，贝叶斯网络，神经网络只不过是这个领域的基础知识，在这些基础方法之上的各种优化分支方法更是数不胜数。大量的所谓机器学习的研究者，在某个问题上，通过对现有方法的小修小补，达到了一些微不足道的提升，或仅仅是对几种模型／优化方法／特征分析／问题领域进行一些常规的比较分析，便可以一本正经的发布一篇学术论文。为了这些方法应用到实际的工程中时，通常需要消耗大量的人力用于前期的数据分析和处理，这一过程被称为“特征工程”。对于此类人员的专业度要求非常高，需求量也非常大，因此这类人才的溢价也非常高，以致于一个刚刚毕业的研究生的薪酬可以轻松超越一个有多年经验的软件工程师。然而这类人力工作通常并没有太多的创新性可言。这是在当前的科技背景下产生的独特的现象——一大批资质平平的智力工作者受到了疯狂的追捧。这一现象必然在机器学习真正取得突破性进展后终结，本文将讨论突破性进展可能的方向。</p>
<p>科技发展有两种驱动力，其一是资本对于更高生产力的追求，其二是纯粹的为了智力上的满足感或是对于某种超自然信仰而展开的对于真理本质的追求。前者更长于将科学成果转化为实际应用，而后者才是实现技术爆炸的真正内核。机器学习领域也是如此，目前正处于科学成果转化的高峰期，是基于前些年的一些基础研究进行的转化。但也是瓶颈期，因为我们感受到了现有方法的局限。目前人们对于人工智能的态度行业内外的看法截然相反：业外的人大部分担心人工智能将超越、统治甚至毁灭人类，业内大部分则恰恰相反。因为专业人士更加清楚，现有方法大多是非常愚蠢的，所谓智能不过是一种魔术。你一定体验过魔术被揭秘的感觉，这就是我兴致勃勃的了解了许多机器学习方法后的感受。可以很负责任的说，我们距离人工智能的本质还差的很远很远。大部分的人醉心于如何更好的表演魔术，只有很少的人去追求真正的本质，而这些追求本质的方法如果在魔术表演上不够精彩，是很容易被忽视的。</p>
<p>人工智能概念最早由约翰·麦卡锡（John McCarthy，1927年9月4日－2011年10月24日）在1956年由他自己召集的达特矛斯会议上提出的：“人工智能就是要让机器的行为看起来就像是人所表现出的智能行为一样”。这是人工智能的早期定义，人们后来意识到这个定义忽略了强人工智能的可能性。所谓强人工智能是指机器拥有真正的智能而不仅仅是“看起来像”，与之相对的则被称为弱人工智能，而目前实现的都是弱人工智能。这引发了一系列的哲学问题——什么是智能？强人工智能是否有可能实现？强人工智能是不是人类的灾难？即便我们出于伦理和人类安全的考虑，不追求实现强人工智能，而只是追求弱人工智能的“看起来像”的目的，也不得不借鉴强人工智能的思路，也就不可避免的不断的朝着强人工智能方向前进。本文不讨论人工智能是否对人类构成威胁的问题，仅从方向和方法上讨论强人工智能的可行性。</p>
<p>人工智能本质上属于仿生学范畴的。智能本身就是很复杂的，而人类仅能认识到自己的智能，不可能理解龙虾或者外星人的智能。所以对人工智能本质的追求，应该是对人类自身深入认识的过程。并且这种认识不能仅仅停留于哲学解构的层面，必须通过工程手段加以重新建构，我们用哲学上和工程上都能理解的语言进行表达，而这种语言我们称之为模型。</p>
<p>人类智能可划分为一些层次，无意识的精神、意识、自我、逻辑、知识、表达、知觉、经验。这些划分并不存在清晰的界限。通过真正的智能与现有弱人工智能的比较，我们可以更好的理解这些概念。</p>
<p>当AlphaGo轻松打败人类棋手时，人类棋手感到压力、恐惧、伤心、绝望。我们为AlphaGo的棋艺感到惊叹，但事实上二者在智能上相比，人类棋手的情绪反应才是真正令人惊叹的。AlphaGo不会为胜利而感到骄傲，也不会为失败而羞耻，他不会为存在而感到快乐，也不会为消失感到痛苦。他甚至根本没有想要胜利的欲望，更不会有人类各种复杂的情感。这些就是无意识的精神，精神在不自觉的驱动着我们的意识，或纵情声色，或知耻而勇，或趋利避害，或舍生忘死。虽然我们看起来是由理智所控制的动物，但精神不受理智的控制。这点在恋爱中的人身上尤为明显，即便他们的理智告诉他们不要对另一方过于依恋，但他们却无法控制自己所思所想。对于音乐的感受也是类似的，不过是不同频率的声波的组合，竟能使人不由自主的感到快乐或悲伤，多么的神奇。生物学家认为，当多巴胺分泌增加时，人就会感到快乐。多巴胺是一种神经传导物质，这就像是给神经系统的奖励，以刺激我们更加努力的工作。但这并不能解释快乐的本质，我们可以想象我们给车添加了燃油和润滑油，这相当于是给了车奖励，但是车并不会感到快乐。我们在训练AlphaGo时，每次他获胜，我们也会给他一个反馈，告诉他这次做的结果不错，他就会更加靠近这个结果，但他并不会感到快乐。那么为什么多巴胺分泌增加人就会感到快乐呢？显然多巴胺不是快乐的本质，只是快乐的使者而已。</p>
<p>人类诸多行为有可能是后天习得的，但有些则是在被创造时就存在的。食欲、快乐、痛觉、饥饿感、自我、对母亲的依恋，这些都是与生俱来的。所以我们可以认为某种无意识的精神在母体中被创造时就存在了。但是从那一刻这种精神开始存在呢？我们很难想象单细胞状态的受精卵拥有这种精神。当基因代码一层层展开，从单细胞到多细胞，脊椎动物，鱼、两栖、哺乳类、灵长目、人科、智人属的基因片段开始逐步表达，胚胎的结构从简单到复杂，直到降生时我们依然没有发育完全，经历长时间的幼态持续，青春期，我们更多的情绪和欲望被展现出来。当我们回忆我们成长阶段的各种性格变化，这一切究竟是在哪一刻如何产生的呢？我们可以肯定这种精神不是来自于单一的细胞，而是来自大量脑细胞的集体创造。在我们大脑中有一个部分，是在基因中硬编码的部分，做出一些基本的身体机能感受判断、审美判断、情感判断。这部分不受理智控制的部分，我们称为精神。如果没有这种本能的精神，我们的智力也就没有了用武之地。我们可以设想任何的人工智能程序，必须拥有某个目标，这就是他的欲望，在欲望被满足时会得到某种激励。但人类的欲望是如此的不同，在欲望被满足时得到的奖励感受也是如此不同。他不是一个简单弱人工智能所能解释的，以梯度下降法举例：当错误率很高时，函数会修正自己的参数朝着错误率更低的方向前进一小步，直到错误率不再有明显下降。错误率最小是他的目标，但是这个函数是如此机械，我们不可能他是有精神的。</p>
<p><em>精神是目标，但目标不一定是精神，精神本身就是复杂的，是在长期进化中形成的一种复杂的多细胞集体创造。</em></p>
<p><em>人类的意识是一系列智能子系统的集合</em>。佛教所谓六根即：眼耳鼻舌身意，意识是起到总控的作用。意识可以连接我们的视觉、听觉、触觉、味觉、嗅觉，可以控制我们的肌肉，意识是用来做出决策的。精神是意识的内核，是引擎，为意识指出明确的方向。精神是底层的，意识是上层的。当我们在梦境时，我们的意识模糊，但是精神却是清醒的，梦境混乱无序，我们对身体的控制减弱，但我们对于快乐悲伤的感受依然真实。意识不仅仅是各个单元的连接器，其自身也是复杂的，可能是某种层垒的结构，最低级的意识就是对于感觉的条件反射，更高级的意识活动则包含知识、经验、逻辑。意识不等于记忆、知识、逻辑、推理的能力。草原上的野兽，尤其是那些捕猎者，它们都拥有很强的意识，但不等于他们拥有人类一样的高级智力活动。</p>
<p>感觉系统大体是类似的，目前的人工智能在某种程度上已经能够做到一些感知能力，尤其是视觉、听觉方面。对于视觉而言，如果在你的周围空间随便选取一个点，那么通过这个点的只有一些杂乱的电磁波而已，我们称之为光线，但是当我们的眼睛置于这个点，我们能够从这堆杂乱的电磁波中提取出周围物体的信息，投射在我们的视网膜上，视网膜将这些刺激信号传递到我们的视觉处理系统，我们能够分辨“颜色”，“形状”，“材质”，“种类”，“运动”。听觉也是同样神奇，我们的双耳中的空气分子不过是在无序的震动，我们却能从中提取出音色、音高，如果有多种声音同时存在，我们也能很好的将这些声音分离。在一个混乱的环境下，如果我们想要看清或听清一些东西，我们必须付出更多的注意力，也就是我们需要投入更多的意识。这说明我们的感觉系统不仅仅是一个物理系统，更加是一个信息处理的系统。我们通过信息的时间变化，组合，特征分离和重新组合，得出结论。因此对于同一事物的感受，不同的个体是不一样的，这也就是我们说的主观感受。比如我们对于颜色的感受，在意识中的投射是不一样的，一个很明显的证据就是色盲。所以我们虽然都认同某个物体是红色的，红色在我的意识中与在你的意识中可能是完全不同的。在跨越物种之时，感受的区别必然更加明显。有些物种则拥有完全不同的感觉器官，比如可以看到地球磁场的知更鸟（引用）。感觉系统的模拟，首先需要有相应的仪器设备采集信号，信号必须经过无意识的处理，转化为抽象的特征，将抽象特征交给意识体来处理。</p>
<p>假设我们已经能够创造出具有精神、意识、感觉的系统，我们可以开始讨论“自我”这个问题。这个问题看起来非常诡异，一个具有精神、意识、感觉的系统，难道会没有自我的概念吗？如果一个意识体就是整个世界的全部，除了自身之外再无其他，他还会有自我的概念存在吗？自我相对于非我而存在的，没有非我，也就没有自我。如果一个意识体置于一个容器之内，他对容器外的世界一无所知，在他看来他就是全部。当我们沉浸于某个目标时，我们也会忘记了自己的存在。当我们进入竞争环境，自我概念就会强化。自我与存在是同一个问题，保持自我存在是一个本能的精神元素之一。   。。。必须区分内外，头发是不是我，指甲是不是手臂是不是我？</p>
<p>未完待续。。。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2018/06/01/conscious-of-machine/" class="archive-article-date">
  	<time datetime="2018-06-01T03:49:41.000Z" itemprop="datePublished"><i class="icon-clock"></i>2018-06-01</time>
</a>
      
      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a><a class="article-category-link" href="/categories/AI/%E9%9A%8F%E7%AC%94/">随笔</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-guileen-texasholdem-ai" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2015/01/19/guileen-texasholdem-ai/">桂糊涂的德州扑克AI</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <p>如何编写德州扑克的AI。</p>
<p>德州扑克与其他的棋牌类游戏不同，他不一定存在着最优解，更多的是一种心理竞技。这对于人类来说可能很简单，但对于电脑来说却并不擅长。我的目标就是将人类的思维用代码的方式重现出来。</p>
<p>对德州的分析，首先是概率。一手牌的胜率是多少。在底牌，翻牌，转牌，河牌阶段的算法依次如下：</p>
<p>底牌阶段的算法采用查表法。13不同花色的牌组合共有169种，分为，对子面，同花面，杂花面。这里有个小技巧，13可以用4个bit来表示，两张底牌的组合，正好可以用一个字节来表示。我用高位在前表示同花，低位在前表示杂花。</p>
<p>河牌阶段的胜率算法也比较简单。我采用的是穷举法————去除自己的底牌和公牌后，组合遍历对手可能的底牌，将对手的底牌与自己的底牌比较，胜率=获胜次数/组合数。组合数共有995种。这里的关键是如何高效比较我方牌与对手牌的大小。</p>
<p>转牌阶段的胜率，依然采用穷举法————组合遍历河牌，并计算河牌胜率的平均值。河牌共有46种可能，因此算法复杂度为O(45540)。这一切比较都依赖于快速比较手牌，如果手牌比较速度慢，将无法快速计算出这一胜率。</p>
<p>翻牌阶段胜率，无法再采用转牌阶段胜率的算法，这里有个技巧，翻牌阶段胜率近似=发一张牌后的胜率。</p>
<p>如何快速比较自己和对方的手牌。这里也使用了一个小技巧，就是将每张牌面的大小数值化。德州牌面比较是先比较牌型，再比较牌值。因此，将牌型的值作为最高位，将牌面的值作为后5位，即可形成一个比较值。如红3黑5方A梅5方5，牌值为=3555e3。3代表3条牌型，555e3是牌面值。A一般作为14牌值处理。从高牌，对子，两对，三条，顺子，葫芦，同花，四条，同花顺，依次是0，1，2，3，4，5，6，7，8。牌面值直接用2,3,4,5,6,7,8,9,A,B,C,D,E。以16进制表示依然可以有很好的可读性。</p>
<p>在做牌型检测时，同花，顺子比较特殊，其他的4条，3条，葫芦，两对，对子，高牌，都是相似的。</p>
<p>同花检测，将同样花色的牌放进桶里，一个桶里的牌大于等于5张，则为同花，若这5张还能成顺，则为同花顺。</p>
<p>顺子检测时，A较为特殊，既可以和10，J，Q，K搭配，也可以和2，3，4，5，6搭配。将14位数组，设置1，0，1表示存在该牌，0表示不存在。从大到小遍历计数，连续计数5次则为顺子。之所以从大到小遍历是为了找到最大的顺子而不是最小的顺子。</p>
<p>其他牌型比较，先将牌放进13个桶里，然后将这些桶排序。排序方法是，桶里的牌越多越大，牌一样多比较桶本身的编号。这样我们就得出了[3个3，2个J，2个8，1个K]的桶列表。如果第一个桶是4张，那我们的牌型就是4条。第一个桶是3张就是3条，同时第二个桶是2张就是葫芦，前两个桶是两张就是两对，第一个桶是两张是一对，否则就是杂牌。</p>
<p>但是仅仅知道胜率是远远不够的，这只是人类思维的开始。人类对于胜率的判断很多是基于经验，但这种经验非常高效，综合考虑了很多数学概率中没考虑到的因素：比如，玩家数量，底池大小，对手下注量。人类有恐惧感，也有冒险精神，这都是电脑所不具备的。</p>
<p>有没有可能设计一种AI，只根据标准的概率下注，完全不考虑人类心理因素呢？</p>
<p>德州高手会研究对手。如果一个玩家从来不Bluff（诈赌），并且他的下注量和他的胜率相关。有经验的牌手，可以根据他的下注量反推出他的胜率，再根据他的胜率反推出他的手牌。与一个透明的玩家玩牌，要赢可能要看手气，要输真的很难。</p>
<p>另一方面，一个绝不冒险的玩家，也会被轻易的Bluff掉，从而使对手的实际胜率远大于理论上的胜率。</p>
<p>还有一方面，那就是恐惧。电脑不会恐惧，当对手ALLIN的时候，人类会忌惮，会重新评估对手的牌力。但电脑不会，他认为的大牌，无论多大的注都会跟。</p>
<p>因此，学会冒险与恐惧是编写德州AI的最大难点。</p>
<p>人数概率修正。人数影响概率。<br>恐惧修正。<br>安全跟注值计算。<br>Value控制。<br>Balnace。<br>BLUFF。Bluff的时机。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2015/01/19/guileen-texasholdem-ai/" class="archive-article-date">
  	<time datetime="2015-01-18T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2015-01-19</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/AI/" rel="tag">AI</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/game/" rel="tag">game</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
    <article id="post-mathematical-model-of-1024-game" class="article article-type-post" itemscope itemprop="blogPost">
  <div class="article-inner">
    
      <input type="hidden" class="isFancy" />
    
    
      <header class="article-header">
        
  
    <h1 itemprop="name">
      <a class="article-title" href="/2014/04/21/mathematical-model-of-1024-game/">小游戏1024的AI数学模型</a>
    </h1>
  

      </header>
    
    <div class="article-entry" itemprop="articleBody">
      
        <h2 id="游戏规则"><a href="#游戏规则" class="headerlink" title="游戏规则"></a>游戏规则</h2><p>1024 是一个近期比较流行的小游戏，也有称之为2048。游戏在一个4x4的方格(grid)中进行，方格中会在随机的一个空白位置填上一个数字2，玩家可以选择将所有数字向上、向下、向左、或向右的某一方向移动，所有数字将往这一方向移动，如果数字碰到了边界或一个不相同的数字，则停止移动，如果碰到了相同的数字则这两个数字将会合并为两数之和。如果移动之后局面未有任何改变，则此方向禁止移动。在玩家进行了一次移动后，方格中会再随机的产生一个数字，如此循环，直到4个方向都不可以移动则游戏结束。玩家的目标是尽可能的合成更大的数字。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br><span class="line">6</span><br><span class="line">7</span><br><span class="line">8</span><br><span class="line">9</span><br><span class="line">10</span><br><span class="line">11</span><br><span class="line">12</span><br><span class="line">13</span><br><span class="line">14</span><br><span class="line">15</span><br><span class="line">16</span><br><span class="line">17</span><br><span class="line">18</span><br><span class="line">19</span><br><span class="line">20</span><br><span class="line">21</span><br><span class="line">22</span><br><span class="line">23</span><br><span class="line">24</span><br></pre></td><td class="code"><pre><span class="line">2 2 0 0           4 0 0 0</span><br><span class="line">4 0 0 0           4 0 0 0</span><br><span class="line">0 0 0 0  向左移动 0 0 0 0</span><br><span class="line">0 0 0 0           0 0 0 0</span><br><span class="line"></span><br><span class="line">2 2 0 0           0 0 0 4</span><br><span class="line">4 0 0 0           0 0 0 4</span><br><span class="line">0 0 0 0  向右移动 0 0 0 0</span><br><span class="line">0 0 0 0           0 0 0 0</span><br><span class="line"></span><br><span class="line">2 2 0 0           0 0 0 0</span><br><span class="line">4 0 0 0           0 0 0 0</span><br><span class="line">0 0 0 0  向下移动 2 0 0 0</span><br><span class="line">0 0 0 0           4 2 0 0</span><br><span class="line"></span><br><span class="line">2 2 0 0           2 2 0 0</span><br><span class="line">4 0 0 0           4 0 0 0</span><br><span class="line">0 0 0 0  向上移动 0 0 0 0  局面未改变，禁止此方向</span><br><span class="line">0 0 0 0           0 0 0 0</span><br><span class="line"></span><br><span class="line">   2    4    8   16</span><br><span class="line">  32   64  128  256</span><br><span class="line">  64    2    8 1024  任何方向都无法移动，终局。</span><br><span class="line">   2    4   16    8</span><br></pre></td></tr></table></figure>

<h2 id="目标"><a href="#目标" class="headerlink" title="目标"></a>目标</h2><p>笔者尝试通过AI来解决这个问题以得出尽可能大的数字。在开发过程中设计了多种数学模型来求解这一问题，希望借本文与大家分享数学建模的一些思路。</p>
<p>我的AI算法使用的是<a href="http://en.wikipedia.org/wiki/Alpha%E2%80%93beta_pruning" target="_blank" rel="noopener">alpha-beta pruning</a>。Alpha-beta pruning算法常用于博弈程序的AI，比如象棋、国际象棋、五子棋等。其基本思路与<a href="http://en.wikipedia.org/wiki/Minimax" target="_blank" rel="noopener">Minimax search</a>相似，通过对当前局面及其分支的评分，得出最优解的算法，只是通过剪枝的算法大大降低了算法的复杂度。1024这个游戏也可以理解为一个博弈游戏，玩家向某方向移动，电脑落下一个数字，这是一个玩家与电脑博弈的过程，玩家希望得出最大数字，电脑则是尽可能的让这个数字出现在玩家最不希望它出现的位置（实际游戏过程中，这个数字的出现位置是随机的）。</p>
<p>Alpha-beta pruning算法是非常依赖于启发函数（Heuristic）的，启发函数是对于一个局面G的优劣的评估值，记为h(G)，h(G)越大，代表局面越好，h(G)越小代表局面越坏。Alpha-beta pruning的目标是做出当前局面G下的最佳决策，依据于一定深度的子分支的评价结果。因为深度有限且部分分支被抛弃，所以alpha-beta不能保证搜索到的结果是最优的，所以启发函数h(G)不仅用来评价局面的好坏，也用于引导搜索的过程。</p>
<p>本文要讨论的即是关于1024游戏的启发函数h(G)的设计。</p>
<h2 id="数学模型"><a href="#数学模型" class="headerlink" title="数学模型"></a>数学模型</h2><p>我们都知道数学是什么，但什么是数学模型？我们说铁比棉花“重”，这不是数学模型，而我们说铁的密度更大，而密度=质量/体积，这就是数学模型。我们说某个人速度很快，这不是数学模型，速度=距离/时间，他的速度是10米每秒，这就是数学模型。我们说今天阴天，可能会下雨，这不是数学模型，而我们说今天下雨的概率是P(下雨|阴天)，这就是数学模型。</p>
<p>当先贤们创造经典力学的时候，他们是在对我们的日常知识建立数学模型。比如你说推一个轻的车比推一个重的车更容易，但如果你说，F=ma，那么你就是牛顿。</p>
<p>顺便说一句，我认为数学教育的目标应该是建立数学模型，而不是解题。</p>
<p>关于1024这个游戏，我们说局面A比局面B更好，那么怎么样对一个局面的好坏建立数学模型呢？<code>h(G_A) &lt; h(G_B)</code> 则认为局面B比局面A更好。</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br><span class="line">5</span><br></pre></td><td class="code"><pre><span class="line">2 4 2 8     2 4 8 16</span><br><span class="line">4 2 4 4     2 2 4 8</span><br><span class="line">2 4 2 4     4 4 8 4</span><br><span class="line">4 2 0 0     0 0 4 8</span><br><span class="line">   A           B</span><br></pre></td></tr></table></figure>

<h2 id="最大值模型"><a href="#最大值模型" class="headerlink" title="最大值模型"></a>最大值模型</h2><p>基于游戏的基本目标，我们希望尽可能的合成更大的数字，我们可以很显然得出一个数学模型：当局面G里出现的最大数字越大，则代表局面越好。即：</p>
<p>h(G) = Max(G)</p>
<p>假设G中目前的最大数字是256，基于此模型，AI将会尽可能的将这个数字合成到512，然后将512合成1024，然后是2048。看起来是很合理的。但实际测试中发现了如下的局面：</p>
<figure class="highlight plain"><table><tr><td class="gutter"><pre><span class="line">1</span><br><span class="line">2</span><br><span class="line">3</span><br><span class="line">4</span><br></pre></td><td class="code"><pre><span class="line">4   2   4   8</span><br><span class="line">2   512 16  2</span><br><span class="line">64  2   32  4</span><br><span class="line">2   8   4   2</span><br></pre></td></tr></table></figure>

<p>为什么发生这种局面？因为我们的AI过于急功近利了，因为最大值模型。就像我们设计了一个象棋程序，直接吃掉对方老将。。。</p>

      
    </div>
    <div class="article-info article-info-index">
      
      <a href="/2014/04/21/mathematical-model-of-1024-game/" class="archive-article-date">
  	<time datetime="2014-04-20T16:00:00.000Z" itemprop="datePublished"><i class="icon-clock"></i>2014-04-21</time>
</a>
      
	<div class="article-tag tagcloud">
		<i class="icon-price-tags"></i>
		<ul class="article-tag-list" itemprop="keywords"><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/algorithm/" rel="tag">algorithm</a></li><li class="article-tag-list-item"><a class="article-tag-list-link" href="/tags/mathematics/" rel="tag">mathematics</a></li></ul>
	</div>

      
	<div class="article-category tagcloud">
	<i class="icon-price-tags"></i>
	<a class="article-category-link" href="/categories/AI/">AI</a>
	</div>


      <div class="clearfix"></div>
    </div>
  </div>
</article>












  
  


      </div>
      <footer id="footer">
  <div class="outer">
    <div id="footer-info">
    	<div class="footer-left">
    		&copy; 2019 桂糊涂
    	</div>
      	<div class="footer-right">
      		<a href="http://hexo.io/" target="_blank">Hexo</a>  Theme <a href="https://github.com/litten/hexo-theme-yilia" target="_blank">Yilia</a> by Litten
      	</div>
    </div>
  </div>
</footer>
    </div>
    <script>
	var yiliaConfig = {
		fancybox: true,
		mathjax: true,
		animate: true,
		isHome: false,
		isPost: false,
		isArchive: false,
		isTag: false,
		isCategory: true,
		open_in_new: false,
		root: "/",
		innerArchive: true
	}
</script>


<script src="/./main.js"></script>



<script type="text/x-mathjax-config">
MathJax.Hub.Config({
    tex2jax: {
        inlineMath: [ ['$','$'], ["\\(","\\)"]  ],
        processEscapes: true,
        skipTags: ['script', 'noscript', 'style', 'textarea', 'pre', 'code']
    }
});

MathJax.Hub.Queue(function() {
    var all = MathJax.Hub.getAllJax(), i;
    for(i=0; i < all.length; i += 1) {
        all[i].SourceElement().parentNode.className += ' has-jax';
    }
});
</script>

<script src="//cdn.mathjax.org/mathjax/latest/MathJax.js?config=TeX-MML-AM_CHTML">
</script>


    
<div class="tools-col">
  <ul class="btn-wrap">
    
      <li class="chose" data-hook="tools-section-all"><span class="text">全部</span><i class="icon-book"></i></li>
    
    
      <li data-hook="tools-section-tag"><span class="text">标签</span><i class="icon-price-tags"></i></li>
    
    
    
      <li data-hook="tools-section-me"><span class="text">我</span><i class="icon-smile"></i></li>
    
  </ul>
  <div class="tools-wrap">
    
    	<section class="tools-section tools-section-all chose">
    	</section>
    

    
    	<section class="tools-section tools-section-tag">
    			<div class="widget tagcloud" id="js-tagcloud">
    				<a href="/tags/3D/" style="font-size: 10px;">3D</a> <a href="/tags/AI/" style="font-size: 11.25px;">AI</a> <a href="/tags/CI/" style="font-size: 12.5px;">CI</a> <a href="/tags/HTTP/" style="font-size: 10px;">HTTP</a> <a href="/tags/IM/" style="font-size: 10px;">IM</a> <a href="/tags/Objective-C/" style="font-size: 10px;">Objective-C</a> <a href="/tags/React/" style="font-size: 10px;">React</a> <a href="/tags/UI/" style="font-size: 10px;">UI</a> <a href="/tags/VPS/" style="font-size: 10px;">VPS</a> <a href="/tags/algorithm/" style="font-size: 12.5px;">algorithm</a> <a href="/tags/algorithms/" style="font-size: 11.25px;">algorithms</a> <a href="/tags/android/" style="font-size: 12.5px;">android</a> <a href="/tags/architecture/" style="font-size: 18.75px;">architecture</a> <a href="/tags/books/" style="font-size: 10px;">books</a> <a href="/tags/cloud-services/" style="font-size: 10px;">cloud-services</a> <a href="/tags/cluster/" style="font-size: 10px;">cluster</a> <a href="/tags/data/" style="font-size: 10px;">data</a> <a href="/tags/data-format/" style="font-size: 12.5px;">data-format</a> <a href="/tags/editor/" style="font-size: 15px;">editor</a> <a href="/tags/education/" style="font-size: 10px;">education</a> <a href="/tags/engineering/" style="font-size: 10px;">engineering</a> <a href="/tags/ffmpeg/" style="font-size: 10px;">ffmpeg</a> <a href="/tags/font/" style="font-size: 10px;">font</a> <a href="/tags/game/" style="font-size: 15px;">game</a> <a href="/tags/game-dev/" style="font-size: 17.5px;">game-dev</a> <a href="/tags/game-dev-books/" style="font-size: 10px;">game-dev, books</a> <a href="/tags/generator/" style="font-size: 12.5px;">generator</a> <a href="/tags/git/" style="font-size: 10px;">git</a> <a href="/tags/go/" style="font-size: 13.75px;">go</a> <a href="/tags/gossip/" style="font-size: 10px;">gossip</a> <a href="/tags/greek/" style="font-size: 10px;">greek</a> <a href="/tags/hack/" style="font-size: 11.25px;">hack</a> <a href="/tags/hash/" style="font-size: 10px;">hash</a> <a href="/tags/iOS/" style="font-size: 10px;">iOS</a> <a href="/tags/input-methods/" style="font-size: 10px;">input-methods</a> <a href="/tags/java/" style="font-size: 10px;">java</a> <a href="/tags/js/" style="font-size: 12.5px;">js</a> <a href="/tags/live-dev/" style="font-size: 10px;">live-dev</a> <a href="/tags/lua/" style="font-size: 10px;">lua</a> <a href="/tags/machine-learning/" style="font-size: 12.5px;">machine-learning</a> <a href="/tags/makefile/" style="font-size: 10px;">makefile</a> <a href="/tags/marketing/" style="font-size: 11.25px;">marketing</a> <a href="/tags/mathematics/" style="font-size: 13.75px;">mathematics</a> <a href="/tags/mysql/" style="font-size: 12.5px;">mysql</a> <a href="/tags/network/" style="font-size: 16.25px;">network</a> <a href="/tags/nginx/" style="font-size: 11.25px;">nginx</a> <a href="/tags/node-js/" style="font-size: 12.5px;">node.js</a> <a href="/tags/nosql/" style="font-size: 10px;">nosql</a> <a href="/tags/ops/" style="font-size: 20px;">ops</a> <a href="/tags/python/" style="font-size: 10px;">python</a> <a href="/tags/redis/" style="font-size: 11.25px;">redis</a> <a href="/tags/security/" style="font-size: 11.25px;">security</a> <a href="/tags/slide/" style="font-size: 10px;">slide</a> <a href="/tags/svn/" style="font-size: 10px;">svn</a> <a href="/tags/tcp/" style="font-size: 12.5px;">tcp</a> <a href="/tags/terminal/" style="font-size: 10px;">terminal</a> <a href="/tags/tools/" style="font-size: 12.5px;">tools</a> <a href="/tags/udp/" style="font-size: 10px;">udp</a> <a href="/tags/video/" style="font-size: 10px;">video</a> <a href="/tags/vim/" style="font-size: 11.25px;">vim</a> <a href="/tags/web/" style="font-size: 17.5px;">web</a>
    			</div>
    	</section>
    

    

    
    	<section class="tools-section tools-section-me">
  	  	
  	  		<div class="aboutme-wrap" id="js-aboutme">一&lt;br&gt;个&lt;br&gt;码&lt;br&gt;农</div>
  	  	
    	</section>
    
  </div>
  
</div>
    <!-- Root element of PhotoSwipe. Must have class pswp. -->
<div class="pswp" tabindex="-1" role="dialog" aria-hidden="true">

    <!-- Background of PhotoSwipe. 
         It's a separate element as animating opacity is faster than rgba(). -->
    <div class="pswp__bg"></div>

    <!-- Slides wrapper with overflow:hidden. -->
    <div class="pswp__scroll-wrap">

        <!-- Container that holds slides. 
            PhotoSwipe keeps only 3 of them in the DOM to save memory.
            Don't modify these 3 pswp__item elements, data is added later on. -->
        <div class="pswp__container">
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
            <div class="pswp__item"></div>
        </div>

        <!-- Default (PhotoSwipeUI_Default) interface on top of sliding area. Can be changed. -->
        <div class="pswp__ui pswp__ui--hidden">

            <div class="pswp__top-bar">

                <!--  Controls are self-explanatory. Order can be changed. -->

                <div class="pswp__counter"></div>

                <button class="pswp__button pswp__button--close" title="Close (Esc)"></button>

                <button class="pswp__button pswp__button--share" style="display:none" title="Share"></button>

                <button class="pswp__button pswp__button--fs" title="Toggle fullscreen"></button>

                <button class="pswp__button pswp__button--zoom" title="Zoom in/out"></button>

                <!-- Preloader demo http://codepen.io/dimsemenov/pen/yyBWoR -->
                <!-- element will get class pswp__preloader--active when preloader is running -->
                <div class="pswp__preloader">
                    <div class="pswp__preloader__icn">
                      <div class="pswp__preloader__cut">
                        <div class="pswp__preloader__donut"></div>
                      </div>
                    </div>
                </div>
            </div>

            <div class="pswp__share-modal pswp__share-modal--hidden pswp__single-tap">
                <div class="pswp__share-tooltip"></div> 
            </div>

            <button class="pswp__button pswp__button--arrow--left" title="Previous (arrow left)">
            </button>

            <button class="pswp__button pswp__button--arrow--right" title="Next (arrow right)">
            </button>

            <div class="pswp__caption">
                <div class="pswp__caption__center"></div>
            </div>

        </div>

    </div>

</div>
  </div>
</body>
</html>